{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../llm_gemini_pdf_chat/docs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Generative Deep Learning\\n\\nTeaching Machines to Paint, Write, Compose and Play\\n\\nDavid Foster\\n\\nGenerative Deep Learning Teaching Machines to Paint, Write, Compose, and Play\\n\\nDavid Foster\\n\\nBeijing Beijing\\n\\nBoston Boston\\n\\nFarnham Sebastopol Farnham Sebastopol\\n\\nTokyo Tokyo\\n\\nGenerative Deep Learning by David Foster\\n\\nCopyright © 2019 Applied Data Science Partners Ltd. All rights reserved.\\n\\nPrinted in the United States of America.\\n\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\n\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\\n\\nDevelopment Editor: Michele Cronin Acquisitions Editor: Jonathan Hassell Production Editor: Katherine Tozer Copyeditor: Rachel Head Proofreader: Charles Roumeliotis\\n\\nIndexer: Judith McConville Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Rebecca Demarest\\n\\nJuly 2019:\\n\\nFirst Edition\\n\\nRevision History for the First Edition 2019-06-26: First Release\\n\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492041948 for release details.\\n\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Generative Deep Learning, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\n\\nThe views expressed in this work are those of the author, and do not represent the publisher’s views. While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\\n\\n978-1-492-04194-8\\n\\n[LSI]\\n\\nTable of Contents\\n\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\\n\\nPart I.\\n\\nIntroduction to Generative Deep Learning\\n\\n1. Generative Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 What Is Generative Modeling? 1 Generative Versus Discriminative Modeling 2 Advances in Machine Learning 4 The Rise of Generative Modeling 5 The Generative Modeling Framework 7 Probabilistic Generative Models 10 Hello Wrodl! 13 Your First Probabilistic Generative Model 14 Naive Bayes 17 Hello Wrodl! Continued 20 The Challenges of Generative Modeling 22 Representation Learning 23 Setting Up Your Environment 27 Summary 29\\n\\n2. Deep Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Structured and Unstructured Data 31 Deep Neural Networks 33 Keras and TensorFlow 34 Your First Deep Neural Network 35 Loading the Data 35\\n\\niii\\n\\nBuilding the Model 37 Compiling the Model 41 Training the Model 43 Evaluating the Model 44 Improving the Model 46 Convolutional Layers 46 Batch Normalization 51 Dropout Layers 54 Putting It All Together 55 Summary 59\\n\\n3. Variational Autoencoders. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 The Art Exhibition 61 Autoencoders 64 Your First Autoencoder 66 The Encoder 66 The Decoder 68 Joining the Encoder to the Decoder 71 Analysis of the Autoencoder 72 The Variational Art Exhibition 75 Building a Variational Autoencoder 78 The Encoder 78 The Loss Function 84 Analysis of the Variational Autoencoder 85 Using VAEs to Generate Faces 86 Training the VAE 87 Analysis of the VAE 91 Generating New Faces 92 Latent Space Arithmetic 93 Morphing Between Faces 94 Summary 95\\n\\n4. Generative Adversarial Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 Ganimals 97 Introduction to GANs 99 Your First GAN 100 The Discriminator 101 The Generator 103 Training the GAN 107 GAN Challenges 112 Oscillating Loss 112\\n\\niv\\n\\n|\\n\\nTable of Contents\\n\\nMode Collapse 113 Uninformative Loss 114 Hyperparameters 114 Tackling the GAN Challenges 115 Wasserstein GAN 115 Wasserstein Loss 115 The Lipschitz Constraint 117 Weight Clipping 118 Training the WGAN 119 Analysis of the WGAN 120 WGAN-GP 121 The Gradient Penalty Loss 121 Analysis of WGAN-GP 125 Summary 127\\n\\nPart II.\\n\\nTeaching Machines to Paint, Write, Compose, and Play\\n\\n5. Paint. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 Apples and Organges 132 CycleGAN 135 Your First CycleGAN 137 Overview 137 The Generators (U-Net) 139 The Discriminators 142 Compiling the CycleGAN 144 Training the CycleGAN 146 Analysis of the CycleGAN 147 Creating a CycleGAN to Paint Like Monet 149 The Generators (ResNet) 150 Analysis of the CycleGAN 151 Neural Style Transfer 153 Content Loss 154 Style Loss 156 Total Variance Loss 160 Running the Neural Style Transfer 160 Analysis of the Neural Style Transfer Model 161 Summary 162\\n\\n6. Write. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 The Literary Society for Troublesome Miscreants 166\\n\\nTable of Contents\\n\\n|\\n\\nv\\n\\nLong Short-Term Memory Networks 167 Your First LSTM Network 168 Tokenization 168 Building the Dataset 171 The LSTM Architecture 172 The Embedding Layer 172 The LSTM Layer 174 The LSTM Cell 176 Generating New Text 179 RNN Extensions 183 Stacked Recurrent Networks 183 Gated Recurrent Units 185 Bidirectional Cells 187 Encoder–Decoder Models 187 A Question and Answer Generator 190 A Question-Answer Dataset 191 Model Architecture 192 Inference 196 Model Results 198 Summary 200\\n\\n7. Compose. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 Preliminaries 202 Musical Notation 202 Your First Music-Generating RNN 205 Attention 206 Building an Attention Mechanism in Keras 208 Analysis of the RNN with Attention 213 Attention in Encoder–Decoder Networks 217 Generating Polyphonic Music 221 The Musical Organ 221 Your First MuseGAN 223 The MuseGAN Generator 226 Chords, Style, Melody, and Groove 227 The Bar Generator 229 Putting It All Together 230 The Critic 232 Analysis of the MuseGAN 233 Summary 235 Table of Contents\\n\\n7. Compose. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 Preliminaries 202 Musical Notation 202 Your First Music-Generating RNN 205 Attention 206 Building an Attention Mechanism in Keras 208 Analysis of the RNN with Attention 213 Attention in Encoder–Decoder Networks 217 Generating Polyphonic Music 221 The Musical Organ 221 Your First MuseGAN 223 The MuseGAN Generator 226 Chords, Style, Melody, and Groove 227 The Bar Generator 229 Putting It All Together 230 The Critic 232 Analysis of the MuseGAN 233 Summary 235 Table of Contents\\n\\n8. Play. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 Reinforcement Learning 238 OpenAI Gym 239 World Model Architecture 241 The Variational Autoencoder 242 The MDN-RNN 243 The Controller 243 Setup 244 Training Process Overview 245 Collecting Random Rollout Data 245 Training the VAE 248 The VAE Architecture 249 Exploring the VAE 252 Collecting Data to Train the RNN 255 Training the MDN-RNN 257 The MDN-RNN Architecture 258 Sampling the Next z and Reward from the MDN-RNN 259 The MDN-RNN Loss Function 259 Training the Controller 261 The Controller Architecture 262 CMA-ES 262 Parallelizing CMA-ES 265 Output from the Controller Training 267 In-Dream Training 268 In-Dream Training the Controller 270 Challenges of In-Dream Training 272 Summary 273\\n\\n9. The Future of Generative Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 Five Years of Progress 275 The Transformer 277 Positional Encoding 279 Multihead Attention 280 The Decoder 283 Analysis of the Transformer 283 BERT 285 GPT-2 285 MuseNet 286 Advances in Image Generation 287 ProGAN 287 Self-Attention GAN (SAGAN) 289 vii\\n\\nBigGAN 291 StyleGAN 292 Applications of Generative Modeling 296 AI Art 296 AI Music 297\\n\\n10. Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\\n\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\\n\\nviii\\n\\n|\\n\\nTable of Contents\\n\\nPreface\\n\\nWhat I cannot create, I do not understand.\\n\\n—Richard Feynman\\n\\nAn undeniable part of the human condition is our ability to create. Since our earliest days as cave people, we have sought opportunities to generate original and beautiful creations. For early man, this took the form of cave paintings depicting wild animals and abstract patterns, created with pigments placed carefully and methodically onto rock. The Romantic Era gave us the mastery of Tchaikovsky symphonies, with their ability to inspire feelings of triumph and tragedy through sound waves, woven together to form beautiful melodies and harmonies. And in recent times, we have found ourselves rushing to bookshops at midnight to buy stories about a fictional wizard, because the combination of letters creates a narrative that wills us to turn the page and find out what happens to our hero.\\n\\nIt is therefore not surprising that humanity has started to ask the ultimate question of creativity: can we create something that is in itself creative?\\n\\nThis is the question that generative modeling aims to answer. With recent advances in methodology and technology, we are now able to build machines that can paint origi‐ nal artwork in a given style, write coherent paragraphs with long-term structure, compose music that is pleasant to listen to, and develop winning strategies for com‐ plex games by generating imaginary future scenarios. This is just the start of a gener‐ ative revolution that will leave us with no choice but to find answers to some of the biggest questions about the mechanics of creativity, and ultimately, what it means to be human.\\n\\nIn short, there has never been a better time to learn about generative modeling—so let’s get started!\\n\\nix\\n\\nObjective and Approach This book covers the key techniques that have dominated the generative modeling landscape in recent years and have allowed us to make impressive progress in creative tasks. As well as covering core generative modeling theory, we will be building full working examples of some of the key models from the literature and walking through the codebase for each, step by step.\\n\\nThroughout the book, you will find short, allegorical stories that help explain the mechanics of some of the models we will be building. I believe that one of the best ways to teach a new abstract theory is to first convert it into something that isn’t quite so abstract, such as a story, before diving into the technical explanation. The individ‐ ual steps of the theory are clearer within this context because they involve people, actions, and emotions, all of which are well understood, rather than neural networks, backpropagation, and loss functions, which are abstract constructs.\\n\\nThe story and the model explanation are just the same mechanics explained in two different domains. You might therefore find it useful to refer back to the relevant story while learning about each model. If you are already familiar with a particular technique, then have fun finding the parallels of each model element within the story!\\n\\nIn Part I of this book I shall introduce the key techniques that we will be using to build generative models, including an overview of deep learning, variational autoen‐ coders, and generative adversarial networks. In Part II, we will be building on these techniques to tackle several creative tasks, such as painting, writing, and composing music through models such as CycleGAN, encoder–decoder models, and MuseGAN. In addition, we shall see how generative modeling can be used to optimize playing strategy for a game (World Models) and take a look at the most cutting-edge genera‐ tive architectures available today, such as StyleGAN, BigGAN, BERT, GPT-2, and MuseNet.\\n\\nPrerequisites This book assumes that you have experience coding in Python. If you are not familiar with Python, the best place to start is through LearningPython.org. There are many free resources online that will allow you to develop enough Python knowledge to work with the examples in this book.\\n\\nAlso, since some of the models are described using mathematical notation, it will be useful to have a solid understanding of linear algebra (for example, matrix multiplica‐ tion, etc.) and general probability theory.\\n\\nFinally, you will need an environment in which to run the code examples from the book’s GitHub repository. I have deliberately ensured that all of the examples in this book do not require prohibitively large amounts of computational resources to train.\\n\\nx\\n\\n| Preface\\n\\nThere is a myth that you need a GPU in order to start training deep learning models —while this is of course helpful and will speed up training, it is not essential. In fact, if you are new to deep learning, I encourage you to first get to grips with the essen‐ tials by experimenting with small examples on your laptop, before spending money and time researching hardware to speed up training.\\n\\nOther Resources Two books I highly recommend as a general introduction to machine learning and deep learning are as follows:\\n\\nHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems by Aurelien Geron (O’Reilly)\\n\\nDeep Learning with Python by Francois Chollet (Manning)\\n\\nMost of the papers in this book are sourced through arXiv, a free repository of scien‐ tific research papers. It is now common for authors to post papers to arXiv before they are fully peer-reviewed. Reviewing the recent submissions is a great way to keep on top of the most cutting-edge developments in the field.\\n\\nI also highly recommend the website Papers with Code, where you can find the latest state-of-the-art results in a variety of machine learning tasks, alongside links to the papers and official GitHub repositories. It is an excellent resource for anyone wanting to quickly understand which techniques are currently achieving the highest scores in a range of tasks and has certainly helped me to decide which techniques to cover in this book.\\n\\nFinally, a useful resource for training deep learning models on accelerated hardware is Google Colaboratory. This is a free Jupyter Notebook environment that requires no setup and runs entirely in the cloud. You can tell the notebook to run on a GPU that is provided for free, for up to 12 hours of runtime. While it is not essential to run the examples in this book on a GPU, it may help to speed up the training process. Either way, Colab is a great way to access GPU resources for free.\\n\\nPreface\\n\\n|\\n\\nxi\\n\\nConventions Used in This Book The following typographical conventions are used in this book:\\n\\nItalic\\n\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\n\\nConstant width\\n\\nUsed for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements, and keywords.\\n\\nConstant width bold\\n\\nShows commands or other text that should be typed literally by the user.\\n\\nConstant width italic\\n\\nShows text that should be replaced with user-supplied values or by values deter‐ mined by context.\\n\\nThis element signifies a general note.\\n\\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/davidADSP/GDL_code.\\n\\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a signifi‐ cant amount of example code from this book into your product’s documentation does require permission.\\n\\nWe appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Generative Deep Learning by David Foster (O’Reilly). Copyright 2019 Applied Data Science Partners Ltd., 978-1-492-04194-8.”\\n\\nxii\\n\\n| Preface\\n\\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\\n\\nO’Reilly Online Learning\\n\\nFor almost 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help compa‐ nies succeed.\\n\\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, conferences, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in- depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, please visit http://oreilly.com.\\n\\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\\n\\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\\n\\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/generative-dl.\\n\\nTo comment or ask technical questions about this book, send email to bookques‐ tions@oreilly.com.\\n\\nFor more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com.\\n\\nFind us on Facebook: http://facebook.com/oreilly\\n\\nFollow us on Twitter: http://twitter.com/oreillymedia\\n\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\n\\nPreface\\n\\n|\\n\\nxiii\\n\\nAcknowledgments There are so many people I would like to thank for helping me write this book.\\n\\nFirst, I would like to thank everyone who has taken time to technically review the book—in particular, Luba Elliott, Darren Richardson, Eric George, Chris Schon, Sigurður Skúli Sigurgeirsson, Hao-Wen Dong, David Ha, and Lorna Barclay.\\n\\nAlso, a huge thanks to my colleagues at Applied Data Science Partners, Ross Witeszc‐ zak, Chris Schon, Daniel Sharp, and Amy Bull. Your patience with me while I have taken time to finish the book is hugely appreciated, and I am greatly looking forward to all the machine learning projects we will complete together in the future! Particular thanks to Ross—had we not decided to start a business together, this book might never have taken shape, so thank you for believing in me as your business partner!\\n\\nI also want to thank anyone who has ever taught me anything mathematical—I was extremely fortunate to have fantastic math teachers at school, who developed my interest in the subject and encouraged me to pursue it further at university. I would like to thank you for your commitment and for going out of your way to share your knowledge of the subject with me.\\n\\nA huge thank you goes to the staff at O’Reilly for guiding me through the process of writing this book. A special thanks goes to Michele Cronin, who has been there at each step, providing useful feedback and sending me friendly reminders to keep com‐ pleting chapters! Also to Katie Tozer, Rachel Head, and Melanie Yarbrough for get‐ ting the book into production, and Mike Loukides for first reaching out to ask if I’d be interested in writing a book. You have all been so supportive of this project from the start, and I want to thank you for providing me with a platform on which to write about something that I love.\\n\\nThroughout the writing process, my family has been a constant source of encourage‐ ment and support. A huge thank you goes to my mum, Gillian Foster, for checking every single line of text for typos and for teaching me how to add up in the first place! Your attention to detail has been extremely helpful while proofreading this book, and I’m really grateful for all the opportunities that both you and dad have given me. My dad, Clive Foster, originally taught me how to program a computer—this book is full of practical examples, and that’s thanks to his early patience while I fumbled around in BASIC trying to make football games as a teenager. My brother, Rob Foster, is the most modest genius you will ever find, particularly within linguistics—chatting with him about AI and the future of text-based machine learning has been amazingly helpful. Last, I would like to thank my Nana, who is a constant source of inspiration and fun for all of us. Her love of literature is one of the reasons I first decided that writing a book would be an exciting thing to do.\\n\\nxiv\\n\\n| Preface\\n\\nFinally, I would like to thank my fiancée (and soon to be wife) Lorna Barclay. As well as technically reviewing every word of this book, she has provided endless support to me throughout the writing process, making me tea, bringing me various snacks, and generally helping me to make this a better guide to generative modeling through her meticulous attention to detail and deep expertise in statistics and machine learning. I certainly couldn’t have completed this project without you, and I’m grateful for the time you have invested in helping me restructure and expand parts of the book that needed more explanation. I promise I won’t talk about generative modeling at the dinner table for at least a few weeks after it is published.\\n\\nPreface\\n\\n|\\n\\nxv\\n\\nPART I Introduction to Generative Deep Learning\\n\\nThe first four chapters of this book aim to introduce the core techniques that you’ll need to start building generative deep learning models.\\n\\nIn Chapter 1, we shall first take a broad look at the field of generative modeling and consider the type of problem that we are trying to solve from a probabilistic perspec‐ tive. We will then explore our first example of a basic probabilistic generative model and analyze why deep learning techniques may need to be deployed as the complexity of the generative task grows.\\n\\nChapter 2 provides a guide to the deep learning tools and techniques that you will need to start building more complex generative models. This is intended to be a prac‐ tical guide to deep learning rather than a theoretical analysis of the field. In particular, I will introduce Keras, a framework for building neural networks that can be used to construct and train some of the most cutting-edge deep neural network architectures published in the literature.\\n\\nIn Chapter 3, we shall take a look at our first generative deep learning model, the var‐ iational autoencoder. This powerful technique will allow us to not only generate real‐ istic faces, but also alter existing images—for example, by adding a smile or changing the color of someone’s hair.\\n\\nChapter 4 explores one of the most successful generative modeling techniques of recent years, the generative adversarial network. This elegant framework for structur‐ ing a generative modeling problem is the underlying engine behind most state-of- the-art generative models. We shall see the ways that it has been fine-tuned and adapted to continually push the boundaries of what generative modeling is able to achieve.\\n\\nCHAPTER 1 Generative Modeling\\n\\nThis chapter is a general introduction to the field of generative modeling. We shall first look at what it means to say that a model is generative and learn how it differs from the more widely studied discriminative modeling. Then I will introduce the framework and core mathematical ideas that will allow us to structure our general approach to problems that require a generative solution.\\n\\nWith this in place, we will then build our first example of a generative model (Naive Bayes) that is probabilistic in nature. We shall see that this allows us to generate novel examples that are outside of our training dataset, but shall also explore the reasons why this type of model may fail as the size and complexity of the space of possible creations increases.\\n\\nWhat Is Generative Modeling? A generative model can be broadly defined as follows:\\n\\nA generative model describes how a dataset is generated, in terms of a probabilistic model. By sampling from this model, we are able to generate new data.\\n\\nSuppose we have a dataset containing images of horses. We may wish to build a model that can generate a new image of a horse that has never existed but still looks real because the model has learned the general rules that govern the appearance of a horse. This is the kind of problem that can be solved using generative modeling. A summary of a typical generative modeling process is shown in Figure 1-1.\\n\\nFirst, we require a dataset consisting of many examples of the entity we are trying to generate. This is known as the training data, and one such data point is called an observation.\\n\\n1\\n\\nFigure 1-1. The generative modeling process\\n\\nEach observation consists of many features—for an image generation problem, the features are usually the individual pixel values. It is our goal to build a model that can generate new sets of features that look as if they have been created using the same rules as the original data. Conceptually, for image generation this is an incredibly dif‐ ficult task, considering the vast number of ways that individual pixel values can be assigned and the relatively tiny number of such arrangements that constitute an image of the entity we are trying to simulate.\\n\\nA generative model must also be probabilistic rather than deterministic. If our model is merely a fixed calculation, such as taking the average value of each pixel in the dataset, it is not generative because the model produces the same output every time. The model must include a stochastic (random) element that influences the individual samples generated by the model.\\n\\nIn other words, we can imagine that there is some unknown probabilistic distribution that explains why some images are likely to be found in the training dataset and other images are not. It is our job to build a model that mimics this distribution as closely as possible and then sample from it to generate new, distinct observations that look as if they could have been included in the original training set.\\n\\nGenerative Versus Discriminative Modeling In order to truly understand what generative modeling aims to achieve and why this is important, it is useful to compare it to its counterpart, discriminative modeling. If you have studied machine learning, most problems you will have faced will have most likely been discriminative in nature. To understand the difference, let’s look at an example.\\n\\nSuppose we have a dataset of paintings, some painted by Van Gogh and some by other artists. With enough data, we could train a discriminative model to predict if a given painting was painted by Van Gogh. Our model would learn that certain colors,\\n\\n2\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nshapes, and textures are more likely to indicate that a painting is by the Dutch master, and for paintings with these features, the model would upweight its prediction accordingly. Figure 1-2 shows the discriminative modeling process—note how it dif‐ fers from the generative modeling process shown in Figure 1-1.\\n\\nFigure 1-2. The discriminative modeling process\\n\\nOne key difference is that when performing discriminative modeling, each observa‐ tion in the training data has a label. For a binary classification problem such as our artist discriminator, Van Gogh paintings would be labeled 1 and non–Van Gogh paintings labeled 0. Our model then learns how to discriminate between these two groups and outputs the probability that a new observation has label 1—i.e., that it was painted by Van Gogh.\\n\\nFor this reason, discriminative modeling is synonymous with supervised learning, or learning a function that maps an input to an output using a labeled dataset. Genera‐ tive modeling is usually performed with an unlabeled dataset (that is, as a form of unsupervised learning), though it can also be applied to a labeled dataset to learn how to generate observations from each distinct class.\\n\\nLet’s take a look at some mathematical notation to describe the difference between generative and discriminative modeling.\\n\\nDiscriminative modeling estimates p y x —the probability of a label y given observa‐ tion x.\\n\\nGenerative modeling estimates p x —the probability of observing observation x.\\n\\nIf the dataset is labeled, we can also build a generative model that estimates the distri‐ bution p x y .\\n\\nWhat Is Generative Modeling?\\n\\n|\\n\\n3\\n\\nIn other words, discriminative modeling attempts to estimate the probability that an observation x belongs to category y. Generative modeling doesn’t care about labeling observations. Instead, it attempts to estimate the probability of seeing the observation at all.\\n\\nThe key point is that even if we were able to build a perfect discriminative model to identify Van Gogh paintings, it would still have no idea how to create a painting that looks like a Van Gogh. It can only output probabilities against existing images, as this is what it has been trained to do. We would instead need to train a generative model, which can output sets of pixels that have a high chance of belonging to the original training dataset.\\n\\nAdvances in Machine Learning To understand why generative modeling can be considered the next frontier for machine learning, we must first look at why discriminative modeling has been the driving force behind most progress in machine learning methodology in the last two decades, both in academia and in industry.\\n\\nFrom an academic perspective, progress in discriminative modeling is certainly easier to monitor, as we can measure performance metrics against certain high-profile clas‐ sification tasks to determine the current best-in-class methodology. Generative mod‐ els are often more difficult to evaluate, especially when the quality of the output is largely subjective. Therefore, much emphasis in recent years has been placed on training discriminative models to reach human or superhuman performance in a variety of image or text classification tasks.\\n\\nFor example, for image classification, the key breakthrough came in 2012 when a team led by Geoff Hinton at the University of Toronto won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with a deep convolutional neural network. The competition involves classifying images into one of a thousand categories and is used as a benchmark to compare the latest state-of-the-art techniques. The deep learning model had an error rate of 16%—a massive improvement on the next best model, which only achieved a 26.2% error rate. This sparked a deep learning boom that has resulted in the error rate falling even further year after year. The 2015 winner achieved superhuman performance for the first time, with an error rate of 4%, and the current state-of-the-art model achieves an error rate of just 2%. Many would now consider the challenge a solved problem.\\n\\nAs well as it being easier to publish measurable results within an academic setting, discriminative modeling has historically been more readily applicable to business problems than generative modeling. Generally, in a business setting, we don’t care how the data was generated, but instead want to know how a new example should be categorized or valued. For example:\\n\\n4\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nGiven a satellite image, a government defense official would only care about the probability that it contains enemy units, not the probability that this particular image should appear.\\n\\nA customer relations manager would only be interested in knowing if the senti‐ ment of an incoming email is positive or negative and wouldn’t find much use in a generative model that could output examples of customer emails that don’t yet exist.\\n\\nA doctor would want to know the chance that a given retinal image indicates glaucoma, rather than have access to a model that can generate novel pictures of the back of an eye.\\n\\nAs most solutions required by businesses are in the domain of discriminative model‐ ing, there has been a rise in the number of Machine-Learning-as-a-Service (MLaaS) tools that aim to commoditize the use of discriminative modeling within industry, by largely automating the build, validation, and monitoring processes that are common to almost all discriminative modeling tasks.\\n\\nThe Rise of Generative Modeling While discriminative modeling has so far provided the bulk of the impetus behind advances in machine learning, in the last three to five years many of the most inter‐ esting advancements in the field have come through novel applications of deep learn‐ ing to generative modeling tasks.\\n\\nIn particular, there has been increased media attention on generative modeling projects such as StyleGAN from NVIDIA,1 which is able to create hyper-realistic images of human faces, and the GPT-2 language model from OpenAI,2 which is able to complete a passage of text given a short introductory paragraph.\\n\\nFigure 1-3 shows the striking progress that has already been made in facial image generation since 2014.3 There are clear positive applications here for industries such as game design and cinematography, and improvements in automatic music genera‐ tion will also surely start to resonate within these domains. It remains to be seen whether we will one day read news articles or novels written by a generative model, but the recent progress in this area is staggering and it is certainly not outrageous to suggest that this one day may be the case. While exciting, this also raises ethical\\n\\n1 Tero Karras, Samuli Laine, and Timo Aila, “A Style-Based Generator Architecture for Generative Adversarial\\n\\nNetworks,” 12 December 2018, https://arxiv.org/abs/1812.04948.\\n\\n2 Alec Radford et al., “Language Models Are Unsupervised Multitask Learners,” 2019, https://paperswith\\n\\ncode.com/paper/language-models-are-unsupervised-multitask.\\n\\n3 Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,”\\n\\nFebruary 2018, https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf.\\n\\nWhat Is Generative Modeling?\\n\\n|\\n\\n5\\n\\nquestions around the proliferation of fake content on the internet and means it may become ever harder to trust what we see and read through public channels of communication.\\n\\nFigure 1-3. Face generation using generative modeling has improved significantly in the last four years4\\n\\nAs well as the practical uses of generative modeling (many of which are yet to be dis‐ covered), there are three deeper reasons why generative modeling can be considered the key to unlocking a far more sophisticated form of artificial intelligence, that goes beyond what discriminative modeling alone can achieve.\\n\\nFirst, purely from a theoretical point of view, we should not be content with only being able to excel at categorizing data but should also seek a more complete under‐ standing of how the data was generated in the first place. This is undoubtedly a more difficult problem to solve, due to the high dimensionality of the space of feasible out‐ puts and the relatively small number of creations that we would class as belonging to the dataset. However, as we shall see, many of the same techniques that have driven development in discriminative modeling, such as deep learning, can be utilized by generative models too.\\n\\nSecond, it is highly likely that generative modeling will be central to driving future developments in other fields of machine learning, such as reinforcement learning (the study of teaching agents to optimize a goal in an environment through trial and error). For example, we could use reinforcement learning to train a robot to walk across a given terrain. The general approach would be to build a computer simulation of the terrain and then run many experiments where the agent tries out different strategies. Over time the agent would learn which strategies are more successful than others and therefore gradually improve. A typical problem with this approach is that the physics of the environment is often highly complex and would need be calculated at each timestep in order to feed the information back to the agent to decide its next move. However, if the agent were able to simulate its environment through a genera‐ tive model, it wouldn’t need to test out the strategy in the computer simulation or in\\n\\n4 Source: Brundage et al., 2018.\\n\\n6\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nthe real world, but instead could learn in its own imaginary environment. In Chap‐ ter 8 we shall see this idea in action, training a car to drive as fast as possible around a track by allowing it to learn directly from its own hallucinated environment.\\n\\nFinally, if we are to truly say that we have built a machine that has acquired a form of intelligence that is comparable to a human’s, generative modeling must surely be part of the solution. One of the finest examples of a generative model in the natural world is the person reading this book. Take a moment to consider what an incredible gener‐ ative model you are. You can close your eyes and imagine what an elephant would look like from any possible angle. You can imagine a number of plausible different endings to your favorite TV show, and you can plan your week ahead by working through various futures in your mind’s eye and taking action accordingly. Current neuroscientific theory suggests that our perception of reality is not a highly complex discriminative model operating on our sensory input to produce predictions of what we are experiencing, but is instead a generative model that is trained from birth to produce simulations of our surroundings that accurately match the future. Some the‐ ories even suggest that the output from this generative model is what we directly per‐ ceive as reality. Clearly, a deep understanding of how we can build machines to acquire this ability will be central to our continued understanding of the workings of the brain and general artificial intelligence.\\n\\nWith this in mind, let’s begin our journey into the exciting world of generative model‐ ing. To begin with we shall look at the simplest examples of generative models and some of the ideas that will help us to work through the more complex architectures that we will encounter later in the book.\\n\\nThe Generative Modeling Framework Let’s start by playing a generative modeling game in just two dimensions. I’ve chosen a rule that has been used to generate the set of points X in Figure 1-4. Let’s call this rule pdata. Your challenge is to choose a different point x = x1, x2 in the space that looks like it has been generated by the same rule.\\n\\nWhat Is Generative Modeling?\\n\\n|\\n\\n7\\n\\nFigure 1-4. A set of points in two dimensions, generated by an unknown rule pdata\\n\\nWhere did you choose? You probably used your knowledge of the existing data points to construct a mental model, pmodel, of whereabouts in the space the point is more likely to be found. In this respect, pmodel is an estimate of pdata. Perhaps you decided that pmodel should look like Figure 1-5—a rectangular box where points may be found, and an area outside of the box where there is no chance of finding any points. To gen‐ erate a new observation, you can simply choose a point at random within the box, or more formally, sample from the distribution pmodel. Congratulations, you have just devised your first generative model!\\n\\nFigure 1-5. The orange box, pmodel, is an estimate of the true data-generating distribution, pdata\\n\\n8\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nWhile this isn’t the most complex example, we can use it to understand what genera‐ tive modeling is trying to achieve. The following framework sets out our motivations.\\n\\nThe Generative Modeling Framework\\n\\nWe have a dataset of observations X.\\n\\nWe assume that the observations have been generated according to some unknown distribution, pdata.\\n\\nA generative model pmodel tries to mimic pdata. If we achieve this goal, we can sam‐ ple from pmodel to generate observations that appear to have been drawn from pdata.\\n\\nWe are impressed by pmodel if:\\n\\n— Rule 1: It can generate examples that appear to have been drawn from pdata. — Rule 2: It can generate examples that are suitably different from the observa‐ tions in X. In other words, the model shouldn’t simply reproduce things it has already seen.\\n\\nLet’s now reveal the true data-generating distribution, pdata, and see how the frame‐ work applies to this example.\\n\\nAs we can see from Figure 1-6, the data-generating rule is simply a uniform distribu‐ tion over the land mass of the world, with no chance of finding a point in the sea.\\n\\nFigure 1-6. The orange box, pmodel, is an estimate of the true data-generating distribution pdata (the gray area)\\n\\nWhat Is Generative Modeling?\\n\\n|\\n\\n9\\n\\nClearly, our model pmodel is an oversimplification of pdata. Points A, B, and C show three observations generated by pmodel with varying degrees of success:\\n\\nPoint A breaks Rule 1 of the Generative Modeling Framework—it does not appear to have been generated by pdata as it’s in the middle of the sea.\\n\\nPoint B is so close to a point in the dataset that we shouldn’t be impressed that our model can generate such a point. If all the examples generated by the model were like this, it would break Rule 2 of the Generative Modeling Framework.\\n\\nPoint C can be deemed a success because it could have been generated by pdata and is suitably different from any point in the original dataset.\\n\\nThe field of generative modeling is diverse and the problem definition can take a great variety of forms. However, in most scenarios the Generative Modeling Frame‐ work captures how we should broadly think about tackling the problem.\\n\\nLet’s now build our first nontrivial example of a generative model.\\n\\nProbabilistic Generative Models Firstly, if you have never studied probability, don’t worry. To build and run many of the deep learning models that we shall see later in this book, it is not essential to have a deep understanding of statistical theory. However, to gain a full appreciation of the history of the task that we are trying to tackle, it’s worth trying to build a generative model that doesn’t rely on deep learning and instead is grounded purely in probabil‐ istic theory. This way, you will have the foundations in place to understand all genera‐ tive models, whether based on deep learning or not, from the same probabilistic standpoint.\\n\\nIf you already have a good understanding of probability, that’s great and much of the next section may already be familiar to you. How‐ ever, there is a fun example in the middle of this chapter, so be sure not to miss out on that!\\n\\nAs a first step, we shall define four key terms: sample space, density function, paramet‐ ric modeling, and maximum likelihood estimation.\\n\\n10\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nSample Space\\n\\nThe sample space is the complete set of all values an observation x can take.\\n\\nIn our previous example, the sample space consists of all points of latitude and longi‐ tude x = x1, x2 on the world map.\\n\\nFor example, x = (40.7306, –73.9352) is a point in the sample space (New York City).\\n\\nProbability Density Function A probability density function (or simply density function), p x , is a function that maps a point x in the sample space to a number between 0 and 1. The sum5 of the density function over all points in the sample space must equal 1, so that it is a well- defined probability distribution.6\\n\\nIn the world map example, the density function of our model is 0 outside of the orange box and constant inside of the box.\\n\\nWhile there is only one true density function pdata that is assumed to have generated the observable dataset, there are infinitely many density functions pmodel that we can use to estimate pdata. In order to structure our approach to finding a suitable pmodel(X) we can use a technique known as parametric modeling.\\n\\nParametric Modeling A parametric model, pθ x , is a family of density functions that can be described using a finite number of parameters, θ.\\n\\nThe family of all possible boxes you could draw on Figure 1-5 is an example of a para‐ metric model. In this case, there are four parameters: the coordinates of the bottom- left θ1, θ2 and top-right θ3, θ4 corners of the box.\\n\\n5 Or integral if the sample space is continuous.\\n\\n6 If the sample space is discrete, p(x) is simply the probability assigned to observing point x.\\n\\nProbabilistic Generative Models\\n\\n|\\n\\n11\\n\\nThus, each density function pθ x in this parametric model (i.e., each box) can be uniquely represented by four numbers, θ = θ1, θ2, θ3, θ4 .\\n\\nLikelihood The likelihood ℒ θ ∣ x of a parameter set θ is a function that measures the plausibil‐ ity of θ, given some observed point x.\\n\\nIt is defined as follows:\\n\\nℒ θ x = pθ x\\n\\nThat is, the likelihood of θ given some observed point x is defined to be the value of the density function parameterized by θ, at the point x.\\n\\nIf we have a whole dataset X of independent observations then we can write:\\n\\nℒ θ X = Π\\n\\nx ∈ X\\n\\npθ x\\n\\nSince this product can be quite computationally difficult to work with, we often use the log-likelihood ℓ instead:\\n\\nℓ θ X =\\n\\nΣ x ∈ X\\n\\nlog pθ x\\n\\nThere are statistical reasons why the likelihood is defined in this way, but it is enough for us to understand why, intuitively, this makes sense. We are simply defining the likelihood of a set of parameters θ to be equal to the probability of seeing the data under the model parameterized by θ.\\n\\nIn the world map example, an orange box that only covered the left half of the map would have a likelihood of 0—it couldn’t possibly have generated the dataset as we have observed points in the right half of the map. The orange box in Figure 1-5 has a positive likelihood as the density function is positive for all data points under this model.\\n\\nIt therefore makes intuitive sense that the focus of parametric modeling should be to find the optimal value θ of the parameter set that maximizes the likelihood of observ‐ ing the dataset X. This technique is quite appropriately called maximum likelihood estimation.\\n\\n12\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nMaximum Likelihood Estimation\\n\\nMaximum likelihood estimation is the technique that allows us to estimate θ—the set of parameters θ of a density function, pθ x , that are most likely to explain some observed data X.\\n\\nMore formally:\\n\\nθ = argmax\\n\\nℒ θ X\\n\\nθ\\n\\nθ is also called the maximum likelihood estimate (MLE).\\n\\nWe now have all the necessary terminology to start describing how we can build a probabilistic generative model.\\n\\nMost chapters in this book will contain a short story that helps to describe a particu‐ lar technique. In this chapter, we shall start by taking a trip to planet Wrodl, where our first generative modeling assignment awaits…\\n\\nHello Wrodl! The year is 2047 and you are delighted to have been appointed as the new Chief Fash‐ ion Officer (CFO) of Planet Wrodl. As CFO, it is your sole responsibility to create new and exciting fashion trends for the inhabitants of the planet to follow.\\n\\nThe Wrodlers are known to be quite particular when it comes to fashion, so your task is to generate new styles that are similar to those that already exist on the planet, but not identical.\\n\\nOn arrival, you are presented with a dataset featuring 50 observations of Wrodler fashion (Figure 1-7) and told that you have a day to come up with 10 new styles to present to the Fashion Police for inspection. You’re allowed to play around with hair‐ styles, hair color, glasses, clothing type, and clothing color to create your masterpieces.\\n\\nProbabilistic Generative Models\\n\\n|\\n\\n13\\n\\nFigure 1-7. Headshots of 50 Wrodlers7\\n\\nAs you’re a data scientist at heart, you decide to deploy a generative model to solve the problem. After a brief visit to the Intergalactic Library, you pick up a book called Generative Deep Learning and begin to read…\\n\\nTo be continued…\\n\\nYour First Probabilistic Generative Model Let’s take a closer look at the Wrodl dataset. It consists of N = 50 observations of fash‐ ions currently seen on the planet. Each observation can be described by five features, (accessoriesType, clothingColor, clothingType, hairColor, topType), as shown in Table 1-1.\\n\\nTable 1-1. The first 10 observations in the Wrodler face dataset\\n\\nface_id accessoriesType Round 0 Round 1 Sunglasses 2 Round 3 Round 4 Blank 5 Sunglasses 6 Round 7\\n\\nclothingColor White White White White White White White White\\n\\nclothingType ShirtScoopNeck Red Overall ShirtScoopNeck Blonde ShirtScoopNeck Red Overall Overall Overall ShirtScoopNeck\\n\\nhairColor\\n\\ntopType ShortHairShortFlat ShortHairFrizzle ShortHairShortFlat LongHairStraight\\n\\nSilverGray\\n\\nSilverGray NoHair Black SilverGray SilverGray\\n\\nLongHairStraight LongHairStraight ShortHairShortFlat\\n\\n7 Images sourced from https://getavataaars.com.\\n\\n14\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nface_id accessoriesType Round 8 Round 9\\n\\nclothingColor Pink PastelOrange\\n\\nclothingType Hoodie ShirtScoopNeck Blonde\\n\\nhairColor SilverGray\\n\\ntopType LongHairStraight LongHairStraight\\n\\nThe possible values for each feature include:\\n\\n7 different hairstyles (topType):\\n\\n— NoHair, LongHairBun, LongHairCurly, LongHairStraight, ShortHairShort‐\\n\\nWaved, ShortHairShortFlat, ShortHairFrizzle\\n\\n6 different hair colors (hairColor):\\n\\n— Black, Blonde, Brown, PastelPink, Red, SilverGray\\n\\n3 different kinds of glasses (accessoriesType):\\n\\n— Blank, Round, Sunglasses\\n\\n4 different kinds of clothing (clothingType):\\n\\n— Hoodie, Overall, ShirtScoopNeck, ShirtVNeck\\n\\n8 different clothing colors (clothingColor):\\n\\n— Black, Blue01, Gray01, PastelGreen, PastelOrange, Pink, Red, White\\n\\nThere are 7 × 6 × 3 × 4 × 8 = 4,032 different combinations of these features, so there are 4,032 points in the sample space.\\n\\nWe can imagine that our dataset has been generated by some distribution pdata that favors some feature values over others. For example, we can see from the images in Figure 1-7 that white clothing seems to be a popular choice, as are silver-gray hair and scoop-neck T-shirts.\\n\\nThe problem is that we do not know pdata explicitly—all we have to work with is the sample of observations X generated by pdata. The goal of generative modeling is to use these observations to build a pmodel that can accurately mimic the observations pro‐ duced by pdata.\\n\\nTo achieve this, we could simply assign a probability to each possible combination of features, based on the data we have seen. Therefore, this parametric model would have d = 4,031 parameters—one for each point in the sample space of possibilities, minus one since the value of the last parameter would be forced so that the total sums to 1. Thus the parameters of the model that we are trying to estimate are θ1, . . . , θ4031 .\\n\\nProbabilistic Generative Models\\n\\n|\\n\\n15\\n\\nThis particular class of parametric model is known as a multinomial distribution, and the maximum likelihood estimate θ j of each parameter is given by:\\n\\nθ j =\\n\\nn j N\\n\\nwhere n j is the number of times that combination j was observed in the dataset and N = 50 is the total number of observations.\\n\\nIn other words, the estimate for each parameter is just the proportion of times that its corresponding combination was observed in the dataset.\\n\\nFor example, the following combination (let’s call it combination 1) appears twice in the dataset:\\n\\n(LongHairStraight, Red, Round, ShirtScoopNeck, White)\\n\\nTherefore:\\n\\nθ1 = 2/50 = 0.04\\n\\nAs another example, the following combination (let’s call it combination 2) doesn’t appear at all in the dataset:\\n\\n(LongHairStraight, Red, Round, ShirtScoopNeck, Blue01)\\n\\nTherefore:\\n\\nθ2 = 0/50 = 0\\n\\nWe can calculate all of the θ j values in this way, to define a distribution over our sam‐ ple space. Since we can sample from this distribution, our list could potentially be called a generative model. However, it fails in one major respect: it can never generate anything that it hasn’t already seen, since θ j = 0 for any combination that wasn’t in the original dataset X.\\n\\nTo address this, we could assign an additional pseudocount of 1 to each possible com‐ bination of features. This is known as additive smoothing. Under this model, our MLE for the parameters would be:\\n\\nθ j =\\n\\nn j + 1 N + d\\n\\n16\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nNow, every single combination has a nonzero probability of being sampled, including those that were not in the original dataset. However, this still fails to be a satisfactory generative model, because the probability of observing a point not in the original dataset is just a constant. If we tried to use such a model to generate Picasso paint‐ ings, it would assign just as much weight to a random collection of colorful pixels as to a replica of a Picasso painting that differs only very slightly from a genuine painting.\\n\\nWe would ideally like our generative model to upweight areas of the sample space that it believes are more likely, due to some inherent structure learned from the data, rather than just placing all probabilistic weight on the points that are present in the dataset.\\n\\nTo achieve this, we need to choose a different parametric model.\\n\\nNaive Bayes The Naive Bayes parametric model makes use of a simple assumption that drastically reduces the number of parameters we need to estimate.\\n\\nWe make the naive assumption that each feature xj is independent of every other fea‐ ture xk.8 Relating this to the Wrodl dataset, we are assuming that the choice of hair color has no impact on the choice of clothing type, and the type of glasses that some‐ one wears has no impact on their hairstyle, for example. More formally, for all fea‐ tures xj, xk:\\n\\np x j xk = p x j\\n\\nThis is known as the Naive Bayes assumption. To apply this assumption, we first make use of the chain rule of probability to write the density function as a product of con‐ ditional probabilities:\\n\\np x = p x1, . . . , xK\\n\\n= p x2, . . . , xK x1 p x1 = p x3, . . . , xK x1, x2 p x2\\n\\n∣ x1 p x1\\n\\nK = Π k = 1\\n\\np xk x1, . . . , xk − 1\\n\\n8 When a response variable y is present, the Naive Bayes assumption states that there is conditional independ‐\\n\\nence between each pair of features xj, xk given y.\\n\\nProbabilistic Generative Models\\n\\n|\\n\\n17\\n\\nwhere K is the total number of features (i.e., 5 for the Wrodl example).\\n\\nWe now apply the Naive Bayes assumption to simplify the last line:\\n\\nK p x = Π k = 1\\n\\np xk\\n\\nThis is the Naive Bayes model. The problem is reduced to estimating the parameters θkl = p xk = l for each feature separately and multiplying these to find the probability for any possible combination.\\n\\nHow many parameters do we now need to estimate? For each feature, we need to esti‐ mate a parameter for each value that the feature can take. Therefore, in the Wrodl example, this model is defined by only 7 + 6 + 3 + 4 + 8 – 5 = 23 parameters.9\\n\\nThe maximum likelihood estimates θkl are as follows:\\n\\nθkl =\\n\\nnkl N\\n\\nwhere θkl is the number of times that the feature k takes on the value l in the dataset and N = 50 is the total number of observations.\\n\\nTable 1-2 shows the calculated parameters for the Wrodl dataset.\\n\\nTable 1-2. The MLEs for the parameters under the Naive Bayes model\\n\\ntopType\\n\\nNoHair LongHairBun LongHairCurly LongHairStraight ShortHairShortWaved ShortHairShortFlat ShortHairFrizzle Grand Total\\n\\nn\\n\\n7 0 1 23 1 11 7 50\\n\\n^ θ 0.14 0.00 0.02 0.46 0.02 0.22 0.14 1.00\\n\\nhairColor\\n\\nBlack Blonde Brown PastelPink Red SilverGrey Grand Total\\n\\nn\\n\\n7 6 2 3 8 24 50\\n\\n^ θ 0.14 0.12 0.04 0.06 0.16 0.48 1.00\\n\\nclothingColor\\n\\nBlack Blue01 Grey01 PastelGreen PastelOrange Pink Red White Grand Total\\n\\nn\\n\\n0 4 10 5 2 4 3 22 50\\n\\n^ θ 0.00 0.08 0.20 0.10 0.04 0.08 0.06 0.44 1.00\\n\\n9 The –5 is due to the fact that the last parameter for each feature is forced to ensure that the sum of the param‐\\n\\neters for this feature sums to 1.\\n\\n18\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\naccessoriesType\\n\\nBlank Round Sunglasses Grand Total\\n\\nn\\n\\n11 22 17 50\\n\\n^ θ 0.22 0.44 0.34 1.00\\n\\nclothingType\\n\\nHoodie Overall ShirtScoopNeck ShirtVNeck Grand Total\\n\\nn\\n\\n7 18 19 6 50\\n\\n^ θ 0.14 0.36 0.38 0.12 1.00\\n\\nTo calculate the probability of the model generating some observation x, we simply multiply together the individual feature probabilities. For example:\\n\\np(LongHairStraight, Red, Round, ShirtScoopNeck, White)\\n\\n= p(LongHairStraight) × p(Red) × p(Round) × p(ShirtScoopNeck) × p(White) = 0.46 × 0.16 × 0.44 × 0.38 × 0.44 = 0.0054\\n\\nNotice that this combination doesn’t appear in the original dataset, but our model still allocates it a nonzero probability, so it is still able to be generated. Also, it has a higher probability of being sampled than, say, (LongHairStraight, Red, Round, ShirtScoop‐ Neck, White), because white clothing appears more often than blue clothing in the dataset.\\n\\nTherefore, a Naive Bayes model is able to learn some structure from the data and use this to generate new examples that were not seen in the original dataset. The model has estimated the probability of seeing each feature value independently, so that under the Naive Bayes assumption we can multiply these probabilities to build our full density function, pθ(x).\\n\\nFigure 1-8 shows 10 observations sampled from the model.\\n\\nProbabilistic Generative Models\\n\\n|\\n\\n19\\n\\nFigure 1-8. Ten new Wrodl styles, generated using the Naive Bayes model\\n\\nFor this simple problem, the Naive Bayes assumption that each feature is independent of every other feature is reasonable and therefore produces a good generative model.\\n\\nNow let’s see what happens when this assumption breaks down\\n\\nHello Wrodl! Continued You feel a certain sense of pride as you look upon the 10 new creations generated by your Naive Bayes model. Glowing with success, you turn your attention to another planet’s fashion dilemma—but this time the problem isn’t quite as simple.\\n\\nOn the conveniently named Planet Pixel, the dataset you are provided with doesn’t consist of the five high-level features that you saw on Wrodl (hairColor, accessories‐ Type, etc.), but instead contains just the values of the 32 × 32 pixels that make up each image. Thus each observation now has 32 × 32 = 1,024 features and each feature can take any of 256 values (the individual colors in the palette).\\n\\nImages from the new dataset are shown in Figure 1-9, and a sample of the pixel values for the first 10 observations appears in Table 1-3.\\n\\n20\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nFigure 1-9. Fashions on Planet Pixel\\n\\nTable 1-3. The values of pixels 458–467 from the first 10 observations on Planet Pixel\\n\\nface_id px_458 px_459 px_460 px_461 px_462 px_463 px_464 px_465 px_466 px_467 0 1 2 3 4 5 6 7 8 9\\n\\n49 43 37 54 2 44 12 36 54 49\\n\\n14 10 12 9 2 15 9 9 11 17\\n\\n14 10 12 9 5 15 2 9 11 17\\n\\n19 17 14 14 2 21 31 13 16 19\\n\\n7 9 11 10 4 14 16 11 10 12\\n\\n5 3 4 4 4 3 3 4 4 6\\n\\n5 3 4 4 4 3 3 4 4 6\\n\\n12 18 6 16 4 4 16 12 19 22\\n\\n19 17 14 14 2 21 31 13 16 19\\n\\n14 10 12 9 5 15 2 9 11 17\\n\\nYou decide to try your trusty Naive Bayes model once more, this time trained on the pixel dataset. The model will estimate the maximum likelihood parameters that gov‐ ern the distribution of the color of each pixel so that you are able to sample from this distribution to generate new observations. However, when you do so, it is clear that something has gone very wrong.\\n\\nRather than producing novel fashions, the model outputs 10 very similar images that have no distinguishable accessories or clear blocks of hair or clothing color (Figure 1-10). Why is this?\\n\\nProbabilistic Generative Models\\n\\n|\\n\\n21\\n\\nFigure 1-10. Ten new Planet Pixel styles, generated by the Naive Bayes model\\n\\nThe Challenges of Generative Modeling First, since the Naive Bayes model is sampling pixels independently, it has no way of knowing that two adjacent pixels are probably quite similar in shade, as they are part of the same item of clothing, for example. The model can generate the facial color and mouth, as all of these pixels in the training set are roughly the same shade in each observation; however for the T-shirt pixels, each pixel is sampled at random from a variety of different colors in the training set, with no regard to the colors that have been sampled in neighboring pixels. Additionally, there is no mechanism for pixels near the eyes to form circular glasses shapes, or red pixels near the top of the image to exhibit a wavy pattern to represent a particular hairstyle, for example.\\n\\nSecond, there are now an incomprehensibly vast number of possible observations in the sample space. Only a tiny proportion of these are recognizable faces, and an even smaller subset are faces that adhere to the fashion rules on Planet Pixel. Therefore, if our Naive Bayes model is working directly with the highly correlated pixel values, the chance of it finding a satisfying combination of values is incredibly small.\\n\\nIn summary, on Planet Wrodl individual features are independent and the sample space is relatively small, so Naive Bayes works well. On Planet Pixel, the assumption that every pixel value is independent of every other pixel value clearly doesn’t hold. Pixel values are highly correlated and the sample space is vast, so finding a valid face by sampling pixels independently is almost impossible. This explains why Naive Bayes models cannot be expected to work well on raw image data.\\n\\n22\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nThis example highlights the two key challenges that a generative model must over‐ come in order to be successful.\\n\\nGenerative Modeling Challenges\\n\\nHow does the model cope with the high degree of conditional dependence between features?\\n\\nHow does the model find one of the tiny proportion of satisfying possible gener‐ ated observations among a high-dimensional sample space?\\n\\nDeep learning is the key to solving both of these challenges.\\n\\nWe need a model that can infer relevant structure from the data, rather than being told which assumptions to make in advance. This is exactly where deep learning excels and is one of the key reasons why the technique has driven the major recent advances in generative modeling.\\n\\nThe fact that deep learning can form its own features in a lower-dimensional space means that it is a form of representation learning. It is important to understand the key concepts of representation learning before we tackle deep learning in the next chapter.\\n\\nRepresentation Learning The core idea behind representation learning is that instead of trying to model the high-dimensional sample space directly, we should instead describe each observation in the training set using some low-dimensional latent space and then learn a mapping function that can take a point in the latent space and map it to a point in the original domain. In other words, each point in the latent space is the representation of some high-dimensional image.\\n\\nWhat does this mean in practice? Let’s suppose we have a training set consisting of grayscale images of biscuit tins (Figure 1-11).\\n\\nThe Challenges of Generative Modeling\\n\\n|\\n\\n23\\n\\nFigure 1-11. The biscuit tin dataset\\n\\nTo us, it is obvious that there are two features that can uniquely represent each of these tins: the height and width of the tin. Given a height and width, we could draw the corresponding tin, even if its image wasn’t in the training set. However, this is not so easy for a machine—it would first need to establish that height and width are the two latent space dimensions that best describe this dataset, then learn the mapping function, f, that can take a point in this space and map it to a grayscale biscuit tin image. The resulting latent space of biscuit tins and generation process are shown in Figure 1-12.\\n\\nDeep learning gives us the ability to learn the often highly complex mapping function f in a variety of ways. We shall explore some of the most important techniques in later chapters of this book. For now, it is enough to understand at a high level what repre‐ sentation learning is trying to achieve.\\n\\nOne of the advantages of using representation learning is that we can perform opera‐ tions within the more manageable latent space that affect high-level properties of the image. It is not obvious how to adjust the shading of every single pixel to make a given biscuit tin image taller. However, in the latent space, it’s simply a case of adding 1 to the height latent dimension, then applying the mapping function to return to the image domain. We shall see an explicit example of this in the next chapter, applied not to biscuit tins but to faces.\\n\\n24\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nFigure 1-12. The latent space of biscuit tins and the function f that maps a point in the latent space to the original image domain\\n\\nRepresentation learning comes so naturally to us as humans that you may never have stopped to think just how amazing it is that we can do it so effortlessly. Suppose you wanted to describe your appearance to someone who was looking for you in a crowd of people and didn’t know what you looked like. You wouldn’t start by stating the color of pixel 1 of your hair, then pixel 2, then pixel 3, etc. Instead, you would make the reasonable assumption that the other person has a general idea of what an average human looks like, then amend this baseline with features that describe groups of pix‐ els, such as I have very blonde hair or I wear glasses. With no more than 10 or so of these statements, the person would be able to map the description back into pixels to generate an image of you in their head. The image wouldn’t be perfect, but it would be a close enough likeness to your actual appearance for them to find you among pos‐ sibly hundreds of other people, even if they’ve never seen you before.\\n\\nNote that representation learning doesn’t just assign values to a given set of features such as blondeness of hair, height, etc., for some given image. The power of represen‐ tation learning is that it actually learns which features are most important for it to describe the given observations and how to generate these features from the raw data. Mathematically speaking, it tries to find the highly nonlinear manifold on which the data lies and then establish the dimensions required to fully describe this space. This is shown in Figure 1-13.\\n\\nThe Challenges of Generative Modeling\\n\\n|\\n\\n25\\n\\nFigure 1-13. The cube represents the extremely high-dimensional space of all images; rep‐ resentation learning tries to find the lower-dimensional latent subspace or manifold on which particular kinds of image lie (for example, the dog manifold)\\n\\nIn summary, representation learning establishes the most relevant high-level features that describe how groups of pixels are displayed so that is it likely that any point in the latent space is the representation of a well-formed image. By tweaking the values of features in the latent space we can produce novel representations that, when\\n\\n26\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nmapped back to the original image domain, have a much better chance of looking real than if we’d tried to work directly with the individual raw pixels.\\n\\nNow that you have an understanding of representation learning, which forms the backbone of many of the generative deep learning examples in this book, all that remains is to set up your environment so that you can begin building generative deep learning models of your own.\\n\\nSetting Up Your Environment Throughout this book, there are many worked examples of how to build the models that we will be discussing in the text.\\n\\nTo get access to these examples, you’ll need to clone the Git repository that accompa‐ nies this book. Git is an open source version control system and will allow you to copy the code locally so that you can run the notebooks on your own machine, or perhaps in a cloud-based environment. You may already have this installed, but if not, follow the instructions relevant to your operating system.\\n\\nTo clone the repository for this book, navigate to the folder where you would like to store the files and type the following into your terminal:\\n\\ngit clone https://github.com/davidADSP/GDL_code.git\\n\\nAlways make sure that you have the most up-to-date version of the codebase by run‐ ning the following command:\\n\\ngit pull\\n\\nYou should now be able to see the files in a folder on your machine.\\n\\nNext, you need to set up a virtual environment. This is simply a folder into which you’ll install a fresh copy of Python and all of the packages that we will be using in this book. This way, you can be sure that your system version of Python isn’t affected by any of the libraries that we will be using.\\n\\nIf you are using Anaconda, you can set up a virtual environment as follows:\\n\\nconda create -n generative python=3.6 ipykernel\\n\\nIf not, you can install virtualenv and virtualenvwrapper with the command:10\\n\\npip install virtualenv virtualenvwrapper\\n\\n10 For full instructions on installing virtualenvwrapper, consult the documentation.\\n\\nSetting Up Your Environment\\n\\n|\\n\\n27\\n\\nYou will also need to add the following lines to your shell startup script (e.g., .bash_profile):\\n\\nexport WORKON_HOME=$HOME/.virtualenvs export VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3 source /usr/local/bin/virtualenvwrapper.sh\\n\\nThe location where your virtual environments will be stored\\n\\nThe default version of Python to use when a virtual environment is created— make sure this points at Python 3, rather than Python 2.\\n\\nReloads the virtualenvwrapper initialization script\\n\\nTo create a virtual environment called generative, simply enter the following into your terminal:\\n\\nmkvirtualenv generative\\n\\nYou’ll know that you’re inside the virtual environment because your terminal will show (generative) at the start of the prompt.\\n\\nNow you can go ahead and install all the packages that we’ll be using in this book with the following command:\\n\\npip install -r requirements.txt\\n\\nThroughout this book, we will use Python 3. The requirements.txt file contains the names and version numbers of all the packages that you will need to run the examples.\\n\\nTo check everything works as expected, from inside your virtual environment type python into your terminal and then try to import Keras (a deep learning library that we will be using extensively in this book). You should see a Python 3 prompt, with Keras reporting that it is using the TensorFlow backend as shown in Figure 1-14.\\n\\nFigure 1-14. Setting up your environment\\n\\nFinally, you will need to ensure you are set up to access your virtual environment through Jupyter notebooks on your machine. Jupyter is a way to interactively code in\\n\\n28\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nPython through your browser and is a great option for developing new ideas and sharing code. Most of the examples in this book are written using Jupyter notebooks.\\n\\nTo do this, run the following command from your terminal inside your virtual environment:\\n\\npython -m ipykernel install --user --name generative\\n\\nThis gives you access to the virtual environment that you’ve just set up (genera tive) inside Jupyter notebooks.\\n\\nTo check that it has installed correctly, navigate in your terminal to the folder where you have cloned the book repository and type:\\n\\njupyter notebook\\n\\nA window should open in your browser showing a screen similar to Figure 1-15. Click the notebook you wish to run and, from the Kernel → Change kernel dropdown, select the generative virtual environment.\\n\\nFigure 1-15. Jupyter notebook\\n\\nYou are now ready to start building generative deep neural networks.\\n\\nSummary This chapter introduced the field of generative modeling, an important branch of machine learning that complements the more widely studied discriminative model‐ ing. Our first basic example of a generative model utilized the Naive Bayes assump‐ tion to produce a probability distribution that was able to represent inherent structure in the data and generate examples outside of the training set. We also saw how these kinds of basic models can fail as the complexity of the generative task grows, and analyzed the general challenges associated with generative modeling. Finally, we took our first look at representation learning, an important concept that forms the core of many generative models.\\n\\nSummary\\n\\n|\\n\\n29\\n\\nIn Chapter 2, we will begin our exploration of deep learning and see how to use Keras to build models that can perform discriminative modeling tasks. This will give us the necessary foundations to go on to tackle generative deep learning in later chapters.\\n\\n30\\n\\n|\\n\\nChapter 1: Generative Modeling\\n\\nCHAPTER 2 Deep Learning\\n\\nLet’s start with a basic definition of deep learning:\\n\\nDeep learning is a class of machine learning algorithm that uses multiple stacked layers of processing units to learn high-level representations from unstructured data.\\n\\nTo understand deep learning fully, and particularly why it is so useful within genera‐ tive modeling, we need to delve into this definition a bit further. First, what do we mean by “unstructured data” and its counterpart, “structured data”?\\n\\nStructured and Unstructured Data Many types of machine learning algorithm require structured, tabular data as input, arranged into columns of features that describe each observation. For example, a per‐ son’s age, income, and number of website visits in the last month are all features that could help to predict if the person will subscribe to a particular online service in the coming month. We could use a structured table of these features to train a logistic regression, random forest, or XGBoost model to predict the binary response variable —did the person subscribe (1) or not (0)? Here, each individual feature contains a nugget of information about the observation, and the model would learn how these features interact to influence the response.\\n\\nUnstructured data refers to any data that is not naturally arranged into columns of features, such as images, audio, and text. There is of course spatial structure to an image, temporal structure to a recording, and both spatial and temporal structure to video data, but since the data does not arrive in columns of features, it is considered unstructured, as shown in Figure 2-1.\\n\\n31\\n\\nFigure 2-1. The difference between structured and unstructured data\\n\\nWhen our data is unstructured, individual pixels, frequencies, or characters are almost entirely uninformative. For example, knowing that pixel 234 of an image is a muddy shade of brown doesn’t really help identify if the image is of a house or a dog, and knowing that character 24 of a sentence is an e doesn’t help predict if the text is about football or politics.\\n\\nPixels or characters are really just the dimples of the canvas into which higher-level informative features, such as an image of a chimney or the word striker, are embed‐ ded. If the chimney in the image were placed on the other side of the house, the image would still contain a chimney, but this information would now be carried by completely different pixels. If the word striker appeared slightly earlier or later in the text, the text would still be about football, but different character positions would provide this information. The granularity of the data combined with the high degree of spatial dependence destroys the concept of the pixel or character as an informative feature in its own right.\\n\\nFor this reason, if we train logistic regression, random forest, or XGBoost algorithms on raw pixel values, the trained model will often perform poorly for all but the sim‐ plest of classification tasks. These models rely on the input features to be informative and not spatially dependent. A deep learning model, on the other hand, can learn how to build high-level informative features by itself, directly from the unstructured data.\\n\\nDeep learning can be applied to structured data, but its real power, especially with regard to generative modeling, comes from its ability to work with unstructured data. Most often, we want to generate unstructured data such as new images or original strings of text, which is why deep learning has had such a profound impact on the field of generative modeling.\\n\\n32\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nDeep Neural Networks The majority of deep learning systems are artificial neural networks (ANNs, or just neural networks for short) with multiple stacked hidden layers. For this reason, deep learning has now almost become synonymous with deep neural networks. However, it is important to point out that any system that employs many layers to learn high level representations of the input data is also a form of deep learning (e.g., deep belief net‐ works and deep Boltzmann machines).\\n\\nLet’s start by taking a high-level look at how a deep neural network can make a pre‐ diction about a given input.\\n\\nA deep neural network consists of a series of stacked layers. Each layer contains units, that are connected to the previous layer’s units through a set of weights. As we shall see, there are many different types of layer, but one of the most common is the dense layer that connects all units in the layer directly to every unit in the previous layer. By stacking layers, the units in each subsequent layer can represent increasingly sophisti‐ cated aspects of the original input.\\n\\nFigure 2-2. Deep learning conceptual diagram\\n\\nFor example, in Figure 2-2, layer 1 consists of units that activate more strongly when they detect particular basic properties of the input image, such as edges. The output from these units is then passed to the units of layer 2, which are able to use this infor‐ mation to detect slightly more complex features—and so on, through the network. The final output layer is the culmination of this process, where the network outputs a set of numbers that can be converted into probabilities, to represent the chance that the original input belongs to one of n categories.\\n\\nDeep Neural Networks\\n\\n|\\n\\n33\\n\\nThe magic of deep neural networks lies in finding the set of weights for each layer that results in the most accurate predictions. The process of finding these weights is what we mean by training the network.\\n\\nDuring the training process, batches of images are passed through the network and the output is compared to the ground truth. The error in the prediction is then propagated backward through the network, adjusting each set of weights a small amount in the direction that improves the prediction most significantly. This process is appropriately called backpropagation. Gradually, each unit becomes skilled at iden‐ tifying a particular feature that ultimately helps the network to make better predictions.\\n\\nDeep neural networks can have any number of middle or hidden layers. For example, ResNet,1 designed for image recognition, contains 152 layers. We shall see in Chap‐ ter 3 that we can use deep neural networks to influence high-level features of an image, such as hair color or expression of a face, by manually tweaking the values of these hidden layers. This is only possible because the deeper layers of the network are capturing high-level features that we can work with directly.\\n\\nNext, we’ll dive straight into the practical side of deep learning and get set up with Keras and TensorFlow, the two libraries that will enable you to start building your own generative deep neural networks.\\n\\nKeras and TensorFlow Keras is a high-level Python library for building neural networks and is the core library that we shall be using in this book. It is extremely flexible and has a very user- friendly API, making it an ideal choice for getting started with deep learning. More‐ over, Keras provides numerous useful building blocks that can be plugged together to create highly complex deep learning architectures through its functional API.\\n\\nKeras is not the library that performs the low-level array operations required to train neural networks. Instead Keras utilizes one of three backend libraries for this pur‐ pose: TensorFlow, CNTK, or Theano. You are free to choose whichever you are most comfortable with, or whichever library works fastest for a particular network archi‐ tecture. For most purposes, it doesn’t matter which you choose as you usually won’t be coding directly using the underlying backend framework. In this book we use Ten‐ sorFlow as it is the most widely adopted and best documented of the three.\\n\\nTensorFlow is an open-source Python library for machine learning, developed by Google. It is now one of the most utilized frameworks for building machine learning solutions, with particular emphasis on the manipulation of tensors (hence the name).\\n\\n1 Kaiming He et al., “Deep Residual Learning for Image Recognition,” 10 December 2015, https://arxiv.org/abs/\\n\\n1512.03385.\\n\\n34\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nWithin the context of deep learning, tensors are simply multidimensional arrays that store the data as it flows through the network. As we shall see, understanding how each layer of a neural network changes the shape of the data as it flows through the network is a key part of truly understanding the mechanics of deep learning.\\n\\nIf you are just getting started with deep learning, I highly recommend that you choose Keras with a TensorFlow backend as your toolkit. These two libraries are a powerful combination that will allow you to build any network that you can think of in a production environment, while also giving you the easy-to-learn API that is so important for rapid development of new ideas and concepts.\\n\\nYour First Deep Neural Network Let’s start by seeing how easy it is to build a deep neural network in Keras.\\n\\nWe will be working through the Jupyter notebook in the book repository called 02_01_deep_learning_deep_neural_network.ipynb.\\n\\nLoading the Data For this example we will be using the CIFAR-10 dataset, a collection of 60,000 32 × 32–pixel color images that comes bundled with Keras out of the box. Each image is classified into exactly one of 10 classes, as shown in Figure 2-3.\\n\\nThe following code loads and scales the data:\\n\\nimport numpy as np from keras.utils import to_categorical from keras.datasets import cifar10\\n\\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\\n\\nNUM_CLASSES = 10\\n\\nx_train = x_train.astype(\\'float32\\') / 255.0 x_test = x_test.astype(\\'float32\\') / 255.0\\n\\ny_train = to_categorical(y_train, NUM_CLASSES) y_test = to_categorical(y_test, NUM_CLASSES)\\n\\nLoads the CIFAR-10 dataset. x_train and x_test are numpy arrays of shape [50000, 32, 32, 3] and [10000, 32, 32, 3], respectively. y_train and y_test are numpy arrays with shape [50000, 1] and [10000, 1], respectively, containing the integer labels in the range 0 to 9 for the class of each image.\\n\\nYour First Deep Neural Network\\n\\n|\\n\\n35\\n\\nBy default the image data consists of integers between 0 and 255 for each pixel channel. Neural networks work best when each input is inside the range –1 to 1, so we need to divide by 255.\\n\\nWe also need to change the integer labeling of the images to one-hot-encoded vectors. If the class integer label of an image is i, then its one-hot encoding is a vector of length 10 (the number of classes) that has 0s in all but the ith element, which is 1. The new shapes of y_train and y_test are therefore [50000, 10] and [10000, 10] respectively.\\n\\nFigure 2-3. Example images from the CIFAR-10 dataset2\\n\\n2 Source: Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images,” 2009, https://\\n\\nwww.cs.toronto.edu/~kriz/cifar.html.\\n\\n36\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nIt’s worth noting the shape of the image data in x_train: [50000, 32, 32, 3]. The first dimension of this array references the index of the image in the dataset, the sec‐ ond and third relate to the size of the image, and the last is the channel (i.e., red, green, or blue, since these are RGB images). There are no columns or rows in this dataset; instead, this is a tensor with four dimensions. For example, the following entry refers to the green channel (1) value of the pixel in the (12,13) position of image 54:\\n\\nx_train[54, 12, 13, 1] # 0.36862746\\n\\nBuilding the Model In Keras there are two ways to define the structure of your neural network: as a Sequential model or using the Functional API.\\n\\nA Sequential model is useful for quickly defining a linear stack of layers (i.e., where one layer follows on directly from the previous layer without any branching). How‐ ever, many of the models in this book require that the output from a layer is passed to multiple separate layers beneath it, or conversely, that a layer receives input from multiple layers above it.\\n\\nTo be able to build networks with branches, we need to use the Functional API, which is a lot more flexible. I recommend that even if you are just starting out building lin‐ ear models with Keras, you still use the Functional API rather than Sequential mod‐ els, since it will serve you better in the long run as your neural networks become more architecturally complex. The Functional API will give you complete freedom over the design of your deep neural network.\\n\\nTo demonstrate the difference between the two methods, Examples 2-1 and 2-2 show the same network coded using a Sequential model and the Functional API. Feel free to try both and observe that they give the same result.\\n\\nExample 2-1. The architecture using a Sequential model\\n\\nfrom keras.models import Sequential from keras.layers import Flatten, Dense\\n\\nmodel = Sequential([ Dense(200, activation = \\'relu\\', input_shape=(32, 32, 3)), Flatten(), Dense(150, activation = \\'relu\\'), Dense(10, activation = \\'softmax\\'), ])\\n\\nYour First Deep Neural Network\\n\\n|\\n\\n37\\n\\nExample 2-2. The architecture using the Functional API\\n\\nfrom keras.layers import Input, Flatten, Dense from keras.models import Model\\n\\ninput_layer = Input(shape=(32,32, 3))\\n\\nx = Flatten()(input_layer)\\n\\nx = Dense(units=200, activation = \\'relu\\')(x) x = Dense(units=150, activation = \\'relu\\')(x)\\n\\noutput_layer = Dense(units=10, activation = \\'softmax\\')(x)\\n\\nmodel = Model(input_layer, output_layer)>\\n\\nHere, we are using three different types of layer: Input, Flatten, and Dense.\\n\\nThe Input layer is an entry point into the network. We tell the network the shape of each data element to expect as a tuple. Notice that we do not specify the batch size; this isn’t necessary as we can pass any number of images into the Input layer simulta‐ neously. We do not need to explicitly state the batch size in the Input layer definition.\\n\\nNext we flatten this input into a vector, using a Flatten layer. This results in a vector of length 3,072 (= 32 × 32 × 3). The reason we do this is because the subsequent Dense layer requires that its input is flat, rather than a multidimensional array. As we shall see later, other layer types require multidimensional arrays as input, so you need to be aware of the required input and output shape of each layer type to understand when it is necessary to use Flatten.\\n\\nThe Dense layer is perhaps the most fundamental layer type in any neural network. It contains a given number of units that are densely connected to the previous layer— that is, every unit in the layer is connected to every unit in the previous layer, through a single connection that carries a weight (which can be positive or negative). The out‐ put from a given unit is the weighted sum of the input it receives from the previous layer, which is then passed through a nonlinear activation function before being sent to the following layer. The activation function is critical to ensure the neural network is able to learn complex functions and doesn’t just output a linear combination of its input.\\n\\nThere are many kinds of activation function, but the three most important are ReLU, sigmoid, and softmax.\\n\\nThe ReLU (rectified linear unit) activation function is defined to be zero if the input is negative and is otherwise equal to the input. The LeakyReLU activation function is very similar to ReLU, with one key difference: whereas the ReLU activation function returns zero for input values less than zero, the LeakyReLU function returns a small\\n\\n38\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nnegative number proportional to the input. ReLU units can sometimes die if they always output zero, because of a large bias toward negative values preactivation. In this case, the gradient is zero and therefore no error is propagated back through this unit. LeakyReLU activations fix the issue by always ensuring the gradient is nonzero. ReLU-based functions are now established to be the most reliable activations to use between the layers of a deep network to encourage stable training.\\n\\nThe sigmoid activation is useful if you wish the output from the layer to be scaled between 0 and 1—for example, for binary classification problems with one output unit or multilabel classification problems, where each observation can belong to more than one class. Figure 2-4 shows ReLU, LeakyReLU, and sigmoid activation functions side by side for comparison.\\n\\nFigure 2-4. The ReLU, LeakyReLU, and sigmoid activation functions\\n\\nThe softmax activation is useful if you want the total sum of the output from the layer to equal 1, for example, for multiclass classification problems where each observation only belongs to exactly one class. It is defined as:\\n\\nyi =\\n\\ne\\n\\nx i\\n\\nJ ∑ j = 1\\n\\ne\\n\\nx\\n\\nj\\n\\nHere, J is the total number of units in the layer. In our neural network, we use a soft‐ max activation in the final layer to ensure that the output is a set of 10 probabilities that sum to 1, which can be interpreted as the chance that the image belongs to each class.\\n\\nIn Keras, activation functions can also be defined in a separate layer as follows:\\n\\nx = Dense(units=200)(x) x = Activation(\\'relu\\')(x)\\n\\nThis is equivalent to:\\n\\nx = Dense(units=200, activation = \\'relu\\')(x)\\n\\nYour First Deep Neural Network\\n\\n|\\n\\n39\\n\\nIn our example, we pass the input through two dense hidden layers, the first with 200 units and the second with 150, both with ReLU activation functions. A diagram of the total network is shown in Figure 2-5.\\n\\nFigure 2-5. A diagram of the neural network trained on CIFAR-10 data\\n\\nThe final step is to define the model itself, using the Model class. In Keras a model is defined by the input and output layers. In our case, we have one input layer that we defined earlier, and the output layer is the final Dense layer of 10 units. It is also possi‐ ble to define models with multiple input and output layers; we shall see this in action later in the book.\\n\\nIn our example, as required, the shape of our Input layer matches the shape of x_train and the shape of our Dense output layer matches the shape of y_train. To illustrate this, we can use the model.summary() method to see the shape of the net‐ work at each layer as shown in Figure 2-6.\\n\\n40\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nFigure 2-6. The summary of the model\\n\\nNotice how Keras uses None as a marker to show that it doesn’t yet know the number of observations that will be passed into the network. In fact, it doesn’t need to; we could just as easily pass one observation through the network at a time as 1,000. That’s because tensor operations are conducted across all observations simultaneously using linear algebra—this is the part handled by TensorFlow. It is also the reason why you get a performance increase when training deep neural networks on GPUs instead of CPUs: GPUs are optimized for large tensor multiplications since these calculations are also necessary for complex graphics manipulation.\\n\\nThe summary method also gives the number of parameters (weights) that will be trained at each layer. If ever you find that your model is training too slowly, check the summary to see if there are any layers that contain a huge number of weights. If so, you should consider whether the number of units in the layer could be reduced to speed up training.\\n\\nCompiling the Model In this step, we compile the model with an optimizer and a loss function:\\n\\nfrom keras.optimizers import Adam\\n\\nopt = Adam(lr=0.0005) model.compile(loss=\\'categorical_crossentropy\\', optimizer=opt, metrics=[\\'accuracy\\'])\\n\\nThe loss function is used by the neural network to compare its predicted output to the ground truth. It returns a single number for each observation; the greater this number, the worse the network has performed for this observation.\\n\\nYour First Deep Neural Network\\n\\n|\\n\\n41\\n\\nKeras provides many built-in loss functions to choose from, or you can create your own. Three of the most commonly used are mean squared error, categorical cross- entropy, and binary cross-entropy. It is important to understand when it is appropri‐ ate to use each.\\n\\nIf your neural network is designed to solve a regression problem (i.e., the output is continuous), then you can use the mean squared error loss. This is the mean of the squared difference between the ground truth yi and predicted value pi of each output unit, where the mean is taken over all n output units:\\n\\nMSE =\\n\\nn\\n\\n1 n ∑\\n\\ni = 1\\n\\nyi − pi\\n\\n2\\n\\nIf you are working on a classification problem where each observation only belongs to one class, then categorical cross-entropy is the correct loss function. This is defined as follows:\\n\\nn − ∑ i = 1\\n\\nyi log pi\\n\\nFinally, if you are working on a binary classification problem with one output unit, or a multilabel problem where each observation can belong to multiple classes simulta‐ neously, you should use binary cross-entropy:\\n\\n−\\n\\nn\\n\\n1 n ∑\\n\\ni = 1\\n\\nyi log pi + 1 − yi\\n\\nlog 1 − pi\\n\\nThe optimizer is the algorithm that will be used to update the weights in the neural network based on the gradient of the loss function. One of the most commonly used and stable optimizers is Adam.3 In most cases, you shouldn’t need to tweak the default parameters of the Adam optimizer, except for the learning rate. The greater the learn‐ ing rate, the larger the change in weights at each training step. While training is ini‐ tially faster with a large learning rate, the downside is that it may result in less stable training and may not find the minima of the loss function. This is a parameter that you may want to tune or adjust during training.\\n\\n3 Diederik Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,” 22 December 2014, https://\\n\\narxiv.org/abs/1412.6980v8.\\n\\n42\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nAnother common optimizer that you may come across is RMSProp. Again, you shouldn’t need to adjust the parameters of this optimizer too much, but it is worth reading the Keras documentation to understand the role of each parameter.\\n\\nWe pass both the loss function and the optimizer into the compile method of the model, as well as a metrics parameter where we can specify any additional metrics that we would like reporting on during training, such as accuracy.\\n\\nTraining the Model Thus far, we haven’t shown the model any data and have just set up the architecture and compiled the model with a loss function and optimizer.\\n\\nTo train the model, simply call the fit method, as shown here:\\n\\nmodel.fit(x_train , y_train , batch_size = 32 , epochs = 10 , shuffle = True )\\n\\nThe raw image data.\\n\\nThe one-hot-encoded class labels.\\n\\nThe batch_size determines how many observations will be passed to the net‐ work at each training step.\\n\\nThe epochs determine how many times the network will be shown the full train‐ ing data.\\n\\nIf shuffle = True, the batches will be drawn randomly without replacement from the training data at each training step.\\n\\nThis will start training a deep neural network to predict the category of an image from the CIFAR-10 dataset.\\n\\nThe training process works as follows. First, the weights of the network are initialized to small random values. Then the network performs a series of training steps.\\n\\nAt each training step, one batch of images is passed through the network and the errors are backpropagated to update the weights. The batch_size determines how many images are in each training step batch. The larger the batch size, the more sta‐ ble the gradient calculation, but the slower each training step. It would be far too time-consuming and computationally intensive to use the entire dataset to calculate the gradient at each training step, so generally a batch size between 32 and 256 is\\n\\nYour First Deep Neural Network\\n\\n|\\n\\n43\\n\\nused. It is also now recommended practice to increase the batch size as training progresses.4\\n\\nThis continues until all observations in the dataset have been seen once. This com‐ pletes the first epoch. The data is then passed through the network again in batches as part of the second epoch. This process repeats until the specified number of epochs have elapsed.\\n\\nDuring training, Keras outputs the progress of the procedure, as shown in Figure 2-7. We can see that the training dataset of 50,000 observations has been shown to the net‐ work 10 times (i.e., over 10 epochs), at a rate of approximately 160 microseconds per observation. The categorical cross-entropy loss has fallen from 1.842 to 1.357, result‐ ing in an accuracy increase from 33.5% after the first epoch to 51.9% after the tenth epoch.\\n\\nFigure 2-7. The output from the fit method\\n\\nEvaluating the Model We know the model achieves an accuracy of 51.9% on the training set, but how does it perform on data it has never seen?\\n\\nTo answer this question we can use the evaluate method provided by Keras:\\n\\nmodel.evaluate(x_test, y_test)\\n\\n4 Samuel L. Smith et al., “Don’t Decay the Learning Rate, Increase the Batch Size,” 1 November 2017, https://\\n\\narxiv.org/abs/1711.00489.\\n\\n44\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nFigure 2-8 shows the output from this method.\\n\\nFigure 2-8. The output from the evaluate method\\n\\nThe output from this method is a list of the metrics we are monitoring: categorical cross-entropy and accuracy. We can see that model accuracy is still 49.0% even on images that it has never seen before. Note that if the model was guessing randomly, it would achieve approximately 10% accuracy (because there are 10 classes), so 50% is a good result given that we have used a very basic neural network.\\n\\nWe can view some of the predictions on the test set using the predict method:\\n\\nCLASSES = np.array([\\'airplane\\', \\'automobile\\', \\'bird\\', \\'cat\\', \\'deer\\', \\'dog\\' , \\'frog\\', \\'horse\\', \\'ship\\', \\'truck\\'])\\n\\npreds = model.predict(x_test) preds_single = CLASSES[np.argmax(preds, axis = -1)] actual_single = CLASSES[np.argmax(y_test, axis = -1)]\\n\\npreds is an array of shape [10000, 10]—i.e., a vector of 10 class probabilities for each observation.\\n\\nWe convert this array of probabilities back into a single prediction using numpy’s argmax function. Here, axis = –1 tells the function to collapse the array over the last dimension (the classes dimension), so that the shape of preds_single is then [10000, 1].\\n\\nWe can view some of the images alongside their labels and predictions with the fol‐ lowing code. As expected, around half are correct:\\n\\nimport matplotlib.pyplot as plt\\n\\nn_to_show = 10 indices = np.random.choice(range(len(x_test)), n_to_show)\\n\\nfig = plt.figure(figsize=(15, 3)) fig.subplots_adjust(hspace=0.4, wspace=0.4)\\n\\nfor i, idx in enumerate(indices): img = x_test[idx] ax = fig.add_subplot(1, n_to_show, i+1) ax.axis(\\'off\\') ax.text(0.5, -0.35, \\'pred = \\' + str(preds_single[idx]), fontsize=10 , ha=\\'center\\', transform=ax.transAxes)\\n\\nYour First Deep Neural Network\\n\\n|\\n\\n45\\n\\nax.text(0.5, -0.7, \\'act = \\' + str(actual_single[idx]), fontsize=10 , ha=\\'center\\', transform=ax.transAxes) ax.imshow(img)\\n\\nFigure 2-9 shows a randomly chosen selection of predictions made by the model, alongside the true labels.\\n\\nFigure 2-9. Some predictions made by the model, alongside the actual labels\\n\\nCongratulations! You’ve just built your first deep neural network using Keras and used it to make predictions on new data. Even though this is a supervised learning problem, when we come to building generative models in future chapters many of the core ideas from this network (such as loss functions, activation functions, and under‐ standing layer shapes) will still be extremely important. Next we’ll look at ways of improving this model, by introducing a few new layer types.\\n\\nImproving the Model One of the reasons our network isn’t yet performing as well as it might is because there isn’t anything in the network that takes into account the spatial structure of the input images. In fact, our first step is to flatten the image into a single vector, so that we can pass it to the first Dense layer!\\n\\nTo achieve this we need to use a convolutional layer.\\n\\nConvolutional Layers First, we need to understand what is meant by a convolution in the context of deep learning.\\n\\nFigure 2-10 shows a 3 × 3 × 1 portion of a grayscale image being convoluted with a 3 × 3 × 1 filter (or kernel).\\n\\n46\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nFigure 2-10. The convolution operation\\n\\nThe convolution is performed by multiplying the filter pixelwise with the portion of the image, and summming the result. The output is more positive when the portion of the image closely matches the filter and more negative when the portion of the image is the inverse of the filter.\\n\\nIf we move the filter across the entire image, from left to right and top to bottom, recording the convolutional output as we go, we obtain a new array that picks out a particular feature of the input, depending on the values in the filter.\\n\\nThis is exactly what a convolutional layer is designed to do, but with multiple filters rather than just one. For example, Figure 2-11 shows two filters that highlight hori‐ zontal and vertical edges. You can see this convolutional process worked through manually in the notebook 02_02_deep_learning_convolutions.ipynb in the book repository.\\n\\nIf we are working with color images, then each filter would have three channels rather than one (i.e. each having shape 3 × 3 × 3) to match the three channels (red, green, blue) of the image.\\n\\nIn Keras, the Conv2D layer applies convolutions to an input tensor with two spatial dimensions (such as an image). For example, the Keras code corresponding to the diagram in Figure 2-11 is:\\n\\ninput_layer = Input(shape=(64,64,1))\\n\\nconv_layer_1 = Conv2D( filters = 2 , kernel_size = (3,3) , strides = 1 , padding = \"same\" )(input_layer)\\n\\nImproving the Model\\n\\n|\\n\\n47\\n\\nFigure 2-11. Two convolutional filters applied to a grayscale image\\n\\nStrides The strides parameter is the step size used by the layer to move the filters across the input. Increasing the stride therefore reduces the size of the output tensor. For exam‐ ple, when strides = 2, the height and width of the output tensor will be half the size of the input tensor. This is useful for reducing the spatial size of the tensor as it passes through the network, while increasing the number of channels.\\n\\nPadding The padding = \"same\" input parameter pads the input data with zeros so that the output size from the layer is exactly the same as the input size when strides = 1.\\n\\nFigure 2-12 shows a 3 × 3 kernel being passed over a 5 × 5 input image, with padding = \"same\" and strides = 1. The output size from this convolutional layer would also be 5 × 5, as the padding allows the kernel to extend over the edge of the image, so that it fits five times in both directions. Without padding, the kernel could only fit three times along each direction, giving an output size of 3 × 3.\\n\\n48\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nFigure 2-12. A 3 × 3 × 1 kernel (gray) being passed over a 5 × 5 × 1 input image (blue), with padding=\"same” and strides = 1, to generate the 5 × 5 × 1 output (green)5\\n\\nSetting padding = \"same\" is a good way to ensure that you are able to easily keep track of the size of the tensor as it passes through many convolutional layers.\\n\\nThe values stored in the filters are the weights that are learned by the neural network through training. Initially these are random, but gradually the filters adapt their weights to start picking out interesting features such as edges or particular color combinations.\\n\\nThe output of a Conv2D layer is another four-dimensional tensor, now of shape (batch_size, height, width, filters), so we can stack Conv2D layers on top of each other to grow the depth of our neural network. It’s really important to understand how the shape of the tensor changes as data flows through from one convolutional layer to the next. To demonstrate this, let’s imagine we are applying Conv2D layers to the CIFAR-10 dataset. This time, instead of one input channel (grayscale) we have three (red, green, and blue).\\n\\nFigure 2-13 represents the following network in Keras:\\n\\ninput_layer = Input(shape=(32,32,3))\\n\\nconv_layer_1 = Conv2D( filters = 10 , kernel_size = (4,4) , strides = 2 , padding = \\'same\\' )(input_layer)\\n\\nconv_layer_2 = Conv2D( filters = 20 , kernel_size = (3,3) , strides = 2 , padding = \\'same\\' )(conv_layer_1)\\n\\n5 Source: Vincent Dumoulin and Francesco Visin, “A Guide to Convolution Arithmetic for Deep Learning,” 12\\n\\nJanuary 2018, https://arxiv.org/pdf/1603.07285.pdf.\\n\\nImproving the Model\\n\\n|\\n\\n49\\n\\nflatten_layer = Flatten()(conv_layer_2)\\n\\noutput_layer = Dense(units=10, activation = \\'softmax\\')(flatten_layer)\\n\\nmodel = Model(input_layer, output_layer)\\n\\nFigure 2-13. A diagram of a convolutional neural network\\n\\nWe can use the model.summary() method to see the shape of the tensor as it passes through the network (Figure 2-14).\\n\\nFigure 2-14. A convolutional neural network summary\\n\\nLet’s analyze this network from input through to output. The input shape is (None, 32, 32, 3)—Keras uses None to represent the fact that we can pass any number of images through the network simultaneously. Since the network is just performing\\n\\n50\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\ntensor algebra, we don’t need to pass images through the network individually, but instead can pass them through together as a batch.\\n\\nThe shape of the filters in the first convolutional layer is 4 × 4 × 3. This is because we have chosen the filter to have height and width of 4 (kernel_size = (4,4)) and there are three channels in the preceding layer (red, green, and blue). Therefore, the number of parameters (or weights) in the layer is (4 × 4 × 3 + 1) × 10 = 490, where the + 1 is due to the inclusion of a bias term attached to each of the filters. It’s worth remembering that the depth of the filters in a layer is always the same as the number of channels in the preceding layer.\\n\\nAs before, the output from each filter when applied to each 4 × 4 × 3 section of the input image will be the pixelwise multiplication of the filter weights and the area of the image it is covering. As strides = 2 and padding = \"same\", the width and height of the output are both halved to 16, and since there are 10 filters the output of the first layer is a batch of tensors each having shape [16, 16, 10].\\n\\nIn general, the shape of the output from a convolutional layer with padding=\"same\" is:\\n\\nNone,\\n\\ninput height stride\\n\\n,\\n\\ninput width stride\\n\\n, f ilters\\n\\nIn the second convolutional layer, we choose the filters to be 3 × 3 and they now have depth 10, to match the number of channels in the previous layer. Since there are 20 filters in this layer, this gives a total number of parameters (weights) of (3 × 3 × 10 + 1) × 20 = 1,820. Again, we use strides = 2 and padding = \"same\", so the width and height both halve. This gives us an overall output shape of (None, 8, 8, 20).\\n\\nAfter applying a series of Conv2D layers, we need to flatten the tensor using the Keras Flatten layer. This results in a set of 8 × 8 × 20 = 1,280 units that we can connect to a final 10-unit Dense layer with softmax activation, which represents the probability of each category in a 10-category classification task.\\n\\nThis example demonstrates how we can chain convolutional layers together to create a convolutional neural network. Before we see how this compares in accuracy to our densely connected neural network, I’m going to introduce two more layer types that can also improve performance: BatchNormalization and Dropout.\\n\\nBatch Normalization One common problem when training a deep neural network is ensuring that the weights of the network remain within a reasonable range of values—if they start to become too large, this is a sign that your network is suffering from what is known as the exploding gradient problem. As errors are propagated backward through the\\n\\nImproving the Model\\n\\n|\\n\\n51\\n\\nnetwork, the calculation of the gradient in the earlier layers can sometimes grow exponentially large, causing wild fluctuations in the weight values. If your loss func‐ tion starts to return NaN, chances are that your weights have grown large enough to cause an overflow error.\\n\\nThis doesn’t necessarily happen immediately as you start training the network. Some‐ times your network can be happily training for hours when suddenly the loss func‐ tion returns NaN and your network has exploded. This can be incredibly annoying, especially when the network has seemingly been training well for a long time. To pre‐ vent this from happening, you need to understand the root cause of the exploding gradient problem.\\n\\nOne of the reasons for scaling input data into a neural network is to ensure a stable start to training over the first few iterations. Since the weights of the network are ini‐ tially randomized, unscaled input could potentially create huge activation values that immediately lead to exploding gradients. For example, instead of passing pixel values from 0–255 into the input layer, we usually scale these values to between –1 and 1.\\n\\nBecause the input is scaled, it’s natural to expect the activations from all future layers to be relatively well scaled as well. Initially, this may be true, but as the network trains and the weights move further away from their random initial values, this assumption can start to break down. This phenomenon is known as covariate shift.\\n\\nImagine you’re carrying a tall pile of books, and you get hit by a gust of wind. You move the books in a direction opposite to the wind to compensate, but in doing so, some of the books shift so that the tower is slightly more unstable than before. Ini‐ tially, this is OK, but with every gust the pile becomes more and more unstable, until eventually the books have shifted so much that the pile collapses. This is covariate shift.\\n\\nRelating this to neural networks, each layer is like a book in the pile. To remain stable, when the network updates the weights, each layer implicitly assumes that the distri‐ bution of its input from the layer beneath is approximately consistent across itera‐ tions. However, since there is nothing to stop any of the activation distributions shifting significantly in a certain direction, this can sometimes lead to runaway weight values and an overall collapse of the network.\\n\\nBatch normalization is a solution that drastically reduces this problem. The solution is surprisingly simple. A batch normalization layer calculates the mean and standard deviation of each of its input channels across the batch and normalizes by subtracting the mean and dividing by the standard deviation. There are then two learned param‐ eters for each channel, the scale (gamma) and shift (beta). The output is simply the normalized input, scaled by gamma and shifted by beta. Figure 2-15 shows the whole process.\\n\\n52\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nFigure 2-15. The batch normalization process6\\n\\nWe can place batch normalization layers after dense or convolutional layers to nor‐ malize the output from those layers. It’s a bit like connecting the layers of books with small sets of adjustable springs that ensure there aren’t any overall huge shifts in their positions over time.\\n\\nYou might be wondering how this layer works at test time. When it comes to predic‐ tion, we may only want to predict a single observation, so there is no batch over which to take averages. To get around this problem, during training a batch normal‐ ization layer also calculates the moving average of the mean and standard deviation of each channel and stores this value as part of the layer to use at test time.\\n\\nHow many parameters are contained within a batch normalization layer? For every channel in the preceding layer, two weights need to be learned: the scale (gamma) and shift (beta). These are the trainable parameters. The moving average and standard deviation also need to be calculated for each channel but since they are derived from the data passing through the layer rather than trained through backpropagation, they are called nontrainable parameters. In total, this gives four parameters for each chan‐ nel in the preceding layer, where two are trainable and two are nontrainable.\\n\\nIn Keras, the BatchNormalization layer implements the batch normalization functionality:\\n\\nBatchNormalization(momentum = 0.9)\\n\\nThe momentum parameter is the weight given to the previous value when calculating the moving average and moving standard deviation.\\n\\n6 Source: Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by\\n\\nReducing Internal Covariate Shift,” 11 February 2015, https://arxiv.org/abs/1502.03167.\\n\\nImproving the Model\\n\\n|\\n\\n53\\n\\nDropout Layers When studying for an exam, it is common practice for students to use past papers and sample questions to improve their knowledge of the subject material. Some stu‐ dents try to memorize the answers to these questions, but then come unstuck in the exam because they haven’t truly understood the subject matter. The best students use the practice material to further their general understanding, so that they are still able to answer correctly when faced with new questions that they haven’t seen before.\\n\\nThe same principle holds for machine learning. Any successful machine learning algorithm must ensure that it generalizes to unseen data, rather than simply remem‐ bering the training dataset. If an algorithm performs well on the training dataset, but not the test dataset, we say that it is suffering from overfitting. To counteract this problem, we use regularization techniques, which ensure that the model is penalized if it starts to overfit.\\n\\nThere are many ways to regularize a machine learning algorithm, but for deep learn‐ ing, one of the most common is by using dropout layers. This idea was introduced by Geoffrey Hinton in 2012 and presented in a 2014 paper by Srivastava et al.7\\n\\nDropout layers are very simple. During training, each dropout layer chooses a ran‐ dom set of units from the preceding layer and sets their output to zero, as shown in Figure 2-16.\\n\\nFigure 2-16. A dropout layer\\n\\n7 Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” Journal of Machine Learning Research 15 (2014): 1929–1958, http://jmlr.org/papers/volume15/srivastava14a/srivas tava14a.pdf.\\n\\n54\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nIncredibly, this simple addition drastically reduces overfitting, by ensuring that the network doesn’t become overdependent on certain units or groups of units that, in effect, just remember observations from the training set. If we use dropout layers, the network cannot rely too much on any one unit and therefore knowledge is more evenly spread across the whole network. This makes the model much better at gener‐ alizing to unseen data, because the network has been trained to produce accurate pre‐ dictions even under unfamiliar conditions, such as those caused by dropping random units. There are no weights to learn within a dropout layer, as the units to drop are decided stochastically. At test time, the dropout layer doesn’t drop any units, so that the full network is used to make predictions.\\n\\nReturning to our analogy, it’s a bit like a math student practicing past papers with a random selection of key formulae missing from their formula book. This way, they learn how to answer questions through an understanding of the core principles, rather than always looking up the formulae in the same places in the book. When it comes to test time, they will find it much easier to answer questions that they have never seen before, due to their ability to generalize beyond the training material.\\n\\nThe Dropout layer in Keras implements this functionality, with the rate parameter specifying the proportion of units to drop from the preceding layer:\\n\\nDropout(rate = 0.25)\\n\\nDropout layers are used most commonly after Dense layers since these are most prone to overfitting due to the higher number of weights, though you can also use them after convolutional layers.\\n\\nBatch normalization also has been shown to reduce overfitting, and therefore many modern deep learning architectures don’t use drop‐ out at all, and rely solely on batch normalization for regularization. As with most deep learning principles, there is no golden rule that applies in every situation—the only way to know for sure what’s best is to test different architectures and see which performs best on a holdout set of data.\\n\\nPutting It All Together You’ve now seen three new Keras layer types: Conv2D, BatchNormalization, and Drop out. Let’s put these pieces together into a new deep learning architecture and see how it performs on the CIFAR-10 dataset.\\n\\nYou can run the following example in the Jupyter notebook in the book repository called 02_03_deep_learning_conv_neural_network.ipynb.\\n\\nThe model architecture we shall test is shown here:\\n\\nImproving the Model\\n\\n|\\n\\n55\\n\\ninput_layer = Input((32,32,3))\\n\\nx = Conv2D(filters = 32, kernel_size = 3\\n\\n, strides = 1, padding = \\'same\\')(input_layer)\\n\\nx = BatchNormalization()(x) x = LeakyReLU()(x)\\n\\nx = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = \\'same\\')(x) x = BatchNormalization()(x) x = LeakyReLU()(x)\\n\\nx = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \\'same\\')(x) x = BatchNormalization()(x) x = LeakyReLU()(x)\\n\\nx = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = \\'same\\')(x) x = BatchNormalization()(x) x = LeakyReLU()(x)\\n\\nx = Flatten()(x)\\n\\nx = Dense(128)(x) x = BatchNormalization()(x) x = LeakyReLU()(x) x = Dropout(rate = 0.5)(x)\\n\\nx = Dense(NUM_CLASSES)(x) output_layer = Activation(\\'softmax\\')(x)\\n\\nmodel = Model(input_layer, output_layer)\\n\\nWe use four stacked Conv2D layers, each followed by a BatchNormalization and a LeakyReLU layer. After flattening the resulting tensor, we pass the data through a Dense layer of size 128, again followed by a BatchNormalization and a LeakyReLU layer. This is immediately followed by a Dropout layer for regularization, and the net‐ work is concluded with an output Dense layer of size 10.\\n\\nThe order in which to use the BatchNormalization and Activa tion layers is a matter of preference. I like to place the BatchNorm alization before the Activation, but successful architectures use these layers the other way around. If you do choose to use BatchNormalization before Activation then you can remember the order using the acronym BAD (BatchNormaliza tion, Activation then Dropout)!\\n\\nsome\\n\\nThe model summary is shown in Figure 2-17.\\n\\n56\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nFigure 2-17. Convolutional neural network (CNN) for CIFAR-10\\n\\nImproving the Model\\n\\n|\\n\\n57\\n\\nBefore moving on, make sure you are able to calculate the output shape and number of parameters for each layer by hand. It’s a good exercise to prove to yourself that you have fully understood how each layer is constructed and how it is connected to the preceding layer! Don’t forget to include the bias weights that are included as part of the Conv2D and Dense layers.\\n\\nWe compile and train the model in exactly the same way as before and call the evaluate method to determine its accuracy on the holdout set (Figure 2-18).\\n\\nFigure 2-18. CNN performance\\n\\nAs you can see, this model is now achieving 71.5% accuracy, up from 49.0% previ‐ ously. Much better! Figure 2-19 shows some predictions from our new convolutional model.\\n\\nFigure 2-19. CNN predictions\\n\\nThis improvement has been achieved simply by changing the architecture of the model to include convolutional, batch normalization, and dropout layers. Notice that the number of parameters is actually fewer in our new model than the previous model, even though the number of layers is far greater. This demonstrates the impor‐ tance of being experimental with your model design and being comfortable with how the different layer types can be used to your advantage. When building generative models, it becomes even more important to understand the inner workings of your model since it is the middle layers of your network that capture the high-level fea‐ tures that you are most interested in.\\n\\n58\\n\\n|\\n\\nChapter 2: Deep Learning\\n\\nSummary This chapter introduced the core deep learning concepts that you will need to start building your first deep generative models.\\n\\nA really important point to take away from this chapter is that deep neural networks are completely flexible by design, and there really are no fixed rules when it comes to model architecture. There are guidelines and best practices but you should feel free to experiment with layers and the order in which they appear. You will need to bear in mind that, like a set of building blocks, some layers will not fit together, simply because the input shape of one does not conform to the output shape of the other. This knowledge will come with experience and a solid understanding of how each layer changes the tensor shape as data flows through the network.\\n\\nAnother point to remember is that it is the layers in a deep neural network that are convolutional, rather than the network itself. When people talk about “convolutional neural networks,” they really mean “neural networks that contain convolutional lay‐ ers.” It is important to make this distinction, because you shouldn’t feel constrained to only use the architectures that you have read about in this book or elsewhere; instead, you should see them as examples of how you can piece together the different layer types. Like a child with a set of building blocks, the design of your neural network is only limited by your own imagination—and, crucially, your understanding of how the various layers fit together.\\n\\nIn the next chapter, we shall see how we can use these building blocks to design a net‐ work that can generate images.\\n\\nSummary\\n\\n|\\n\\n59\\n\\nCHAPTER 3 Variational Autoencoders\\n\\nIn 2013, Diederik P. Kingma and Max Welling published a paper that laid the founda‐ tions for a type of neural network known as a variational autoencoder (VAE).1 This is now one of the most fundamental and well-known deep learning architectures for generative modeling. In this chapter, we shall start by building a standard autoen‐ coder and then see how we can extend this framework to develop a variational autoencoder—our first example of a generative deep learning model.\\n\\nAlong the way, we will pick apart both types of model, to understand how they work at a granular level. By the end of the chapter you should have a complete understand‐ ing of how to build and manipulate autoencoder-based models and, in particular, how to build a variational autoencoder from scratch to generate images based on your own training set.\\n\\nLet’s start by paying a visit to a strange art exhibition…\\n\\nThe Art Exhibition Two brothers, Mr. N. Coder and Mr. D. Coder, run an art gallery. One weekend, they host an exhibition focused on monochrome studies of single-digit numbers. The exhibition is particularly strange because it contains only one wall and no physical artwork. When a new painting arrives for display, Mr. N. Coder simply chooses a point on the wall to represent the painting, places a marker at this point, then throws the original artwork away. When a customer requests to see the painting, Mr. D. Coder attempts to re-create the artwork using just the coordinates of the relevant marker on the wall.\\n\\n1 Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes,” 20 December 2013, https://\\n\\narxiv.org/abs/1312.6114.\\n\\n61\\n\\nThe exhibition wall is shown in Figure 3-1, where each black dot is a marker placed by Mr. N. Coder to represent a painting. We also show one of the paintings that has been marked on the wall at the point [–3.5, –0.5] by Mr. N. Coder and then recon‐ structed using just these two numbers by Mr. D. Coder.\\n\\nFigure 3-1. The wall at the art exhibition\\n\\n62\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nIn Figure 3-2 you can see examples of other original paintings (top row), the coordi‐ nates of the point on the wall given by Mr. N. Coder, and the reconstructed paintings produced by Mr. D. Coder (bottom row).\\n\\nFigure 3-2. More examples of reconstructed paintings\\n\\nSo how does Mr. N. Coder decide where to place the markers? The system evolves naturally through years of training and working together, gradually tweaking the pro‐ cesses for marker placement and reconstruction. The brothers carefully monitor the loss of revenue at the ticket office caused by customers asking for money back because of badly reconstructed paintings, and are constantly trying to find a system that minimizes this loss of earnings. As you can see from Figure 3-2, it works remark‐ ably well—customers who come to view the artwork very rarely complain that Mr. D. Coder’s re-created paintings are significantly different from the original pieces they came to see.\\n\\nOne day, Mr. N. Coder has an idea. What if he randomly placed markers on parts of the wall that currently do not have a marker? Mr. D. Coder could then re-create the artwork corresponding to these points, and within a few days they would have their own exhibition of completely original, generated paintings.\\n\\nThe brothers set about their plan and open their new exhibition to the public. Some of the exhibits and corresponding markers are displayed in Figure 3-3.\\n\\nThe Art Exhibition\\n\\n|\\n\\n63\\n\\nFigure 3-3. The new generative art exhibition\\n\\nAs you can see, the plan was not a great success. The overall variety is poor and some pieces of artwork do not really resemble single-digit numbers.\\n\\nSo, what went wrong and how can the brothers improve their scheme?\\n\\nAutoencoders The preceding story is an analogy for an autoencoder, which is a neural network made up of two parts:\\n\\nAn encoder network that compresses high-dimensional input data into a lower- dimensional representation vector\\n\\nA decoder network that decompresses a given representation vector back to the original domain\\n\\nThis process is shown in Figure 3-4.\\n\\n64\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-4. Diagram of an autoencoder\\n\\nThe network is trained to find weights for the encoder and decoder that minimize the loss between the original input and the reconstruction of the input after it has passed through the encoder and decoder.\\n\\nThe representation vector is a compression of the original image into a lower- dimensional, latent space. The idea is that by choosing any point in the latent space, we should be able to generate novel images by passing this point through the decoder, since the decoder has learned how to convert points in the latent space into viable images.\\n\\nIn our analogy, Mr. N. Coder and Mr. D. Coder are using representation vectors inside a two-dimensional latent space (the wall) to encode each image. This helps us to visualize the latent space, since we can easily plot points in 2D. In practice, autoen‐ coders usually have more than two dimensions in order to have more freedom to capture greater nuance in the images.\\n\\nAutoencoders can also be used to clean noisy images, since the encoder learns that it is not useful to capture the position of the random noise inside the latent space. For tasks such as this, a 2D latent space is probably too small to encode sufficient relevant\\n\\nAutoencoders\\n\\n|\\n\\n65\\n\\ninformation from the input. However, as we shall see, increasing the dimensionality of the latent space quickly leads to problems if we want to use the autoencoder as a generative model.\\n\\nYour First Autoencoder Let’s now build an autoencoder in Keras. This example follows the Jupyter notebook 03_01_autoencoder_train.ipynb in the book repository.\\n\\nGenerally speaking, it is a good idea to create a class for your model in a separate file. This way, you can instantiate an Autoencoder object with parameters that define a particular model architecture in the notebook, as shown in Example 3-1. This makes your model very flexible and able to be easily tested and ported to other projects as necessary.\\n\\nExample 3-1. Defining the autoencoder\\n\\nfrom models.AE import Autoencoder\\n\\nAE = Autoencoder( input_dim = (28,28,1) , encoder_conv_filters = [32,64,64, 64] , encoder_conv_kernel_size = [3,3,3,3] , encoder_conv_strides = [1,2,2,1] , decoder_conv_t_filters = [64,64,32,1] , decoder_conv_t_kernel_size = [3,3,3,3] , decoder_conv_t_strides = [1,2,2,1] , z_dim = 2)\\n\\nLet’s now take a look at the architecture of an autoencoder in more detail, starting with the encoder.\\n\\nThe Encoder In an autoencoder, the encoder’s job is to take the input image and map it to a point in the latent space. The architecture of the encoder we will be building is shown in Figure 3-5.\\n\\n66\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-5. Architecture of the encoder\\n\\nTo achieve this, we first create an input layer for the image and pass this through four Conv2D layers in sequence, each capturing increasingly high-level features. We use a stride of 2 on some of the layers to reduce the size of the output. The last convolu‐ tional layer is then flattened and connected to a Dense layer of size 2, which repre‐ sents our two-dimensional latent space.\\n\\nExample 3-2 shows how to build this in Keras.\\n\\nExample 3-2. The encoder\\n\\n### THE ENCODER encoder_input = Input(shape=self.input_dim, name=\\'encoder_input\\')\\n\\nx = encoder_input\\n\\nfor i in range(self.n_layers_encoder): conv_layer = Conv2D( filters = self.encoder_conv_filters[i]\\n\\nAutoencoders\\n\\n|\\n\\n67\\n\\n, kernel_size = self.encoder_conv_kernel_size[i] , strides = self.encoder_conv_strides[i] , padding = \\'same\\' , name = \\'encoder_conv_\\' + str(i) )\\n\\nx = conv_layer(x) x = LeakyReLU()(x)\\n\\nshape_before_flattening = K.int_shape(x)[1:] x = Flatten()(x)\\n\\nencoder_output= Dense(self.z_dim, name=\\'encoder_output\\')(x)\\n\\nself.encoder = Model(encoder_input, encoder_output)\\n\\nDefine the input to the encoder (the image).\\n\\nStack convolutional layers sequentially on top of each other.\\n\\nFlatten the last convolutional layer to a vector.\\n\\nDense layer that connects this vector to the 2D latent space.\\n\\nThe Keras model that defines the encoder—a model that takes an input image and encodes it into the 2D latent space.\\n\\nYou can change the number of convolutional layers in the encoder simply by adding elements to the lists that define the model architecture in the notebook. I strongly recommend experimenting with the parameters that define the models in this book, to understand how the architecture affects the number of weights in each layer, model performance, and model runtime.\\n\\nThe Decoder The decoder is a mirror image of the encoder, except instead of convolutional layers, we use convolutional transpose layers, as shown in Figure 3-6.\\n\\n68\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-6. Architecture of the decoder\\n\\nNote that the decoder doesn’t have to be a mirror image of the encoder. It can be any‐ thing you want, as long as the output from the last layer of the decoder is the same size as the input to the encoder (since our loss function will be comparing these pixel‐ wise).\\n\\nConvolutional Transpose Layers Standard convolutional layers allow us to halve the size of an input tensor in both height and width, by setting strides = 2.\\n\\nThe convolutional transpose layer uses the same principle as a standard convolutional layer (passing a filter across the image), but is different in that setting strides = 2 doubles the size of the input tensor in both height and width.\\n\\nIn a convolutional transpose layer, the strides parameter determines the internal zero padding between pixels in the image as shown in Figure 3-7.\\n\\nAutoencoders\\n\\n|\\n\\n69\\n\\nFigure 3-7. A convolutional transpose layer example—here, a 3 × 3 × 1 filter (gray) is being passed across a 3 × 3 × 1 image (blue) with strides = 2, to produce a 6 × 6 × 1 output tensor (green)2\\n\\nIn Keras, the Conv2DTranspose layer allows us to perform convolutional transpose operations on tensors. By stacking these layers, we can gradually expand the size of each layer, using strides of 2, until we get back to the original image dimension of 28 × 28.\\n\\nExample 3-3 shows how we build the decoder in Keras.\\n\\nExample 3-3. The decoder\\n\\n### THE DECODER decoder_input = Input(shape=(self.z_dim,), name=\\'decoder_input\\')\\n\\nx = Dense(np.prod(shape_before_flattening))(decoder_input) x = Reshape(shape_before_flattening)(x)\\n\\nfor i in range(self.n_layers_decoder): conv_t_layer = Conv2DTranspose( filters = self.decoder_conv_t_filters[i] , kernel_size = self.decoder_conv_t_kernel_size[i] , strides = self.decoder_conv_t_strides[i] , padding = \\'same\\' , name = \\'decoder_conv_t_\\' + str(i) )\\n\\nx = conv_t_layer(x)\\n\\nif i < self.n_layers_decoder - 1: x = LeakyReLU()(x) else:\\n\\n2 Source: Vincent Dumoulin and Francesco Visin, “A Guide to Convolution Arithmetic for Deep Learning,” 12\\n\\nJanuary 2018, https://arxiv.org/pdf/1603.07285.pdf.\\n\\n70\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nx = Activation(\\'sigmoid\\')(x)\\n\\ndecoder_output = x\\n\\nself.decoder = Model(decoder_input, decoder_output)\\n\\nDefine the input to the decoder (the point in the latent space).\\n\\nConnect the input to a Dense layer.\\n\\nReshape this vector into a tensor that can be fed as input into the first convolu‐ tional transpose layer.\\n\\nStack convolutional transpose layers on top of each other.\\n\\nThe Keras model that defines the decoder—a model that takes a point in the latent space and decodes it into the original image domain.\\n\\nJoining the Encoder to the Decoder To train the encoder and decoder simultaneously, we need to define a model that will represent the flow of an image through the encoder and back out through the decoder. Luckily, Keras makes it extremely easy to do this, as you can see in Example 3-4.\\n\\nExample 3-4. The full autoencoder\\n\\n### THE FULL AUTOENCODER model_input = encoder_input # model_output = decoder(encoder_output) #\\n\\nself.model = Model(model_input, model_output) #\\n\\nThe input to the autoencoder is the same as the input to the encoder.\\n\\nThe output from the autoencoder is the output from the encoder passed through the decoder.\\n\\nThe Keras model that defines the full autoencoder—a model that takes an image, and passes it through the encoder and back out through the decoder to generate a reconstruction of the original image.\\n\\nNow that we’ve defined our model, we just need to compile it with a loss function and optimizer, as shown in Example 3-5. The loss function is usually chosen to be either the root mean squared error (RMSE) or binary cross-entropy between the individual pixels of the original image and the reconstruction. Binary cross-entropy places\\n\\nAutoencoders\\n\\n|\\n\\n71\\n\\nheavier penalties on predictions at the extremes that are badly wrong, so it tends to push pixel predictions to the middle of the range. This results in less vibrant images. For this reason, I generally prefer to use RMSE as the loss function. However, there is no right or wrong choice—you should choose whichever works best for your use case.\\n\\nExample 3-5. Compilation\\n\\n### COMPILATION optimizer = Adam(lr=learning_rate)\\n\\ndef r_loss(y_true, y_pred): return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\\n\\nself.model.compile(optimizer=optimizer, loss = r_loss)\\n\\nWe can now train the autoencoder by passing in the input images as both the input and output, as shown in Example 3-6.\\n\\nExample 3-6. Training the autoencoder\\n\\nself.model.fit( x = x_train , y = x_train , batch_size = batch_size , shuffle = True , epochs = 10 , callbacks = callbacks_list )\\n\\nAnalysis of the Autoencoder Now that our autoencoder is trained, we can start to investigate how it is representing images in the latent space. We’ll then see how variational autoencoders are a natural extension that fixes the issues faced by autoencoders. The relevant code is included in the 03_02_autoencoder_analysis.ipynb notebook in the book repository.\\n\\nFirst, let’s take a set of new images that the model hasn’t seen, pass them through the encoder, and plot the 2D representations in a scatter plot. In fact, we’ve already seen this plot: it’s just Mr. N. Coder’s wall from Figure 3-1. Coloring this plot by digit pro‐ duces the chart in Figure 3-8. It’s worth noting that even though the digit labels were never shown to the model during training, the autoencoder has naturally grouped digits that look alike into the same part of the latent space.\\n\\n72\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-8. Plot of the latent space, colored by digit\\n\\nThere are a few interesting points to note:\\n\\n1. The plot is not symmetrical about the point (0, 0), or bounded. For example, there are far more points with negative y-axis values than positive, and some points even extend to a y-axis value of < –30.\\n\\n2. Some digits are represented over a very small area and others over a much larger area.\\n\\n3. There are large gaps between colors containing few points.\\n\\nRemember, our goal is to be able to choose a random point in the latent space, pass this through the decoder, and obtain an image of a digit that looks real. If we do this multiple times, we would also ideally like to get a roughly equal mixture of different kinds of digit (i.e., it shouldn’t always produce the same digit). This was also the aim\\n\\nAutoencoders\\n\\n|\\n\\n73\\n\\nof the Coder brothers when they were choosing random points on their wall to gen‐ erate new artwork for their exhibition.\\n\\nPoint 1 explains why it’s not obvious how we should even go about choosing a ran‐ dom point in the latent space, since the distribution of these points is undefined. Technically, we would be justified in choosing any point in the 2D plane! It’s not even guaranteed that points will be centered around (0,0). This makes sampling from our latent space extremely problematic.\\n\\nPoint 2 explains the lack of diversity in the generated images. Ideally, we’d like to obtain a roughly equal spread of digits when sampling randomly from our latent space. However, with an autoencoder this is not guaranteed. For example, the area of 1’s is far bigger than the area for 8’s, so when we pick points randomly in the space, we’re more likely to sample something that decodes to look like a 1 than an 8.\\n\\nPoint 3 explains why some generated images are poorly formed. In Figure 3-9 we can see three points in the latent space and their decoded images, none of which are par‐ ticularly well formed.\\n\\nFigure 3-9. Some poorly generated images\\n\\nPartly, this is because of the large spaces at the edge of the domain where there are few points—the autoencoder has no reason to ensure that points here are decoded to legible digits as very few images are encoded here. However, more worryingly, even points that are right in the middle of the domain may not be decoded into well- formed images. This is because the autoencoder is not forced to ensure that the space\\n\\n74\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nis continuous. For example, even though the point (2, –2) might be decoded to give a satisfactory image of a 4, there is no mechanism in place to ensure that the point (2.1, –2.1) also produces a satisfactory 4.\\n\\nIn 2D this issue is subtle; the autoencoder only has a small number of dimensions to work with, so naturally it has to squash digit groups together, resulting in the space between digit groups being relatively small. However, as we start to use more dimen‐ sions in the latent space to generate more complex images, such as faces, this problem becomes even more apparent. If we give the autoencoder free rein in how it uses the latent space to encode images, there will be huge gaps between groups of similar points with no incentive for the space between to generate well-formed images.\\n\\nSo how can we solve these three problems, so that our autoencoder framework is ready to be used as a generative model? To explain, let’s revisit the Coder brothers’ art exhibition, where a few changes have taken place since our last visit…\\n\\nThe Variational Art Exhibition Determined to make the generative art exhibition work, Mr. N. Coder recruits the help of his daughter, Epsilon. After a brief discussion, they decide to change the way that new paintings are marked on the wall. The new process works as follows.\\n\\nWhen a new painting arrives at the exhibition, Mr. N. Coder chooses a point on the wall where he would like to place the marker to represent the artwork, as before. However, now, instead of placing the marker on the wall himself, he passes his opin‐ ion of where it should go to Epsilon, who decides where the marker will be placed. She of course takes her father’s opinion into account, so she usually places the marker somewhere near the point that he suggests. Mr. D. Coder then finds the marker where Epsilon placed it and never hears Mr. N. Coder’s original opinion.\\n\\nMr. N. Coder also provides his daughter with an indication of how sure he is that the marker should be placed at the given point. The more certain he is, the closer Epsilon will generally place the point to his suggestion.\\n\\nThere is one final change to the old system. Before, the only feedback mechanism was the loss of earnings at the ticket office resulting from poorly reconstructed images. If the brothers saw that particular paintings weren’t being re-created accurately, they would adjust their understanding of marker placement and image regeneration to ensure revenue loss was minimized.\\n\\nNow, there is another source of feedback. Epsilon is quite lazy and gets annoyed whenever her father tells her to place markers far away from the center of the wall, where the ladder rests. She also doesn’t like it when he is too strict about where the markers should be placed, as then she feels she doesn’t have enough responsibility. Equally, if her father professes little confidence in where the markers should go, she\\n\\nThe Variational Art Exhibition\\n\\n|\\n\\n75\\n\\nfeels like she’s the one doing all the work! His confidence in the marker placements that he provides has to be just right for her to be happy.\\n\\nTo compensate for her annoyance, her father pays her more to do the job whenever he doesn’t stick to these rules. On the balance sheet, this expense is listed as his kitty- loss (KL) divulgence. He therefore needs to be careful that he doesn’t end up paying his daughter too much while also monitoring the loss of revenue at the ticket office. After training with these simple changes, Mr. N. Coder once again tries his strategy of placing markers on portions of the wall that are empty, so that Mr. D. Coder can regenerate these points as original artwork.\\n\\nSome of these points are shown in Figure 3-10, along with the generated images.\\n\\n76\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-10. Artwork from the new exhibition\\n\\nMuch better! The crowds arrive in great waves to see this new, exciting generative art and are amazed by the originality and diversity of the paintings.\\n\\nThe Variational Art Exhibition\\n\\n|\\n\\n77\\n\\nBuilding a Variational Autoencoder The previous story showed how, with a few simple changes, the art exhibition could be transformed into a successful generative process. Let’s now try to understand mathematically what we need to do to our autoencoder to convert it into a variational autoencoder and thus make it a truly generative model.\\n\\nThere are actually only two parts that we need to change: the encoder and the loss function.\\n\\nThe Encoder In an autoencoder, each image is mapped directly to one point in the latent space. In a variational autoencoder, each image is instead mapped to a multivariate normal dis‐ tribution around a point in the latent space, as shown in Figure 3-11.\\n\\nFigure 3-11. The difference between the encoder in an autoencoder and a variational autoencoder\\n\\n78\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nThe Normal Distribution A normal distribution is a probability distribution characterized by a distinctive bell curve shape. In one dimension, it is defined by two variables: the mean (μ) and the variance (σ2). The standard deviation (σ) is the square root of the variance.\\n\\nThe probability density function of the normal distribution in one dimension is:\\n\\nf x ∣ μ, σ2 =\\n\\n1 2πσ2\\n\\ne\\n\\n−\\n\\nx − μ 2\\n\\n2σ\\n\\n2\\n\\nFigure 3-12 shows several normal distributions in one dimension, for different values of the mean and variance. The red curve is the standard normal—the normal distri‐ bution with mean equal to 0 and variance equal to 1.\\n\\nFigure 3-12. The normal distribution in one dimension3\\n\\nWe can sample a point z from a normal distribution with mean μ and standard devia‐ tion σ using the following equation:\\n\\nz = μ + σϵ where ϵ is sampled from a standard normal distribution.\\n\\n3 Source: Wikipedia, http://bit.ly/2ZDWRJv.\\n\\nBuilding a Variational Autoencoder\\n\\n|\\n\\n79\\n\\nThe concept of a normal distribution extends to more than one dimension—the probability density function for a general multivariate normal distribution in k dimensions is as follows:\\n\\nf x1, ..., xk =\\n\\nexp −\\n\\n1 2\\n\\n− μ TΣ−1 − μ\\n\\n2π k Σ\\n\\nIn 2D, the mean vector μ and the symmetric covariance matrix Σ are defined as:\\n\\nμ =\\n\\nμ1 μ2\\n\\n,\\n\\nΣ =\\n\\n2 σ1\\n\\nρσ1σ2\\n\\nρσ1σ2 2 σ2\\n\\nwhere ρ is the correlation between the two dimensions x1 and x2.\\n\\nVariational autoencoders assume that there is no correlation between any of the dimensions in the latent space and therefore that the covariance matrix is diagonal. This means the encoder only needs to map each input to a mean vector and a var‐ iance vector and does not need to worry about covariance between dimensions. We also choose to map to the logarithm of the variance, as this can take any real number in the range (– ∞, ∞), matching the natural output range from a neural network unit, whereas variance values are always positive.\\n\\nTo summarize, the encoder will take each input image and encode it to two vectors, mu and log_var which together define a multivariate normal distribution in the latent space:\\n\\nmu\\n\\nThe mean point of the distribution.\\n\\nlog_var\\n\\nThe logarithm of the variance of each dimension.\\n\\nTo encode an image into a specific point z in the latent space, we can sample from this distribution, using the following equation:\\n\\nz = mu + sigma * epsilon\\n\\nwhere4\\n\\nsigma = exp(log_var / 2)\\n\\n4 σ = exp log σ = exp 2 log σ /2 = exp log σ\\n\\n2\\n\\n/2\\n\\n80\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nepsilon is a point sampled from the standard normal distribution.\\n\\nRelating this back to our story, mu represents Mr. N. Coder’s opinion of where the marker should appear on the wall. epsilon is his daughter’s random choice of how far away from mu the marker should be placed, scaled by sigma, Mr. N. Coder’s confi‐ dence in the marker’s position.\\n\\nSo why does this small change to the encoder help?\\n\\nPreviously, we saw how there was no requirement for the latent space to be continu‐ ous—even if the point (–2, 2) decodes to a well-formed image of a 4, there was no requirement for (–2.1, 2.1) to look similar. Now, since we are sampling a random point from an area around mu, the decoder must ensure that all points in the same neighborhood produce very similar images when decoded, so that the reconstruction loss remains small. This is a very nice property that ensures that even when we choose a point in the latent space that has never been seen by the decoder, it is likely to decode to an image that is well formed.\\n\\nLet’s now see how we build this new version of the encoder in Keras (Example 3-7). You can train your own variational autoencoder on the digits dataset by running the notebook 03_03_vae_digits_train.ipynb in the book repository.\\n\\nExample 3-7. The variational autoencoder’s encoder\\n\\n### THE ENCODER encoder_input = Input(shape=self.input_dim, name=\\'encoder_input\\')\\n\\nx = encoder_input\\n\\nfor i in range(self.n_layers_encoder): conv_layer = Conv2D( filters = self.encoder_conv_filters[i] , kernel_size = self.encoder_conv_kernel_size[i] , strides = self.encoder_conv_strides[i] , padding = \\'same\\' , name = \\'encoder_conv_\\' + str(i) )\\n\\nx = conv_layer(x)\\n\\nif self.use_batch_norm: x = BatchNormalization()(x)\\n\\nx = LeakyReLU()(x) if self.use_dropout: x = Dropout(rate = 0.25)(x)\\n\\nshape_before_flattening = K.int_shape(x)[1:] x = Flatten()(x)\\n\\nBuilding a Variational Autoencoder\\n\\n|\\n\\n81\\n\\nself.mu = Dense(self.z_dim, name=\\'mu\\')(x) self.log_var = Dense(self.z_dim, name=\\'log_var\\')(x) #\\n\\nencoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\\n\\ndef sampling(args): mu, log_var = args epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.) return mu + K.exp(log_var / 2) * epsilon\\n\\nencoder_output = Lambda(sampling, name=\\'encoder_output\\')([self.mu, self.log_var])\\n\\nencoder = Model(encoder_input, encoder_output)\\n\\nInstead of connecting the flattened layer directly to the 2D latent space, we con‐ nect it to layers mu and log_var.\\n\\nThe Keras model that outputs the values of mu and log_var for a given input image.\\n\\nThis Lambda layer samples a point z in the latent space from the normal distribu‐ tion defined by the parameters mu and log_var.\\n\\nThe Keras model that defines the encoder—a model that takes an input image and encodes it into the 2D latent space, by sampling a point from the normal dis‐ tribution defined by mu and log_var.\\n\\nLambda layer A Lambda layer simple wraps any function into Keras layer. For example, the following layer squares its input:\\n\\nLambda(lambda x: x ** 2)\\n\\nThey are useful when you want to apply a function to a tensor that isn’t already included as one of the out-of-the-box Keras layer types.\\n\\nA diagram of the encoder is shown in Figure 3-13.\\n\\nAs mentioned previously, the decoder of a variational autoencoder is identical to the decoder of a plain autoencoder. The only other part we need to change is the loss function.\\n\\n82\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-13. Diagram of the VAE encoder\\n\\nBuilding a Variational Autoencoder\\n\\n|\\n\\n83\\n\\nThe Loss Function Previously, our loss function only consisted of the RMSE loss between images and their reconstruction after being passed through the encoder and decoder. This recon‐ struction loss also appears in a variational autoencoder, but we require one extra com‐ ponent: the Kullback–Leibler (KL) divergence.\\n\\nKL divergence is a way of measuring how much one probability distribution differs from another. In a VAE, we want to measure how different our normal distribution with parameters mu and log_var is from the standard normal distribution. In this special case, the KL divergence has the closed form:\\n\\nkl_loss = -0.5 * sum(1 + log_var - mu ^ 2 - exp(log_var))\\n\\nor in mathematical notation:\\n\\nDKL N μ, σ ∥ N 0, 1 =\\n\\n1 2 ∑ 1 + log σ2 − μ2 − σ2\\n\\nThe sum is taken over all the dimensions in the latent space. kl_loss is minimized to 0 when mu = 0 and log_var = 0 for all dimensions. As these two terms start to differ from 0, kl_loss increases.\\n\\nIn summary, the KL divergence term penalizes the network for encoding observa‐ tions to mu and log_var variables that differ significantly from the parameters of a standard normal distribution, namely mu = 0 and log_var = 0.\\n\\nAgain, relating this back to our story, this term represents Epsilon’s annoyance at hav‐ ing to move the ladder away from the middle of the wall (mu different from 0) and also if Mr. N. Coder’s confidence in the marker position isn’t just right (log_var dif‐ ferent from 0), both of which incur a cost.\\n\\nWhy does this addition to the loss function help?\\n\\nFirst, we now have a well-defined distribution that we can use for choosing points in the latent space—the standard normal distribution. If we sample from this distribu‐ tion, we know that we’re very likely to get a point that lies within the limits of what the VAE is used to seeing. Secondly, since this term tries to force all encoded distribu‐ tions toward the standard normal distribution, there is less chance that large gaps will form between point clusters. Instead, the encoder will try to use the space around the origin symmetrically and efficiently.\\n\\nIn the code, the loss function for a VAE is simply the addition of the reconstruction loss and the KL divergence loss term. We weight the reconstruction loss with a term, r_loss_factor, that ensures that it is well balanced with the KL divergence loss. If we weight the reconstruction loss too heavily, the KL loss will not have the desired\\n\\n84\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nregulatory effect and we will see the same problems that we experienced with the plain autoencoder. If the weighting term is too small, the KL divergence loss will dominate and the reconstructed images will be poor. This weighting term is one of the parameters to tune when you’re training your VAE.\\n\\nExample 3-8 shows how we include the KL divergence term in our loss function.\\n\\nExample 3-8. Including KL divergence in the loss function\\n\\n### COMPILATION optimizer = Adam(lr=learning_rate)\\n\\ndef vae_r_loss(y_true, y_pred): r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3]) return r_loss_factor * r_loss\\n\\ndef vae_kl_loss(y_true, y_pred): kl_loss = -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis = 1) return kl_loss\\n\\ndef vae_loss(y_true, y_pred): r_loss = vae_r_loss(y_true, y_pred) kl_loss = vae_kl_loss(y_true, y_pred) return r_loss + kl_loss\\n\\noptimizer = Adam(lr=learning_rate) self.model.compile(optimizer=optimizer, loss = vae_loss , metrics = [vae_r_loss, vae_kl_loss])\\n\\nAnalysis of the Variational Autoencoder All of the following analysis is available in the book repository, in the notebook 03_04_vae_digits_analysis.ipynb.\\n\\nReferring back to Figure 3-10, we can see several changes in how the latent space is organized. The black dots show the mu values of each encoded image. The KL diver‐ gence loss term ensures that the mu and sigma values never stray too far from a stan‐ dard normal. We can therefore sample from the standard normal distribution to generate new points in the space to be decoded (the red dots).\\n\\nSecondly, there are not so many generated digits that are badly formed, since the latent space is now locally continuous due to fact that the encoder is now stochastic, rather than deterministic.\\n\\nFinally, by coloring points in the latent space by digit (Figure 3-14), we can see that there is no preferential treatment of any one type. The righthand plot shows the space transformed into p-values, and we can see that each color is approximately equally represented. Again, it’s important to remember that the labels were not used at all\\n\\nBuilding a Variational Autoencoder\\n\\n|\\n\\n85\\n\\nduring training—the VAE has learned the various forms of digits by itself in order to help minimize reconstruction loss.\\n\\nFigure 3-14. The latent space of the VAE colored by digit\\n\\nSo far, all of our work on autoencoders and variational autoencoders has been limited to a latent space with two dimensions. This has helped us to visualize the inner work‐ ings of a VAE on the page and understand why the small tweaks that we made to the architecture of the autoencoder helped transform it into a more powerful class of net‐ work that can be used for generative modeling.\\n\\nLet’s now turn our attention to a more complex dataset and see the amazing things that variational autoencoders can achieve when we increase the dimensionality of the latent space.\\n\\nUsing VAEs to Generate Faces We shall be using the CelebFaces Attributes (CelebA) dataset to train our next varia‐ tional autoencoder. This is a collection of over 200,000 color images of celebrity faces, each annotated with various labels (e.g., wearing hat, smiling, etc.). A few examples are shown in Figure 3-15.\\n\\n86\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-15. Some examples from the CelebA dataset5\\n\\nOf course, we don’t need the labels to train the VAE, but these will be useful later when we start exploring how these features are captured in the multidimensional latent space. Once our VAE is trained, we can sample from the latent space to gener‐ ate new examples of celebrity faces.\\n\\nTraining the VAE The network architecture for the faces model is similar to the digits example, with a few slight differences:\\n\\n1. Our data now has three input channels (RGB) instead of one (grayscale). This means we need to change the number of channels in the final convolutional transpose layer of the decoder to 3.\\n\\n2. We shall be using a latent space with two hundred dimensions instead of two. Since faces are much more complex than digits, we increase the dimensionality of the latent space so that the network can encode a satisfactory amount of detail from the images.\\n\\n5 Source: Liu et al., 2015, http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html.\\n\\nUsing VAEs to Generate Faces\\n\\n|\\n\\n87\\n\\n3. There are batch normalization layers after each convolution layer to speed up training. Even though each batch takes a longer time to run, the number of batches required to reach the same loss is greatly reduced. Dropout layers are also used.\\n\\n4. We increase the reconstruction loss factor to ten thousand. This is a parameter that requires tuning; for this dataset and architecture this value was found to gen‐ erate good results.\\n\\n5. We use a generator to feed images to the VAE from a folder, rather than loading all the images into memory up front. Since the VAE trains in batches, there is no need to load all the images into memory first, so instead we use the built-in fit_generator method that Keras provides to read in images only when they are required for training.\\n\\nThe full architectures of the encoder and decoder are shown in Figures 3-16 and 3-17\\n\\n88\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-16. The VAE encoder for the CelebA dataset\\n\\nUsing VAEs to Generate Faces\\n\\n|\\n\\n89\\n\\nFigure 3-17. The VAE decoder for the CelebA dataset\\n\\nTo Jupyter notebook 03_05_vae_faces_train.ipynb from the book repository. After around five epochs of training your VAE should be able to produce novel images of celebrity faces!\\n\\ntrain\\n\\nthe VAE on\\n\\nthe CelebA dataset, run\\n\\nthe\\n\\n90\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nAnalysis of the VAE You can the notebook 03_06_vae_faces_analysis.ipynb, once you have trained the VAE. Many of the ideas in this section were inspired by a 2016 paper by Xianxu Hou et al.6\\n\\nreplicate\\n\\nthe analysis\\n\\nthat\\n\\nfollows by\\n\\nrunning\\n\\nFirst, let’s take a look at a sample of reconstructed faces. The top row in Figure 3-18 shows the original images and the bottom row shows the reconstructions once they have passed through the encoder and decoder.\\n\\nFigure 3-18. Reconstructed faces, after passing through the encoder and decoder\\n\\nWe can see that the VAE has successfully captured the key features of each face—the angle of the head, the hairstyle, the expression, etc. Some of the fine detail is missing, but it is important to remember that the aim of building variational autoencoders isn’t to achieve perfect reconstruction loss. Our end goal is to sample from the latent space in order to generate new faces.\\n\\nFor this to be possible we must check that the distribution of points in the latent space approximately resembles a multivariate standard normal distribution. Since we cannot view all dimensions simultaneously, we can instead check the distribution of each latent dimension individually. If we see any dimensions that are significantly dif‐ ferent from a standard normal distribution, we should probably reduce the recon‐ struction loss factor, since the KL divergence term isn’t having enough effect.\\n\\nThe first 50 dimensions in our latent space are shown in Figure 3-19. There aren’t any distributions that stand out as being significantly different from the standard normal, so we can move on to generating some faces!\\n\\n6 Xianxu Hou et al., “Deep Feature Consistent Variational Autoencoder,” 2 October 2016, https://arxiv.org/abs/\\n\\n1610.00291.\\n\\nUsing VAEs to Generate Faces\\n\\n|\\n\\n91\\n\\nFigure 3-19. Distributions of points for the first 50 dimensions in the latent space\\n\\nGenerating New Faces To generate new faces, we can use the code in Example 3-9.\\n\\nExample 3-9. Generating new faces from the latent space\\n\\nn_to_show = 30\\n\\nznew = np.random.normal(size = (n_to_show,VAE.z_dim))\\n\\nreconst = VAE.decoder.predict(np.array(znew))\\n\\nfig = plt.figure(figsize=(18, 5)) fig.subplots_adjust(hspace=0.4, wspace=0.4) for i in range(n_to_show): ax = fig.add_subplot(3, 10, i+1) ax.imshow(reconst[i, :,:,:]) ax.axis(\\'off\\')\\n\\nplt.show()\\n\\nWe sample 30 points from a standard normal distribution with 200 dimen‐ sions…\\n\\n…then pass these points to the decoder.\\n\\nThe resulting output is a 128 × 128 × 3 image that we can view.\\n\\nThe output is shown in Figure 3-20.\\n\\n92\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nFigure 3-20. New generated faces\\n\\nAmazingly, the VAE is able to take the set of points that we sampled and convert each into a convincing image of a person’s face. While the images are not perfect, they are a giant leap forward from the Naive Bayes model that we started exploring in Chap‐ ter 1. The Naive Bayes model faced the problem of not being able to capture depend‐ ency between adjacent pixels, since it had no notion of higher-level features such as sunglasses or brown hair. The VAE doesn’t suffer from this problem, since the convo‐ lutional layers of the encoder are designed to translate low-level pixels into high-level features and the decoder is trained to perform the opposite task of translating the high-level features in the latent space back to raw pixels.\\n\\nLatent Space Arithmetic One benefit of mapping images into a lower-dimensional space is that we can per‐ form arithmetic on vectors in this latent space that has a visual analogue when deco‐ ded back into the original image domain.\\n\\nFor example, suppose we want to take an image of somebody who looks sad and give them a smile. To do this we first need to find a vector in the latent space that points in the direction of increasing smile. Adding this vector to the encoding of the original image in the latent space will give us a new point which, when decoded, should give us a more smiley version of the original image.\\n\\nSo how can we find the smile vector? Each image in the CelebA dataset is labeled with attributes, one of which is smiling. If we take the average position of encoded images in the latent space with the attribute smiling and subtract the average position of encoded images that do not have the attribute smiling, we will obtain the vector that points from not smiling to smiling, which is exactly what we need.\\n\\nConceptually, we are performing the following vector arithmetic in the latent space, where alpha is a factor that determines how much of the feature vector is added or subtracted:\\n\\nz_new = z + alpha * feature_vector\\n\\nUsing VAEs to Generate Faces\\n\\n|\\n\\n93\\n\\nLet’s see this in action. Figure 3-21 shows several images that have been encoded into the latent space. We then add or subtract multiples of a certain vector (e.g., smile, blonde, male, eyeglasses) to obtain different versions of the image, with only the rele‐ vant feature changed.\\n\\nFigure 3-21. Adding and subtracting features to and from faces\\n\\nIt is quite remarkable that even though we are moving the point a significantly large distance in the latent space, the core image barely changes, except for the one feature that we want to manipulate. This demonstrates the power of variational autoencoders for capturing and adjusting high-level features in images.\\n\\nMorphing Between Faces We can use a similar idea to morph between two faces. Imagine two points in the latent space, A and B, that represent two images. If you started at point A and walked toward point B in a straight line, decoding each point on the line as you went, you would see a gradual transition from the starting face to the end face.\\n\\n94\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nMathematically, we are traversing a straight line, which can be described by the fol‐ lowing equation:\\n\\nz_new = z_A * (1- alpha) + z_B * alpha\\n\\nHere, alpha is a number between 0 and 1 that determines how far along the line we are, away from point A.\\n\\nFigure 3-22 shows this process in action. We take two images, encode them into the latent space, and then decode points along the straight line between them at regular intervals.\\n\\nFigure 3-22. Morphing between two faces\\n\\nIt is worth noting the smoothness of the transition—even where there are multiple features to change simultaneously (e.g., removal of glasses, hair color, gender), the VAE manages to achieve this fluidly, showing that the latent space of the VAE is truly a continuous space that can be traversed and explored to generate a multitude of dif‐ ferent human faces.\\n\\nSummary In this chapter we have seen how variational autoencoders are a powerful tool in the generative modeling toolbox. We started by exploring how plain autoencoders can be used to map high-dimensional images into a low-dimensional latent space, so that high-level features can be extracted from the individually uninformative pixels. How‐ ever, like with the Coder brothers’ art exhibition, we quickly found that there were some drawbacks to using plain autoencoders as a generative model—sampling from the learned latent space was problematic, for a number of reasons.\\n\\nSummary\\n\\n|\\n\\n95\\n\\nVariational autoencoders solve these problems, by introducing randomness into the model and constraining how points in the latent space are distributed. We saw that with a few minor adjustments, we can transform our autoencoder into a variational autoencoder, thus giving it the power to be a generative model.\\n\\nFinally, we applied our new technique to the problem of face generation and saw how we can simply choose points from a standard normal distribution to generate new faces. Moreover, by performing vector arithmetic within the latent space, we can ach‐ ieve some amazing effects, such as face morphing and feature manipulation. With these features, it is easy to see why VAEs have become a prominent technique for gen‐ erative modeling in recent years.\\n\\nIn the next chapter, we shall explore a different kind of generative model that has attracted an even greater amount of attention: the generative adversarial network.\\n\\n96\\n\\n|\\n\\nChapter 3: Variational Autoencoders\\n\\nCHAPTER 4 Generative Adversarial Networks\\n\\nOn Monday, December 5, 2016, at 2:30 p.m., Ian Goodfellow of Google Brain presen‐ ted a tutorial entitled “Generative Adversarial Networks” to the delegates of the Neu‐ ral Information Processing Systems (NIPS) conference in Barcelona.1 The ideas presented in the tutorial are now regarded as one of the key turning points for gener‐ ative modeling and have spawned a wide variety of variations on his core idea that have pushed the field to even greater heights.\\n\\nThis chapter will first lay out the theoretical underpinning of generative adversarial networks (GANs). You will then learn how to use the Python library Keras to start building your own GANs.\\n\\nFirst though, we shall take a trip into the wilderness to meet Gene…\\n\\nGanimals One afternoon, while walking through the local jungle, Gene sees a woman thumbing through a set of black and white photographs, looking worried. He goes over to ask if he can help.\\n\\nThe woman introduces herself as Di, a keen explorer, and explains that she is hunting for the elusive ganimal, a mythical creature that is said to roam around the jungle. Since the creature is nocturnal, she only has a collection of nighttime photos of the beast that she once found lying on the floor of the jungle, dropped by another gani‐ mal enthusiast. Some of these photos are shown in Figure 4-1. Di makes money by selling the images to collectors but is starting to worry, as she hasn’t actually ever seen\\n\\n1 Ian Goodfellow, “NIPS 2016 Tutorial: Generative Adversarial Networks,” 21 December 2016, https://\\n\\narxiv.org/abs/1701.00160v4.\\n\\n97\\n\\nthe creatures and is concerned that her business will falter if she can’t produce more original photographs soon.\\n\\nFigure 4-1. Original ganimal photographs\\n\\nBeing a keen photographer, Gene decides to help Di. He agrees to search for the gani‐ mal himself and give her any photographs of the nocturnal beast that he manages to take.\\n\\nHowever, there is a problem. Since Gene has never seen a ganimal, he doesn’t know how to produce good photos of the creature, and also, since Di has only ever sold the photos she found, she cannot even tell the difference between a good photo of a gani‐ mal and a photo of nothing at all.\\n\\nStarting from this state of ignorance, how can they work together to ensure Gene is eventually able to produce impressive ganimal photographs?\\n\\n98\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nThey come up with following process. Each night, Gene takes 64 photographs, each in a different location with different random moonlight readings, and mixes them with 64 ganimal photos from the original collection. Di then looks at this set of pho‐ tos and tries to guess which were taken by Gene and which are originals. Based on her mistakes, she updates her own understanding of how to discriminate between Gene’s attempts and the original photos. Afterwards, Gene takes another 64 photos and shows them to Di. Di gives each photo a score between 0 and 1, indicating how realistic she thinks each photo is. Based on this feedback, Gene updates the settings on his camera to ensure that next time, he takes photos that Di is more likely to rate highly.\\n\\nThis process continues for many days. Initially, Gene doesn’t get any useful feedback from Di, since she is randomly guessing which photos are genuine. However, after a few weeks of her training ritual, she gradually gets better at this, which means that she can provide better feedback to Gene so that he can adjust his camera accordingly in his training session. This makes Di’s task harder, since now Gene’s photos aren’t quite as easy to distinguish from the real photos, so she must again learn how to improve. This back-and-forth process continues, over many days and weeks.\\n\\nOver time, Gene gets better and better at producing ganimal photos, until eventually, Di is once again resigned to the fact that she cannot tell the difference between Gene’s photos and the originals. They take Gene’s generated photos to the auction and the experts cannot believe the quality of the new sightings—they are just as convincing as the originals. Some examples of Gene’s work are shown in Figure 4-2.\\n\\nFigure 4-2. Samples of Gene’s ganimal photography\\n\\nIntroduction to GANs The adventures of Gene and Di hunting elusive nocturnal ganimals are a metaphor for one of the most important deep learning advancements of recent years: generative adversarial networks.\\n\\nSimply put, a GAN is a battle between two adversaries, the generator and the discrim‐ inator. The generator tries to convert random noise into observations that look as if\\n\\nIntroduction to GANs\\n\\n|\\n\\n99\\n\\nthey have been sampled from the original dataset and the discriminator tries to pre‐ dict whether an observation comes from the original dataset or is one of the genera‐ tor’s forgeries. Examples of the inputs and outputs to the two networks are shown in Figure 4-3.\\n\\nFigure 4-3. Inputs and outputs of the two networks in a GAN\\n\\nAt the start of the process, the generator outputs noisy images and the discriminator predicts randomly. The key to GANs lies in how we alternate the training of the two networks, so that as the generator becomes more adept at fooling the discriminator, the discriminator must adapt in order to maintain its ability to correctly identify which observations are fake. This drives the generator to find new ways to fool the discriminator, and so the cycle continues.\\n\\nTo see this in action, let’s start building our first GAN in Keras, to generate pictures of nocturnal ganimals.\\n\\nYour First GAN First, you’ll need to download the training data. We’ll be using the Quick, Draw! data‐ set from Google. This is a crowdsourced collection of 28 × 28–pixel grayscale doo‐ dles, labeled by subject. The dataset was collected as part of an online game that challenged players to draw a picture of an object or concept, while a neural network tries to guess the subject of the doodle. It’s a really useful and fun dataset for learning the fundamentals of deep learning. For this task you’ll need to download the camel numpy file and save it into the ./data/camel/ folder in the book repository.2 The origi‐\\n\\n2 By happy coincidence, ganimals look exactly like camels.\\n\\n100\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nnal data is scaled in the range [0, 255] to denote the pixel intensity. For this GAN we rescale the data to the range [–1, 1].\\n\\nRunning the notebook 04_01_gan_camel_train.ipynb in the book repository will start training the GAN. As in the previous chapter on VAEs, you can instantiate a GAN object in the notebook, as shown in Example 4-1, and play around with the parame‐ ters to see how it affects the model.\\n\\nExample 4-1. Defining the GAN\\n\\ngan = GAN(input_dim = (28,28,1) , discriminator_conv_filters = [64,64,128,128] , discriminator_conv_kernel_size = [5,5,5,5] , discriminator_conv_strides = [2,2,2,1] , discriminator_batch_norm_momentum = None , discriminator_activation = \\'relu\\' , discriminator_dropout_rate = 0.4 , discriminator_learning_rate = 0.0008 , generator_initial_dense_layer_size = (7, 7, 64) , generator_upsample = [2,2, 1, 1] , generator_conv_filters = [128,64, 64,1] , generator_conv_kernel_size = [5,5,5,5] , generator_conv_strides = [1,1, 1, 1] , generator_batch_norm_momentum = 0.9 , generator_activation = \\'relu\\' , generator_dropout_rate = None , generator_learning_rate = 0.0004 , optimiser = \\'rmsprop\\' , z_dim = 100 )\\n\\nLet’s first take a look at how we build the discriminator.\\n\\nThe Discriminator The goal of the discriminator is to predict if an image is real or fake. This is a super‐ vised image classification problem, so we can use the same network architecture as in Chapter 2: stacked convolutional layers, followed by a dense output layer.\\n\\nIn the original GAN paper, dense layers were used in place of the convolutional lay‐ ers. However, since then, it has been shown that convolutional layers give greater pre‐ dictive power to the discriminator. You may see this type of GAN called a DCGAN (deep convolutional generative adversarial network) in the literature, but now essen‐ tially all GAN architectures contain convolutional layers, so the “DC” is implied when we talk about GANs. It is also common to see batch normalization layers in the discriminator for vanilla GANs, though we choose not to use them here for simplicity.\\n\\nYour First GAN\\n\\n|\\n\\n101\\n\\nThe full architecture of the discriminator we will be building is shown in Figure 4-4.\\n\\nFigure 4-4. The discriminator of the GAN\\n\\nThe Keras code to build the discriminator is provided in Example 4-2.\\n\\nExample 4-2. The discriminator\\n\\ndiscriminator_input = Input(shape=self.input_dim, name=\\'discriminator_input\\') x = discriminator_input\\n\\nfor i in range(self.n_layers_discriminator):\\n\\nx = Conv2D( filters = self.discriminator_conv_filters[i] , kernel_size = self.discriminator_conv_kernel_size[i] , strides = self.discriminator_conv_strides[i] , padding = \\'same\\' , name = \\'discriminator_conv_\\' + str(i) )(x)\\n\\n102\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nif self.discriminator_batch_norm_momentum and i > 0: x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\\n\\nx = Activation(self.discriminator_activation)(x)\\n\\nif self.discriminator_dropout_rate: x = Dropout(rate = self.discriminator_dropout_rate)(x)\\n\\nx = Flatten()(x) discriminator_output= Dense(1, activation=\\'sigmoid\\' , kernel_initializer = self.weight_init)(x)\\n\\ndiscriminator = Model(discriminator_input, discriminator_output)\\n\\nDefine the input to the discriminator (the image).\\n\\nStack convolutional layers on top of each other.\\n\\nFlatten the last convolutional layer to a vector.\\n\\nDense layer of one unit, with a sigmoid activation function that transforms the output from the dense layer to the range [0, 1].\\n\\nThe Keras model that defines the discriminator—a model that takes an input image and outputs a single number between 0 and 1.\\n\\nNotice how we use a stride of 2 in some of the convolutional layers to reduce the size of the tensor as it passes through the network, but increase the number of channels (1 in the grayscale input image, then 64, then 128).\\n\\nThe sigmoid activation in the final layer ensures that the output is scaled between 0 and 1. This will be the predicted probability that the image is real.\\n\\nThe Generator Now let’s build the generator. The input to the generator is a vector, usually drawn from a multivariate standard normal distribution. The output is an image of the same size as an image in the original training data.\\n\\nThis description may remind you of the decoder in a variational autoencoder. In fact, the generator of a GAN fulfills exactly the same purpose as the decoder of a VAE: converting a vector in the latent space to an image. The concept of mapping from a latent space back to the original domain is very common in generative modeling as it gives us the ability to manipulate vectors in the latent space to change high-level fea‐ tures of images in the original domain.\\n\\nThe architecture of the generator we will be building is shown in Figure 4-5.\\n\\nYour First GAN\\n\\n|\\n\\n103\\n\\nFigure 4-5. The generator\\n\\nFirst though, we need to introduce a new layer type: the upsampling layer.\\n\\nUpsampling In the decoder of the variational autoencoder that we built in the previous chapter, we doubled the width and height of the tensor at each layer using Conv2DTranspose lay‐ ers with stride 2. This inserted zero values in between pixels before performing the convolution operations.\\n\\nIn this GAN, we instead use the Keras Upsampling2D layer to double the width and height of the input tensor. This simply repeats each row and column of its input in order to double the size. We then follow this with a normal convolutional layer with stride 1 to perform the convolution operation. It is a similar idea to convolutional\\n\\n104\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\ntranspose, but instead of filling the gaps between pixels with zeros, upsampling just repeats the existing pixel values.\\n\\nBoth of these methods—Upsampling + Conv2D and Conv2DTranspose—are acceptable ways to transform back to the original image domain. It really is a case of testing both methods in your own problem setting and seeing which produces better results. It has been shown that the Conv2DTranspose method can lead to artifacts, or small checker‐ board patterns in the output image (see Figure 4-6) that spoil the quality of the out‐ put. However, they are still used in many of the most impressive GANs in the literature and have proven to be a powerful tool in the deep learning practitioner’s toolbox—again, I suggest you experiment with both methods and see which works best for you.\\n\\nFigure 4-6. Artifacts when using convolutional transpose layers3\\n\\nThe code for building the generator is given in Example 4-3.\\n\\nExample 4-3. The generator\\n\\ngenerator_input = Input(shape=(self.z_dim,), name=\\'generator_input\\') x = generator_input\\n\\nx = Dense(np.prod(self.generator_initial_dense_layer_size))(x)\\n\\nif self.generator_batch_norm_momentum: x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\\n\\n3 Source: Augustus Odena et al., “Deconvolution and Checkerboard Artifacts, 17 October 2016, http://bit.ly/\\n\\n31MgHUQ.\\n\\nYour First GAN\\n\\n|\\n\\n105\\n\\nx = Activation(self.generator_activation)(x)\\n\\nx = Reshape(self.generator_initial_dense_layer_size)(x)\\n\\nif self.generator_dropout_rate: x = Dropout(rate = self.generator_dropout_rate)(x)\\n\\nfor i in range(self.n_layers_generator):\\n\\nx = UpSampling2D()(x) x = Conv2D( filters = self.generator_conv_filters[i] , kernel_size = self.generator_conv_kernel_size[i] , padding = \\'same\\' , name = \\'generator_conv_\\' + str(i) )(x)\\n\\nif i < n_layers_generator - 1: if self.generator_batch_norm_momentum: x = BatchNormalization(momentum = self.generator_batch_norm_momentum))(x) x = Activation(\\'relu\\')(x) else: x = Activation(\\'tanh\\')(x)\\n\\ngenerator_output = x generator = Model(generator_input, generator_output)\\n\\nDefine the input to the generator—a vector of length 100.\\n\\nWe follow this with a Dense layer consisting of 3,136 units…\\n\\n…which, after applying batch normalization and a ReLU activation function, is reshaped to a 7 × 7 × 64 tensor.\\n\\nWe pass this through four Conv2D layers, the first two preceded by Upsampling2D layers, to reshape the tensor to 14 × 14, then 28 × 28 (the original image size). In all but the last layer, we use batch normalization and ReLU activation (LeakyReLU could also be used).\\n\\nAfter the final Conv2D layer, we use a tanh activation to transform the output to the range [–1, 1], to match the original image domain.\\n\\nThe Keras model that defines the generator—a model that accepts a vector of length 100 and outputs a tensor of shape [28, 28, 1].\\n\\n106\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nTraining the GAN As we have seen, the architecture of the generator and discriminator in a GAN is very simple and not so different from the models that we looked at earlier. The key to understanding GANs is in understanding the training process.\\n\\nWe can train the discriminator by creating a training set where some of the images are randomly selected real observations from the training set and some are outputs from the generator. The response would be 1 for the true images and 0 for the gener‐ ated images. If we treat this as a supervised learning problem, we can train the dis‐ criminator to learn how to tell the difference between the original and generated images, outputting values near 1 for the true images and values near 0 for the fake images.\\n\\nTraining the generator is more difficult as there is no training set that tells us the true image that a particular point in the latent space should be mapped to. Instead, we only want the image that is generated to fool the discriminator—that is, when the image is fed as input to the discriminator, we want the output to be close to 1.\\n\\nTherefore, to train the generator, we must first connect it to the discriminator to cre‐ ate a Keras model that we can train. Specifically, we feed the output from the genera‐ tor (a 28 × 28 × 1 image) into the discriminator so that the output from this combined model is the probability that the generated image is real, according to the discriminator. We can train this combined model by creating training batches con‐ sisting of randomly generated 100-dimensional latent vectors as input and a response which is set to 1, since we want to train the generator to produce images that the dis‐ criminator thinks are real.\\n\\nThe loss function is then just the binary cross-entropy loss between the output from the discriminator and the response vector of 1.\\n\\nCrucially, we must freeze the weights of the discriminator while we are training the combined model, so that only the generator’s weights are updated. If we do not freeze the discriminator’s weights, the discriminator will adjust so that it is more likely to predict generated images as real, which is not the desired outcome. We want gener‐ ated images to be predicted close to 1 (real) because the generator is strong, not because the discriminator is weak.\\n\\nA diagram of the training process for the discriminator and generator is shown in Figure 4-7.\\n\\nYour First GAN\\n\\n|\\n\\n107\\n\\nFigure 4-7. Training the GAN\\n\\nLet’s see what this looks like in code. First we need to compile the discriminator model and the model that trains the generator (Example 4-4).\\n\\nExample 4-4. Compiling the GAN\\n\\n### COMPILE MODEL THAT TRAINS THE DISCRIMINATOR\\n\\nself.discriminator.compile( optimizer= RMSprop(lr=0.0008) , loss = \\'binary_crossentropy\\' , metrics = [\\'accuracy\\'] )\\n\\n### COMPILE MODEL THAT TRAINS THE GENERATOR\\n\\nself.discriminator.trainable = False model_input = Input(shape=(self.z_dim,), name=\\'model_input\\') model_output = discriminator(self.generator(model_input))\\n\\n108\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nself.model = Model(model_input, model_output)\\n\\nself.model.compile( optimizer=RMSprop(lr=0.0004) , loss=\\'binary_crossentropy\\' , metrics=[\\'accuracy\\'] )\\n\\nThe discriminator is compiled with binary cross-entropy loss, as the response is binary and we have one output unit with sigmoid activation.\\n\\nNext, we freeze the discriminator weights—this doesn’t affect the existing dis‐ criminator model that we have already compiled.\\n\\nWe define a new model whose input is a 100-dimensional latent vector; this is passed through the generator and frozen discriminator to produce the output probability.\\n\\nAgain, we use a binary cross-entropy loss for the combined model—the learning rate is slower than the discriminator as generally we would like the discriminator to be stronger than the generator. The learning rate is a parameter that should be tuned carefully for each GAN problem setting.\\n\\nThen we train the GAN by alternating training of the discriminator and generator (Example 4-5).\\n\\nExample 4-5. Training the GAN\\n\\ndef train_discriminator(x_train, batch_size):\\n\\nvalid = np.ones((batch_size,1)) fake = np.zeros((batch_size,1))\\n\\n# TRAIN ON REAL IMAGES idx = np.random.randint(0, x_train.shape[0], batch_size) true_imgs = x_train[idx] self.discriminator.train_on_batch(true_imgs, valid)\\n\\n# TRAIN ON GENERATED IMAGES noise = np.random.normal(0, 1, (batch_size, z_dim)) gen_imgs = generator.predict(noise) self.discriminator.train_on_batch(gen_imgs, fake)\\n\\ndef train_generator(batch_size):\\n\\nvalid = np.ones((batch_size,1))\\n\\nYour First GAN\\n\\n|\\n\\n109\\n\\nnoise = np.random.normal(0, 1, (batch_size, z_dim)) self.model.train_on_batch(noise, valid)\\n\\nepochs = 2000 batch_size = 64\\n\\nfor epoch in range(epochs):\\n\\ntrain_discriminator(x_train, batch_size) train_generator(batch_size)\\n\\nOne batch update of the discriminator involves first training on a batch of true images with the response 1…\\n\\n…then on a batch of generated images with the response 0.\\n\\nOne batch update of the generator involves training on a batch of generated images with the response 1. As the discriminator is frozen, its weights will not be affected; instead, the generator weights will move in the direction that allows it to better generate images that are more likely to fool the discriminator (i.e., make the discriminator predict values close to 1).\\n\\nAfter a suitable number of epochs, the discriminator and generator will have found an equilibrium that allows the generator to learn meaningful information from the discriminator and the quality of the images will start to improve (Figure 4-8).\\n\\nFigure 4-8. Loss and accuracy of the discriminator and generator during training\\n\\nBy observing images produced by the generator at specific epochs during training (Figure 4-9), it is clear that the generator is becoming increasingly adept at producing images that could have been drawn from the training set.\\n\\n110\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nFigure 4-9. Output from the generator at specific epochs during training\\n\\nIt is somewhat miraculous that a neural network is able to convert random noise into something meaningful. It is worth remembering that we haven’t provided the model with any additional features beyond the raw pixels, so it has to work out high-level concepts such as how to draw a hump, legs, or a head entirely by itself. The Naive Bayes models that we saw in Chapter 1 wouldn’t be able to achieve this level of sophistication since they cannot model the interdependencies between pixels that are crucial to forming these high-level features.\\n\\nAnother requirement of a successful generative model is that it doesn’t only repro‐ duce images from the training set. To test this, we can find the image from the\\n\\nYour First GAN\\n\\n|\\n\\n111\\n\\ntraining set that is closest to a particular generated example. A good measure for dis‐ tance is the L1 distance, defined as:\\n\\ndef l1_compare_images(img1, img2): return np.mean(np.abs(img1 - img2))\\n\\nFigure 4-10 shows the closest observations in the training set for a selection of gener‐ ated images. We can see that while there is some degree of similarity between the gen‐ erated images and the training set, they are not identical and the GAN is also able to complete some of the unfinished drawings by, for example, adding legs or a head. This shows that the generator has understood these high-level features and can gen‐ erate examples that are distinct from those it has already seen.\\n\\nFigure 4-10. Closest matches of generated images from the training set\\n\\nGAN Challenges While GANs are a major breakthrough for generative modeling, they are also notori‐ ously difficult to train. We will explore some of the most common problems encoun‐ tered when training GANs in this section, then we will look at some adjustments to the GAN framework that remedy many of these problems.\\n\\nOscillating Loss The loss of the discriminator and generator can start to oscillate wildly, rather than exhibiting long-term stability. Typically, there is some small oscillation of the loss between batches, but in the long term you should be looking for loss that stabilizes or gradually increases or decreases (see Figure 4-8), rather than erratically fluctuating, to ensure your GAN converges and improves over time. Figure 4-11 shows an example of a GAN where the loss of the discriminator and generator has started to spiral out\\n\\n112\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nof control, at around batch 1,400. It is difficult to establish if or when this might occur as vanilla GANs are prone to this kind of instability.\\n\\nFigure 4-11. Oscillating loss in an unstable GAN\\n\\nMode Collapse Mode collapse occurs when the generator finds a small number of samples that fool the discriminator and therefore isn’t able to produce any examples other than this limited set. Let’s think about how this might occur. Suppose we train the generator over several batches without updating the discriminator in between. The generator would be inclined to find a single observation (also known as a mode) that always fools the discriminator and would start to map every point in the latent input space to this observation. This means that the gradient of the loss function collapses to near 0. Even if we then try to retrain the discriminator to stop it being fooled by this one point, the generator will simply find another mode that fools the discriminator, since it has already become numb to its input and therefore has no incentive to diversify its output. The effect of mode collapse can be seen in Figure 4-12.\\n\\nGAN Challenges\\n\\n|\\n\\n113\\n\\nFigure 4-12. Mode collapse\\n\\nUninformative Loss Since the deep learning model is compiled to minimize the loss function, it would be natural to think that the smaller the loss function of the generator, the better the qual‐ ity of the images produced. However, since the generator is only graded against the current discriminator and the discriminator is constantly improving, we cannot com‐ pare the loss function evaluated at different points in the training process. Indeed, in Figure 4-8, the loss function of the generator actually increases over time, even though the quality of the images is clearly improving. This lack of correlation between the generator loss and image quality sometimes makes GAN training diffi‐ cult to monitor.\\n\\nHyperparameters As we have seen, even with simple GANs, there are a large number of hyperparame‐ ters to tune. As well as the overall architecture of both the discriminator and the gen‐ erator, there are the parameters that govern the batch normalization, dropout, learning rate, activation layers, convolutional filters, kernel size, striding, batch size, and latent space size to consider. GANs are highly sensitive to very slight changes in all of these parameters, and finding a set of parameters that works is often a case of educated trial and error, rather than following an established set of guidelines.\\n\\n114\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nThis is why it is important to understand the inner workings of the GAN and know how to interpret the loss function—so that you can identify sensible adjustments to the hyperparameters that might improve the stability of the model.\\n\\nTackling the GAN Challenges In recent years, several key advancements have drastically improved the overall sta‐ bility of GAN models and diminished the likelihood of some of the problems listed earlier, such as mode collapse.\\n\\nIn the remainder of this chapter we shall explore two such advancements, the Wasser‐ stein GAN (WGAN) and Wasserstein GAN–Gradient Penalty (WGAN-GP). Both are only minor adjustments to the framework we have explored thus far. The latter is now considered best practice for training the most sophisticated GANs available today.\\n\\nWasserstein GAN The Wasserstein GAN was one of the first big steps toward stabilizing GAN training.4 With a few changes, the authors were able to show how to train GANs that have the following two properties (quoted from the paper):\\n\\nA meaningful loss metric that correlates with the generator’s convergence and sample quality\\n\\nImproved stability of the optimization process\\n\\nSpecifically, the paper introduces a new loss function for both the discriminator and the generator. Using this loss function instead of binary cross entropy results in a more stable convergence of the GAN. The mathematical explanation for this is beyond the scope of this book, but there are some excellent resources available online that explain the rationale behind switching to this loss function.\\n\\nLet’s take a look at the definition of the Wasserstein loss function.\\n\\nWasserstein Loss Let’s first remind ourselves of binary cross-entropy loss - the function that that we are currently using to train the discriminator and generator of the GAN:\\n\\n4 Martin Arjovsky et al., “Wasserstein GAN,” 26 January 2017, https://arxiv.org/pdf/1701.07875.pdf.\\n\\nWasserstein GAN\\n\\n|\\n\\n115\\n\\nBinary cross-entropy loss\\n\\n− 1\\n\\nn n ∑i = 1\\n\\nyi log pi + 1 − yi\\n\\nlog 1 − pi\\n\\nTo train the GAN discriminator D, we calculate the loss when comparing predictions for real images pi=D(xi) to the response yi=1 and predictions for generated images pi=D(G(zi)) to the response yi=0. Therefore for the GAN discriminator, minimizing the loss function can be written as follows:\\n\\nGAN discriminator loss minimization\\n\\nminD − x ∼ p\\n\\nX\\n\\nlog D x + z ∼ p\\n\\nZ\\n\\nlog 1 − D G z\\n\\nTo train the GAN generator G, we calculate the loss when comparing predictions for generated images pi=D(G(zi)) to the response yi=1. Therefore for the GAN generator, minimizing the loss function can be written as follows:\\n\\nGAN generator loss minimization\\n\\nminG − z ∼ p\\n\\nZ\\n\\nlog D G z\\n\\nNow let’s compare this to the Wasserstein loss function.\\n\\nFirst, the Wasserstein loss requires that we use yi=1 and yi=-1 as labels, rather than 1 and 0. We also remove the sigmoid activation from the final layer of the discrimina‐ tor, so that predictions pi are no longer constrained to fall in the range [0,1], but instead can now be any number in the range [–∞, ∞]. For this reason, the discrimina‐ tor in a WGAN is usually referred to as a critic. The Wasserstein loss function is then defined as follows:\\n\\nWasserstein loss\\n\\n− 1\\n\\nn n ∑i = 1\\n\\nyipi\\n\\nTo train the WGAN critic D, we calculate the loss when comparing predictions for a real images pi=D(xi) to the response yi=1 and predictions for generated images pi=D(G(zi)) to the response yi=-1. Therefore for the WGAN critic, minimizing the loss function can be written as follows:\\n\\n116\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nWGAN critic loss minimization\\n\\nminD − x ∼ p\\n\\nX\\n\\nD x − z ∼ p\\n\\nZ\\n\\nD G z\\n\\nIn other words, the WGAN critic tries to maximise the difference between its predic‐ tions for real images and generated images, with real images scoring higher.\\n\\nTo train the WGAN generator, we calculate the loss when comparing predictions for generated images pi=D(G(zi)) to the response yi=1. Therefore for the WGAN genera‐ tor, minimizing the loss function can be written as follows:\\n\\nWGAN generator loss minimization\\n\\nminG − z ∼ p\\n\\nZ\\n\\nD G z\\n\\nWhen we compile the models that train the WGAN critic and generator, we can spec‐ ify that we want to use the Wasserstein loss instead of the binary cross-entropy, as shown in Example 4-6. We also tend to use smaller learning rates for WGANs.\\n\\nExample 4-6. Compiling the models that train the critic and generator\\n\\ndef wasserstein(y_true, y_pred): return -K.mean(y_true * y_pred)\\n\\ncritic.compile( optimizer= RMSprop(lr=0.00005) , loss = wasserstein )\\n\\nmodel.compile( optimizer= RMSprop(lr=0.00005) , loss = wasserstein )\\n\\nThe Lipschitz Constraint It may surprise you that we are now allowing the critic to output any number in the range [–∞, ∞], rather than applying a sigmoid function to restrict the output to the usual [0, 1] range. The Wasserstein loss can therefore be very large, which is unset‐ tling—usually, large numbers in neural networks are to be avoided!\\n\\nIn fact, the authors of the WGAN paper show that for the Wasserstein loss function to work, we also need to place an additional constraint on the critic. Specifically, it is\\n\\nWasserstein GAN\\n\\n|\\n\\n117\\n\\nrequired that the critic is a 1-Lipschitz continuous function. Let’s pick this apart to understand what it means in more detail.\\n\\nThe critic is a function D that converts an image into a prediction. We say that this function is 1-Lipschitz if it satisfies the following inequality for any two input images, x1 and x2:\\n\\nD x1 − D x2 x1 − x2\\n\\n≤ 1\\n\\nHere, x1 – x2 is the average pixelwise absolute difference between two images and D x1 − D x2 is the absolute difference between the critic predictions. Essentially, we require a limit on the rate at which the predictions of the critic can change between two images (i.e., the absolute value of the gradient must be at most 1 every‐ where). We can see this applied to a Lipschitz continuous 1D function in Figure 4-13 —at no point does the line enter the cone, wherever you place the cone on the line. In other words, there is a limit on the rate at which the line can rise or fall at any point.\\n\\nFigure 4-13. A Lipschitz continuous function—there exists a double cone such that wherever it is placed on the line, the function always remains entirely outside the cone5\\n\\nFor those who want to delve deeper into the mathematical rationale behind why the Wasserstein loss only works when this constraint is enforced, Jonathan Hui offers an excellent explanation.\\n\\nWeight Clipping In the WGAN paper, the authors show how it is possible to enforce the Lipschitz con‐ straint by clipping the weights of the critic to lie within a small range, [–0.01, 0.01], after each training batch.\\n\\n5 Source: Wikipedia, http://bit.ly/2Xufwd8.\\n\\n118\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nWe can include this clipping process in our WGAN critic training function shown in Example 4-7.\\n\\nExample 4-7. Training the critic of the WGAN\\n\\ndef train_critic(x_train, batch_size, clip_threshold):\\n\\nvalid = np.ones((batch_size,1)) fake = -np.ones((batch_size,1))\\n\\n# TRAIN ON REAL IMAGES idx = np.random.randint(0, x_train.shape[0], batch_size) true_imgs = x_train[idx] self.critic.train_on_batch(true_imgs, valid)\\n\\n# TRAIN ON GENERATED IMAGES noise = np.random.normal(0, 1, (batch_size, self.z_dim)) gen_imgs = self.generator.predict(noise) self.critic.train_on_batch(gen_imgs, fake)\\n\\nfor l in critic.layers: weights = l.get_weights() weights = [np.clip(w, -clip_threshold, clip_threshold) for w in weights] l.set_weights(weights)\\n\\nTraining the WGAN When using the Wasserstein loss function, we should train the critic to convergence to ensure that the gradients for the generator update are accurate. This is in contrast to a standard GAN, where it is important not to let the discriminator get too strong, to avoid vanishing gradients.\\n\\nTherefore, using the Wasserstein loss removes one of the key difficulties of training GANs—how to balance the training of the discriminator and generator. With WGANs, we can simply train the critic several times between generator updates, to ensure it is close to convergence. A typical ratio used is five critic updates to one gen‐ erator update.\\n\\nThe training loop of the WGAN is shown in Example 4-8.\\n\\nExample 4-8. Training the WGAN\\n\\nfor epoch in range(epochs):\\n\\nfor _ in range(5): train_critic(x_train, batch_size = 128, clip_threshold = 0.01)\\n\\ntrain_generator(batch_size)\\n\\nWasserstein GAN\\n\\n|\\n\\n119\\n\\nWe have now covered all of the key differences between a standard GAN and a WGAN. To recap:\\n\\nA WGAN uses the Wasserstein loss.\\n\\nThe WGAN is trained using labels of 1 for real and –1 for fake.\\n\\nThere is no need for the sigmoid activation in the final layer of the WGAN critic.\\n\\nClip the weights of the critic after each update.\\n\\nTrain the critic multiple times for each update of the generator.\\n\\nAnalysis of the WGAN You can train your own WGAN using code from the Jupyter notebook 04_02_wgan_cifar_train.ipynb in the book repository. This will train a WGAN to generate images of horses from the CIFAR-10 dataset, which we used in Chapter 2.\\n\\nIn Figure 4-14 we show some of the samples generated by the WGAN.\\n\\nFigure 4-14. Examples from the generator of a WGAN trained on images of horses\\n\\nClearly, this is a much more difficult task than our previous ganimal example, but the WGAN has done a good job of establishing the key features of horse images (legs, sky, grass, brownness, shadow, etc.). As well as the images being in color, there are also many varying angles, shapes, and backgrounds for the WGAN to deal with in the training set. Therefore while the image quality isn’t yet perfect, we should be encour‐ aged by the fact that our WGAN is clearly learning the high-level features that make up a color photograph of a horse.\\n\\nOne of the main criticisms of the WGAN is that since we are clipping the weights in the critic, its capacity to learn is greatly diminished. In fact, even in the original\\n\\n120\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nWGAN paper the authors write, “Weight clipping is a clearly terrible way to enforce a Lipschitz constraint.”\\n\\nA strong critic is pivotal to the success of a WGAN, since without accurate gradients, the generator cannot learn how to adapt its weights to produce better samples.\\n\\nTherefore, other researchers have looked for alternative ways to enforce the Lipschitz constraint and improve the capacity of the WGAN to learn complex features. We shall explore one such breakthrough in the next section.\\n\\nWGAN-GP One of the most recent extensions to the WGAN literature is the Wasserstein GAN– Gradient Penalty (WGAN-GP) framework.6\\n\\nThe WGAN-GP generator is defined and compiled in exactly the same way as the WGAN generator. It is only the definition and compilation of the critic that we need to change.\\n\\nIn total, there are three changes we need to make to our WGAN critic to convert it to a WGAN-GP critic:\\n\\nInclude a gradient penalty term in the critic loss function.\\n\\nDon’t clip the weights of the critic.\\n\\nDon’t use batch normalization layers in the critic.\\n\\nLet’s start by seeing how we can build the gradient penalty term into our loss func‐ tion. In the paper introducing this variant, the authors propose an alternative way to enforce the Lipschitz constraint on the critic. Rather than clipping the weights of the critic, they show how the constraint can be enforced directly by including a term in the loss function that penalizes the model if the gradient norm of the critic deviates from 1. This is a much more natural way to achieve the constraint and results in a far more stable training process.\\n\\nThe Gradient Penalty Loss Figure 4-15 is a diagram of the training process for the critic. If we compare this to the original discriminator training process from Figure 4-7, we can see that the key addition is the gradient penalty loss included as part of the overall loss function, alongside the Wasserstein loss from the real and fake images.\\n\\n6 Ishaan Gulrajani et al., “Improved Training of Wasserstein GANs,” 31 March 2017, https://arxiv.org/abs/\\n\\n1704.00028.\\n\\nWGAN-GP\\n\\n|\\n\\n121\\n\\nFigure 4-15. The WGAN-GP critic training process\\n\\nThe gradient penalty loss measures the squared difference between the norm of the gradient of the predictions with respect to the input images and 1. The model will naturally be inclined to find weights that ensure the gradient penalty term is mini‐ mized, thereby encouraging the model to conform to the Lipschitz constraint.\\n\\nIt is intractable to calculate this gradient everywhere during the training process, so instead the WGAN-GP evaluates the gradient at only a handful of points. To ensure a balanced mix, we use a set of interpolated images that lie at randomly chosen points along lines connecting the batch of real images to the batch of fake images pairwise, as shown in Figure 4-16.\\n\\nFigure 4-16. Interpolating between images\\n\\nIn Keras, we can create a RandomWeightedAverage layer to perform this interpolating operation, by inheriting from the built-in _Merge layer:\\n\\nclass RandomWeightedAverage(_Merge): def __init__(self, batch_size): super().__init__() self.batch_size = batch_size\\n\\n122\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\ndef _merge_function(self, inputs): alpha = K.random_uniform((self.batch_size, 1, 1, 1)) return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\\n\\nEach image in the batch gets a random number, between 0 and 1, stored as the vector alpha.\\n\\nThe layer returns the set of pixelwise interpolated images that lie along the lines connecting the real images (inputs[0]) to the fake images (inputs[1]), pairwise, weighted by the alpha value for each pair.\\n\\nThe gradient_penalty_loss function shown in Example 4-9 returns the squared difference between the gradient calculated at the interpolated points and 1.\\n\\nExample 4-9. The gradient penalty loss function\\n\\ndef gradient_penalty_loss(y_true, y_pred, interpolated_samples):\\n\\ngradients = K.gradients(y_pred, interpolated_samples)[0]\\n\\ngradient_l2_norm = K.sqrt( K.sum( K.square(gradients), axis=[1:len(gradients.shape)] ) ) ) gradient_penalty = K.square(1 - gradient_l2_norm) return K.mean(gradient_penalty)\\n\\nThe Keras gradients function calculates the gradients of the predictions for the interpolated images (y_pred) with respect to the input (interpolated_samples).\\n\\nWe calculate the L2 norm of this vector (i.e., its Euclidean length).\\n\\nThe function returns the squared distance between the L2 norm and 1.\\n\\nNow that we have the RandomWeightedAverage layer that can interpolate between two images and the gradient_penalty_loss function that can calculate the gradient loss for the interpolated images, we can use both of these in the model compilation of the critic.\\n\\nIn the WGAN example, we compiled the critic directly, to predict if a given image was real or fake. To compile the WGAN-GP critic, we need to use the interpolated images in the loss function—however, Keras only permits a custom loss function with two parameters, the predictions and the true labels. To get around this issue, we use the Python partial function.\\n\\nWGAN-GP\\n\\n|\\n\\n123\\n\\nExample 4-10 shows the full compilation of the WGAN-GP critic in code.\\n\\nExample 4-10. Compiling the WGAN-GP critic\\n\\nfrom functools import partial\\n\\n### COMPILE CRITIC MODEL\\n\\nself.generator.trainable = False\\n\\nreal_img = Input(shape=self.input_dim) z_disc = Input(shape=(self.z_dim,)) fake_img = self.generator(z_disc)\\n\\nfake = self.critic(fake_img) valid = self.critic(real_img)\\n\\ninterpolated_img = RandomWeightedAverage(self.batch_size)([real_img, fake_img]) validity_interpolated = self.critic(interpolated_img)\\n\\npartial_gp_loss = partial(self.gradient_penalty_loss, interpolated_samples = interpolated_img) partial_gp_loss.__name__ = \\'gradient_penalty\\'\\n\\nself.critic_model = Model(inputs=[real_img, z_disc], outputs=[valid, fake, validity_interpolated])\\n\\nself.critic_model.compile( loss=[self.wasserstein,self.wasserstein, partial_gp_loss] ,optimizer=Adam(lr=self.critic_learning_rate, beta_1=0.5) ,loss_weights=[1, 1, self.grad_weight] )\\n\\nFreeze the weights of the generator. The generator forms part of the model that we are using to train the critic, as the interpolated images are now actively involved in the loss function, so this is required.\\n\\nThere are two inputs to the model: a batch of real images and a set of randomly generated numbers that are used to generate a batch of fake images.\\n\\nThe real and fake images are passed through the critic in order to calculate the Wasserstein loss.\\n\\nThe RandomWeightedAverage layer creates the interpolated images, which are then also passed through the critic.\\n\\nKeras is expecting a loss function with only two inputs—the predictions and true labels—so we define a custom loss function, partial_gp_loss, using the Python\\n\\n124\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\npartial function to pass the interpolated images through to our gradient_pen alty_loss function.\\n\\nKeras requires the function to be named.\\n\\nThe model that trains the critic is defined to have two inputs: the batch of real images and the random input that will generate the batch of fake images. The model has three outputs: 1 for the real images, –1 for the fake images, and a dummy 0 vector, which isn’t actually used but is required by Keras as every loss function must map to an output. Therefore we create the dummy 0 vector to map to the partial_gp_loss function.\\n\\nWe compile the critic with three loss functions: two Wasserstein losses for the real and fake images, and the gradient penalty loss. The overall loss is the sum of these three losses, with the gradient loss weighted by a factor of 10, in line with the recommendations from the original paper. We use the Adam optimizer, which is generally regarded to be the best optimizer for WGAN-GP models.\\n\\nBatch Normalization in WGAN-GP One last consideration we should note before building a WGAN-GP is that batch normalization shouldn’t be used in the critic. This is because batch normalization cre‐ ates correlation between images in the same batch, which makes the gradient penalty loss less effective. Experiments have shown that WGAN-GPs can still produce excel‐ lent results even without batch normalization in the critic.\\n\\nAnalysis of WGAN-GP Running the Jupyter notebook 04_03_wgangp_faces_train.ipynb in the book reposi‐ tory will train a WGAN-GP model on the CelebA dataset of celebrity faces.\\n\\nFirst, let’s take a look at some uncurated example outputs from the generator, after 3,000 training batches (Figure 4-17).\\n\\nWGAN-GP\\n\\n|\\n\\n125\\n\\nFigure 4-17. WGAN-GP CelebA examples\\n\\nClearly the model has learned the significant high-level attributes of a face, and there is no sign of mode collapse.\\n\\nWe can also see how the loss functions of the model evolve over time (Figure 4-18)— the loss functions of both the discriminator and generator are highly stable and con‐ vergent.\\n\\nFigure 4-18. WGAN-GP loss\\n\\nIf we compare the WGAN-GP output to the VAE output from the previous chapter, we can see that the GAN images are generally sharper—especially the definition between the hair and the background. This is true in general; VAEs tend to produce\\n\\n126\\n\\n|\\n\\nChapter 4: Generative Adversarial Networks\\n\\nsofter images that blur color boundaries, whereas GANs are known to produce sharper, more well-defined images.\\n\\nIt is also true that GANs are generally more difficult to train than VAEs and take longer to reach a satisfactory quality. However, most of the state-of-the-art generative models today are GAN-based, as the rewards for training large-scale GANs on GPUs over a longer period of time are significant.\\n\\nSummary In this chapter we have explored three distinct flavors of generative adversarial net‐ works, from the most fundamental vanilla GANs, through the Wasserstein GAN (WGAN), to the current state-of-the-art WGAN-GP models.\\n\\nAll GANs are characterized by a generator versus discriminator (or critic) architec‐ ture, with the discriminator trying to “spot the difference” between real and fake images and the generator aiming to fool the discriminator. By balancing how these two adversaries are trained, the GAN generator can gradually learn how to produce similar observations to those in the training set.\\n\\nWe saw how vanilla GANs can suffer from several problems, including mode collapse and unstable training, and how the Wasserstein loss function remedied many of these problems and made GAN training more predictable and reliable. The natural exten‐ sion of the WGAN is the WGAN-GP, which places the 1-Lipschitz requirement at the heart of the training process by including a term in the loss function to pull the gradi‐ ent norm toward 1.\\n\\nFinally, we applied our new technique to the problem of face generation and saw how by simply choosing points from a standard normal distribution, we can generate new faces. This sampling process is very similar to a VAE, though the faces produced by a GAN are quite different—often sharper, with greater distinction between different parts of the image. When trained on a large number of GPUs, this property allows GANs to produce extremely impressive results and has taken the field of generative modeling to ever greater heights.\\n\\nOverall, we have seen how the GAN framework is extremely flexible and able to be adapted to many interesting problem domains. We’ll look at one such application in the next chapter and explore how we can teach machines to paint.\\n\\nSummary\\n\\n|\\n\\n127\\n\\nPART II Teaching Machines to Paint, Write, Compose, and Play\\n\\nPart I introduced the field of generative deep learning and analyzed two of the most important advancements in recent years, variational autoencoders and generative adversarial networks. The rest of this book presents a set of case studies showing how generative modeling techniques can be applied to particular tasks. The next three chapters focus on three core pillars of human creativity: painting, writing, and musi‐ cal composition.\\n\\nIn Chapter 5, we shall examine two techniques relating to machine painting. First we will look at CycleGAN, which as the name suggests is an adaptation of the GAN architecture that allows the model to learn how to convert a photograph into a paint‐ ing in a particular style (or vice versa). Then we will also explore the neural style transfer technique contained within many photo editing apps that allows you to transfer the style of a painting onto a photograph, to give the impression that it is a painting by the same artist.\\n\\nIn Chapter 6, we shall turn our attention to machine writing, a task that presents dif‐ ferent challenges to image generation. This chapter introduces the recurrent neural network (RNN) architecture that allows us to tackle problems involving sequential data. We shall also see how the encoder–decoder architecture works and build a sim‐ ple question-answer generator.\\n\\nChapter 7 looks at music generation, which, while also a sequential generation prob‐ lem, presents additional challenges such as modeling musical pitch and rhythm. We’ll see that many of the techniques that worked for text generation can still be applied in\\n\\nthis domain, but we’ll also explore a deep learning architecture known as MuseGAN that applies ideas from Chapter 4 (on GANs) to musical data.\\n\\nChapter 8 shows how generative models can be used within other machine learning domains, such as reinforcement learning. This chapter presents one of the most excit‐ ing papers published in recent years, in which the authors show how a generative model can be used as the environment in which the agent trains, thus essentially allowing the agent to dream of possible future scenarios and imagine what might hap‐ pen if it were to take certain actions, entirely within its own conceptual model of the environment.\\n\\nFinally, Chapter 9 summarizes the current landscape of generative modeling and looks back on the techniques that have been presented in this book. We will also look to the future and explore how the most cutting-edge techniques available today might change the way in which we view creativity, and whether we will ever be able to create an artificial entity that can produce content that is creatively indistinguishable from works created by the human pioneers of art, literature, and music.\\n\\nCHAPTER 5 Paint\\n\\nSo far, we have explored various ways in which we can train a model to generate new samples, given only a training set of data we wish to imitate. We’ve applied this to sev‐ eral datasets and seen how in each case, VAEs and GANs are able to learn a mapping between an underlying latent space and the original pixel space. By sampling from a distribution in the latent space, we can use the generative model to map this vector to a novel image in the pixel space.\\n\\nNotice that all of the examples we have seen so far produce novel observations from scratch—that is, there is no input apart from the random latent vector sampled from the latent space that is used to generate the images.\\n\\nA different application of generative models is in the field of style transfer. Here, our aim is to build a model that can transform an input base image in order to give the impression that it comes from the same collection as a given set of style images. This technique has clear commercial applications and is now being used in computer graphics software, computer game design, and mobile phone applications. Some examples of this are shown in Figure 5-1.\\n\\nWith style transfer, our aim isn’t to model the underlying distribution of the style images, but instead to extract only the stylistic components from these images and embed these into the base image. We clearly cannot just merge the style images with the base image through interpolation, as the content of the style images would show through and the colors would become muddy and blurred. Moreover, it may be the style image set as a whole rather than one single image that captures the artist’s style, so we need to find a way to allow the model to learn about style across a whole collec‐ tion of images. We want to give the impression that the artist has used the base image as a guide to produce an original piece of artwork, complete with the same stylistic flair as other works in their collection.\\n\\n131\\n\\nFigure 5-1. Style transfer examples1\\n\\nIn this chapter you’ll learn how to build two different kinds of style transfer model (CycleGAN and Neural Style Transfer) and apply the techniques to your own photos and artwork.\\n\\nWe’ll start by visiting a fruit and vegetable shop where all is not as it seems…\\n\\nApples and Organges Granny Smith and Florida own a greengrocers together. To ensure the shop is run as efficiently as possible, they each look after different areas—specifically, Granny Smith takes great pride in her apple selection and Florida spends hours ensuring the oranges are perfectly arranged.\\n\\nBoth are so convinced that they have the better fruit display that they agree to a deal: the profits from the sales of apples will go entirely to Granny Smith and the profits from the sales of oranges will go entirely to Florida.\\n\\nUnfortunately, neither Granny Smith nor Florida plans on making this a fair contest. When Florida isn’t looking, Granny Smith sneaks into the orange section and starts painting the oranges red to look like apples! Florida has exactly the same plan, and tries to make Granny Smith’s apples more orange-like with a suitably colored spray when her back is turned.\\n\\nWhen customers bring their fruit to the self-checkout tills, they sometimes errone‐ ously select the wrong option on the machine. Customers who took apples sometimes\\n\\n1 Jun-Yan Zhu et al., “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks,” 30\\n\\nMarch 2017, https://arxiv.org/pdf/1703.10593.pdf.\\n\\n132\\n\\n|\\n\\nChapter 5: Paint\\n\\nput them through as oranges due to Florida’s fluorescent spray, and customers who took oranges wrongly pay for apples due to their clever disguise, courtesy of Granny Smith.\\n\\nAt the end of the day, the profits for each fruit are summed and split accordingly— Granny Smith loses money every time one of her apples is sold as an orange, and Florida loses every time one of her oranges is sold as an apple.\\n\\nAfter closing time, both of the disgruntled greengrocers do their best to clean up their own fruit stocks. However, instead of trying to undo the other’s mischievous adjust‐ ments, they both simply apply their own tampering process to their own fruit, to try to make it appear as it did before it was sabotaged. It’s important that they get this right, because if the fruit doesn’t look right, they won’t be able to sell it the next day and will again lose profits.\\n\\nTo ensure consistency over time, they also test their techniques out on their own fruit. Florida checks that if she sprays her oranges orange, they’ll look exactly as they did originally. Granny Smith tests her apple painting skills on her apples for the same reason. If they find that there are obvious discrepancies, they’ll have to spend their hard-earned profits on learning better techniques.\\n\\nThe overall process is shown in Figure 5-2.\\n\\nAt first, customers are inclined to make somewhat random selections at the newly installed self-checkout tills because of their inexperience with the machines. However, over time they become more adept at using the technology, and learn how to identify which fruit has been tampered with.\\n\\nThis forces Granny Smith and Florida to improve at sabotaging each other’s fruit, while also always ensuring that they are still able to use the same process to clean up their own fruit after it has been altered. Moreover, they must also make sure that the technique they use doesn’t affect the appearance of their own fruit.\\n\\nApples and Organges\\n\\n|\\n\\n133\\n\\nFigure 5-2. Diagram of the greengrocers’ tampering, restoration, and testing process\\n\\nAfter many days and weeks of this ridiculous game, they realize something amazing has happened. Customers are thoroughly confused and now can’t tell the difference between real and fake apples and real and fake oranges. Figure 5-3 shows the state of the fruit after tampering and restoration, as well as after testing.\\n\\n134\\n\\n|\\n\\nChapter 5: Paint\\n\\nFigure 5-3. Examples of the oranges and apples in the greengrocers’ store\\n\\nCycleGAN The preceding story is an allegory for a key development in generative modeling and, in particular, style transfer: the cycle-consistent adversarial network, or CycleGAN. The original paper represented a significant step forward in the field of style transfer as it showed how it was possible to train a model that could copy the style from a reference set of images onto a different image, without a training set of paired examples.2\\n\\nPrevious style transfer models, such as pix2pix,3 required each image in the training set to exist in both the source and target domain. While it is possible to manufacture this kind of dataset for some style problem settings (e.g., black and white to color photos, maps to satellite images), for others it is impossible. For example, we do not have original photographs of the pond where Monet painted his Water Lilies series, nor do we have a Picasso painting of the Empire State Building. It would also take\\n\\n2 Jun-Yan Zhu et al., “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks,” 30\\n\\nMarch 2017, https://arxiv.org/pdf/1703.10593.\\n\\n3 Phillip Isola et al., “Image-to-Image Translation with Conditional Adversarial Networks,” 2016, https://\\n\\narxiv.org/abs/1611.07004.\\n\\nCycleGAN\\n\\n|\\n\\n135\\n\\nenormous effort to arrange photos of horses and zebras standing in identical positions.\\n\\nThe CycleGAN paper was released only a few months after the pix2pix paper and shows how it is possible to train a model to tackle problems where we do not have pairs of images in the source and target domains. Figure 5-4 shows the difference between the paired and unpaired datasets of pix2pix and CycleGAN, respectively.\\n\\nFigure 5-4. pix2pix dataset and domain mapping example\\n\\nWhile pix2pix only works in one direction (from source to target), CycleGAN trains the model in both directions simultaneously, so that the model learns to translate images from target to source as well as source to target. This is a consequence of the model architecture, so you get the reverse direction for free.\\n\\nLet’s now see how we can build a CycleGAN model in Keras. To begin with, we shall be using the apples and oranges example from earlier to walk through each part of the CycleGAN and experiment with the architecture. We’ll then apply the same tech‐ nique to build a model that can apply a given artist’s style to a photo of your choice.\\n\\n136\\n\\n|\\n\\nChapter 5: Paint\\n\\nYour First CycleGAN Much of the following code has been inspired by and adapted from the amazing Keras-GAN repository maintained by Erik Linder-Norén. This is an excellent resource for many Keras examples of important GANs from the literature.\\n\\nTo begin, you’ll first need to download the data that we’ll be using to train the Cycle‐ GAN. From inside the folder where you cloned the book’s repository, run the follow‐ ing command:\\n\\nbash ./scripts/download_cyclegan_data.sh apple2orange\\n\\nThis will download the dataset of images of apples and oranges that we will be using. The data is split into four folders: trainA and testA contain images of apples and trainB and testB contain images of oranges. Thus domain A is the space of apple images and domain B is the space of orange images. Our goal is to train a model using the train datasets to convert images from domain A into domain B and vice versa. We will test our model using the test datasets.\\n\\nOverview A CycleGAN is actually composed of four models, two generators and two discrimi‐ nators. The first generator, G_AB, converts images from domain A into domain B. The second generator, G_BA, converts images from domain B into domain A.\\n\\nAs we do not have paired images on which to train our generators, we also need to train two discriminators that will determine if the images produced by the generators are convincing. The first discriminator, d_A, is trained to be able to identify the differ‐ ence between real images from domain A and fake images that have been produced by generator G_BA. Conversely, discriminator d_B is trained to be able to identify the difference between real images from domain B and fake images that have been pro‐ duced by generator G_AB. The relationship between the four models is shown in Figure 5-5.\\n\\nYour First CycleGAN\\n\\n|\\n\\n137\\n\\nFigure 5-5. Diagram of the four CycleGAN models4\\n\\nRunning the notebook 05_01_cyclegan_train.ipynb in the book repository will start training the CycleGAN. As in previous chapters, you can instantiate a CycleGAN object in the notebook, as shown in Example 5-1, and play around with the parame‐ ters to see how it affects the model.\\n\\nExample 5-1. Defining the CycleGAN\\n\\ngan = CycleGAN( input_dim = (128,128,3) , learning_rate = 0.0002 , lambda_validation = 1 , lambda_reconstr = 10 , lambda_id = 2 , generator_type = \\'u-net\\' , gen_n_filters = 32 , disc_n_filters = 32 )\\n\\nLet’s first take a look at the architecture of the generators. Typically, CycleGAN gener‐ ators take one of two forms: U-Net or ResNet (residual network). In their earlier pix2pix paper,5 the authors used a U-Net architecture, but they switched to a ResNet architecture for CycleGAN. We’ll be building both architectures in this chapter, start‐ ing with U-Net.6\\n\\n4 Source: Zhu et al., 2017, https://arxiv.org/pdf/1703.10593.pdf.\\n\\n5 Isola et al., 2016, https://arxiv.org/abs/1611.07004.\\n\\n6 Olaf Ronneberger, Philipp Fischer, and Thomas Brox, “U-Net: Convolutional Networks for Biomedical Image\\n\\nSegmentation,” 18 May 2015, https://arxiv.org/abs/1505.04597.\\n\\n138\\n\\n|\\n\\nChapter 5: Paint\\n\\nThe Generators (U-Net) Figure 5-6 shows the architecture of the U-Net we will be using—no prizes for guess‐ ing why it’s called a U-Net!7\\n\\nFigure 5-6. The U-Net architecture diagram\\n\\nIn a similar manner to a variational autoencoder, a U-Net consists of two halves: the downsampling half, where input images are compressed spatially but expanded channel-wise, and an upsampling half, where representations are expanded spatially while the number of channels is reduced.\\n\\nHowever, unlike in a VAE, there are also skip connections between equivalently shaped layers in the upsampling and downsampling parts of the network. A VAE is linear; data flows through the network from input to the output, one layer after another. A U-Net is different, because it contains skip connections that allow information to shortcut parts of the network and flow through to later layers.\\n\\n7 Ronneberger et al., 2015, https://arxiv.org/abs/1505.04597.\\n\\nYour First CycleGAN\\n\\n|\\n\\n139\\n\\nThe intuition here is that with each subsequent layer in the downsampling part of the network, the model increasingly captures the what of the images and loses informa‐ tion on the where. At the apex of the U, the feature maps will have learned a contex‐ tual understanding of what is in the image, with little understanding of where it is located. For predictive classification models, this is all we require, so we could con‐ nect this to a final Dense layer to output the probability of a particular class being present in the image. However, for the original U-Net application (image segmenta‐ tion) and also for style transfer, it is critical that when we upsample back to the origi‐ nal image size, we pass back into each layer the spatial information that was lost during downsampling. This is exactly why we need the skip connections. They allow the network to blend high-level abstract information captured during the downsam‐ pling process (i.e., the image style) with the specific spatial information that is being fed back in from previous layers in the network (i.e., the image content).\\n\\nTo build in the skip connections, we will need to introduce a new type of layer: `Concatenate`.\\n\\nConcatenate Layer The Concatenate layer simply joins a set of layers together along a particular axis (by default, the last axis). For example, in Keras, we can join two previous layers, x and y together as follows:\\n\\nConcatenate()([x,y])\\n\\nIn the U-Net, we use Concatenate layers to connect upsampling layers to the equiva‐ lently sized layer in the downsampling part of the network. The layers are joined together along the channels dimension so the number of channels increases from k to 2k, while the number of spatial dimensions remains the same.\\n\\nNote that there are no weights to be learned in a Concatenate layer; they are just used to “glue” previous layers together.\\n\\nThe generator also contains another new layer type, InstanceNormalization.\\n\\nInstance Normalization Layer The generator of this CycleGAN uses InstanceNormalization layers rather than BatchNormalization layers, which in style transfer problems can lead to more satis‐ fying results.8\\n\\n8 Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, “Instance Normalization: The Missing Ingredient for\\n\\nFast Stylization,” 27 July 2016, https://arxiv.org/pdf/1607.08022.pdf.\\n\\n140\\n\\n|\\n\\nChapter 5: Paint\\n\\nAn InstanceNormalization layer normalizes every single observation individually, rather than as a batch. Unlike a BatchNormalization layer, it doesn’t require mu and sigma parameters to be calculated as a running average during training, since at test time the layer can normalize per instance in the same way as it does at train time. The means and standard deviations used to normalize each layer are calculated per chan‐ nel and per observation.\\n\\nAlso, for the InstanceNormalization layers in this network, there are no weights to learn since we do not use scaling (gamma) or shift (beta) parameters.\\n\\nFigure 5-7 shows the difference between batch normalization and instance normal‐ ization, as well as two other normalization methods (layer and group normalization).\\n\\nFigure 5-7. Four different normalization methods.9\\n\\nHere, N is the batch axis, C is the channel axis, and (H, W) represent the spatial axes. The cube therefore represents the input tensor to the normalization layer. Pixels col‐ ored blue are normalized by the same mean and variance (calculated over the values of these pixels).\\n\\nWe now have everything we need to build a U-Net generator in Keras, as shown in Example 5-2.\\n\\nExample 5-2. Building the U-Net generator\\n\\ndef build_generator_unet(self):\\n\\ndef downsample(layer_input, filters, f_size=4): d = Conv2D(filters, kernel_size=f_size , strides=2, padding=\\'same\\')(layer_input) d = InstanceNormalization(axis = -1, center = False, scale = False)(d) d = Activation(\\'relu\\')(d)\\n\\n9 Source: Yuxin Wu and Kaiming He, “Group Normalization,” 22 March 2018, https://arxiv.org/pdf/\\n\\n1803.08494.pdf.\\n\\nYour First CycleGAN\\n\\n|\\n\\n141\\n\\nreturn d\\n\\ndef upsample(layer_input, skip_input, filters, f_size=4, dropout_rate=0): u = UpSampling2D(size=2)(layer_input) u = Conv2D(filters, kernel_size=f_size, strides=1, padding=\\'same\\')(u) u = InstanceNormalization(axis = -1, center = False, scale = False)(u) u = Activation(\\'relu\\')(u) if dropout_rate: u = Dropout(dropout_rate)(u)\\n\\nu = Concatenate()([u, skip_input]) return u\\n\\n# Image input img = Input(shape=self.img_shape)\\n\\n# Downsampling d1 = downsample(img, self.gen_n_filters) d2 = downsample(d1, self.gen_n_filters*2) d3 = downsample(d2, self.gen_n_filters*4) d4 = downsample(d3, self.gen_n_filters*8)\\n\\n# Upsampling u1 = upsample(d4, d3, self.gen_n_filters*4) u2 = upsample(u1, d2, self.gen_n_filters*2) u3 = upsample(u2, d1, self.gen_n_filters)\\n\\nu4 = UpSampling2D(size=2)(u3)\\n\\noutput = Conv2D(self.channels, kernel_size=4, strides=1 , padding=\\'same\\', activation=\\'tanh\\')(u4)\\n\\nreturn Model(img, output)\\n\\nThe generator consists of two halves. First, we downsample the image, using Conv2D layers with stride 2.\\n\\nThen we upsample, to return the tensor to the same size as the original image. The upsampling blocks contain Concatenate layers, which give the network the U-Net architecture.\\n\\nThe Discriminators The discriminators that we have seen so far have output a single number: the predic‐ ted probability that the input image is “real.” The discriminators in the CycleGAN that we will be building output an 8 × 8 single-channel tensor rather than a single number.\\n\\n142\\n\\n|\\n\\nChapter 5: Paint\\n\\nThe reason for this is that the CycleGAN inherits its discriminator architecture from a model known as a PatchGAN, where the discriminator divides the image into square overlapping “patches” and guesses if each patch is real or fake, rather than pre‐ dicting for the image as a whole. Therefore the output of the discriminator is a tensor that contains the predicted probability for each patch, rather than just a single number.\\n\\nNote that the patches are predicted simultaneously as we pass an image through the network—we do not divide up the image manually and pass each patch through the network one by one. The division of the image into patches arises naturally as a result of the discriminator’s convolutional architecture.\\n\\nThe benefit of using a PatchGAN discriminator is that the loss function can then measure how good the discriminator is at distinguishing images based on their style rather than their content. Since each individual element of the discriminator predic‐ tion is based only on a small square of the image, it must use the style of the patch, rather than its content, to make its decision. This is exactly what we require; we would rather our discriminator is good at identifying when two images differ in style than content.\\n\\nThe Keras code to build the discriminators is provided in Example 5-3.\\n\\nExample 5-3. Building the discriminators\\n\\ndef build_discriminator(self):\\n\\ndef conv4(layer_input,filters, stride = 2, norm=True): y = Conv2D(filters, kernel_size=4, strides=stride , padding=\\'same\\')(layer_input)\\n\\nif norm: y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\\n\\ny = LeakyReLU(0.2)(y)\\n\\nreturn y\\n\\nimg = Input(shape=self.img_shape)\\n\\ny = conv4(img, self.disc_n_filters, stride = 2, norm = False) y = conv4(y, self.disc_n_filters*2, stride = 2) y = conv4(y, self.disc_n_filters*4, stride = 2) y = conv4(y, self.disc_n_filters*8, stride = 1)\\n\\noutput = Conv2D(1, kernel_size=4, strides=1, padding=\\'same\\')(y)\\n\\nreturn Model(img, output)\\n\\nYour First CycleGAN\\n\\n|\\n\\n143\\n\\nA CycleGAN discriminator is a series of convolutional layers, all with instance normalization (except the first layer).\\n\\nThe final layer is a convolutional layer with only one filter and no activation.\\n\\nCompiling the CycleGAN To recap, we aim to build a set of models that can convert images that are in domain A (e.g., images of apples) to domain B (e.g., images of oranges) and vice versa. We therefore need to compile four distinct models, two generators and two discrimina‐ tors, as follows:\\n\\ng_AB\\n\\nLearns to convert an image from domain A to domain B.\\n\\ng_BA\\n\\nLearns to convert an image from domain B to domain A.\\n\\nd_A\\n\\nLearns the difference between real images from domain A and fake images gen‐ erated by g_BA.\\n\\nd_B\\n\\nLearns the difference between real images from domain B and fake images gener‐ ated by g_AB.\\n\\nWe can compile the two discriminators directly, as we have the inputs (images from each domain) and outputs (binary responses: 1 if the image was from the domain or 0 if it was a generated fake). This is shown in Example 5-4.\\n\\nExample 5-4. Compiling the discriminator\\n\\nself.d_A = self.build_discriminator() self.d_B = self.build_discriminator() self.d_A.compile(loss=\\'mse\\', optimizer=Adam(self.learning_rate, 0.5), metrics=[\\'accuracy\\']) self.d_B.compile(loss=\\'mse\\', optimizer=Adam(self.learning_rate, 0.5), metrics=[\\'accuracy\\'])\\n\\n144\\n\\n|\\n\\nChapter 5: Paint\\n\\nHowever, we cannot compile the generators directly, as we do not have paired images in our dataset. Instead, we judge the generators simultaneously on three criteria:\\n\\n1. Validity. Do the images produced by each generator fool the relevant discrimina‐ tor? (For example, does output from g_BA fool d_A and does output from g_AB fool d_B?)\\n\\n2. Reconstruction. If we apply the two generators one after the other (in both direc‐ tions), do we return to the original image? The CycleGAN gets its name from this cyclic reconstruction criterion.\\n\\n3. Identity. If we apply each generator to images from its own target domain, does the image remain unchanged?\\n\\nExample 5-5 shows how we can compile a model to enforce these three criteria (the numeric markers in the code correspond to the preceding list).\\n\\nExample 5-5. Building the combined model to train the generators\\n\\nself.g_AB = self.build_generator_unet() self.g_BA = self.build_generator_unet()\\n\\nself.d_A.trainable = False self.d_B.trainable = False\\n\\nimg_A = Input(shape=self.img_shape) img_B = Input(shape=self.img_shape) fake_A = self.g_BA(img_B) fake_B = self.g_AB(img_A)\\n\\nvalid_A = self.d_A(fake_A) valid_B = self.d_B(fake_B)\\n\\nreconstr_A = self.g_BA(fake_B) reconstr_B = self.g_AB(fake_A)\\n\\nimg_A_id = self.g_BA(img_A) img_B_id = self.g_AB(img_B)\\n\\nself.combined = Model(inputs=[img_A, img_B], outputs=[ valid_A, valid_B, reconstr_A, reconstr_B, img_A_id, img_B_id ])\\n\\nself.combined.compile(loss=[\\'mse\\', \\'mse\\', \\'mae\\', \\'mae\\', \\'mae\\', \\'mae\\'], loss_weights=[ self.lambda_validation , self.lambda_validation\\n\\nYour First CycleGAN\\n\\n|\\n\\n145\\n\\n, self.lambda_reconstr , self.lambda_reconstr , self.lambda_id , self.lambda_id ], optimizer=optimizer)\\n\\nThe combined model accepts a batch of images from each domain as input and pro‐ vides three outputs (to match the three criteria) for each domain—so, six outputs in total. Notice how we freeze the weights in the discriminator, as is typical with GANs, so that the combined model only trains the generator weights, even though the dis‐ criminator is involved in the model.\\n\\nThe overall loss is the weighted sum of the loss for each criterion. Mean squared error is used for the validity criterion—checking the output from the discriminator against the real (1) or fake (0) response—and mean absolute error is used for the image-to- image-based criteria (reconstruction and identity).\\n\\nTraining the CycleGAN With our discriminators and combined model compiled, we can now train our mod‐ els. This follows the standard GAN practice of alternating the training of the discrim‐ inators with the training of the generators (through the combined model).\\n\\nIn Keras, the code in Example 5-6 describes the training loop.\\n\\nExample 5-6. Training the CycleGAN\\n\\nbatch_size = 1 patch = int(self.img_rows / 2**4) self.disc_patch = (patch, patch, 1)\\n\\nvalid = np.ones((batch_size,) + self.disc_patch) fake = np.zeros((batch_size,) + self.disc_patch)\\n\\nfor epoch in range(self.epoch, epochs): for batch_i, (imgs_A, imgs_B) in enumerate(data_loader.load_batch(batch_size)):\\n\\nfake_B = self.g_AB.predict(imgs_A) fake_A = self.g_BA.predict(imgs_B)\\n\\ndA_loss_real = self.d_A.train_on_batch(imgs_A, valid) dA_loss_fake = self.d_A.train_on_batch(fake_A, fake) dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\\n\\ndB_loss_real = self.d_B.train_on_batch(imgs_B, valid) dB_loss_fake = self.d_B.train_on_batch(fake_B, fake) dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\\n\\n146\\n\\n|\\n\\nChapter 5: Paint\\n\\nd_loss = 0.5 * np.add(dA_loss, dB_loss)\\n\\ng_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, valid, imgs_A, imgs_B, imgs_A, imgs_B])\\n\\nWe use a response of 1 for real images and 0 for generated images. Notice how there is one response per patch, as we are using a PatchGAN discriminator.\\n\\nTo train the discriminators, we first use the respective generator to create a batch of fake images, then we train each discriminator on this fake set and a batch of real images. Typically, for a CycleGAN the batch size is 1 (a single image).\\n\\nThe generators are trained together in one step, through the combined model compiled earlier. See how the six outputs match to the six loss functions defined earlier during compilation.\\n\\nAnalysis of the CycleGAN Let’s see how the CycleGAN performs on our simple dataset of apples and oranges and observe how changing the weighting parameters in the loss function can have dramatic effects on the results.\\n\\nWe have already seen an example of the output from the CycleGAN model in Figure 5-3. Now that you are familiar with the CycleGAN architecture, you might recognize that this image represents the three criteria through which the combined model is judged: validity, reconstruction, and identity.\\n\\nLet’s relabel this image with the appropriate functions from the codebase, so that we can see this more explicitly (Figure 5-8).\\n\\nWe can see that the training of the network has been successful, because each genera‐ tor is visibly altering the input picture to look more like a valid image from the oppo‐ site domain. Moreover, when the generators are applied one after the other, the difference between the input image and the reconstructed image is minimal. Finally, when each generator is applied to an image from its own input domain, the image doesn’t change significantly.\\n\\nYour First CycleGAN\\n\\n|\\n\\n147\\n\\nFigure 5-8. Outputs from the combined model used to calculated the overall loss function\\n\\nIn the original CycleGAN paper, the identity loss was included as an optional addi‐ tion to the necessary reconstruction loss and validity loss. To demonstrate the impor‐ tance of the identity term in the loss function, let’s see what happens if we remove it, by setting the identity loss weighting parameter to zero in the loss function (Figure 5-9).\\n\\n148\\n\\n|\\n\\nChapter 5: Paint\\n\\nFigure 5-9. Output from the CycleGAN when the identity loss weighting is set to zero\\n\\nThe CycleGAN has still managed to translate the oranges into apples but the color of the tray holding the oranges has flipped from black to white, as there is now no iden‐ tity loss term to prevent this shift in background colors. The identity term helps regu‐ late the generator to ensure that it only adjust parts of the image that are necessary to complete the transformation and no more.\\n\\nThis highlights the importance of ensuring the weightings of the three loss functions are well balanced—too little identity loss and the color shift problem appears; too much identity loss and the CycleGAN isn’t sufficiently incentivized to change the input to look like an image from the opposite domain.\\n\\nCreating a CycleGAN to Paint Like Monet Now that we have explored the fundamental structure of a CycleGAN, we can turn our attention to more interesting and impressive applications of the technique.\\n\\nIn the original CycleGAN paper, one of the standout achievements was the ability for the model to learn how to convert a given photo into a painting in the style of a par‐ ticular artist. As this is a CycleGAN, the model is also able to translate the other way, converting an artist’s paintings into realistic-looking photographs.\\n\\nTo download the Monet-to-photo dataset, run the following command from inside the book repository:\\n\\nbash ./scripts/download_cyclegan_data.sh monet2photo\\n\\nThis time we will use the parameter set shown in Example 5-7 to build the model:\\n\\nExample 5-7. Defining the Monet CycleGAN\\n\\ngan = CycleGAN( input_dim = (256,256,3) , learning_rate = 0.0002 , lambda_validation = 1 , lambda_reconstr = 10 , lambda_id = 5\\n\\nCreating a CycleGAN to Paint Like Monet\\n\\n|\\n\\n149\\n\\n, generator_type = \\'resnet\\' , gen_n_filters = 32 , disc_n_filters = 64 )\\n\\nThe Generators (ResNet) In this example, we shall introduce a new type of generator architecture: a residual network, or ResNet.10 The ResNet architecture is similar to a U-Net in that it allows information from previous layers in the network to skip ahead one or more layers. However, rather than creating a U shape by connecting layers from the downsam‐ pling part of the network to corresponding upsampling layers, a ResNet is built of residual blocks stacked on top of each other, where each block contains a skip con‐ nection that sums the input and output of the block, before passing this on to the next layer. A single residual block is shown in Figure 5-10.\\n\\nFigure 5-10. A single residual block\\n\\nIn our CycleGAN, the “weight layers” in the diagram are convolutional layers with instance normalization. In Keras, a residual block can be coded as shown in Example 5-8.\\n\\nExample 5-8. A residual block in Keras\\n\\nfrom keras.layers.merge import add\\n\\ndef residual(layer_input, filters): shortcut = layer_input y = Conv2D(filters, kernel_size=(3, 3), strides=1, padding=\\'same\\')(layer_input) y = InstanceNormalization(axis = -1, center = False, scale = False)(y) y = Activation(\\'relu\\')(y)\\n\\n10 Kaiming He et al., “Deep Residual Learning for Image Recognition,” 10 December 2015, https://arxiv.org/abs/\\n\\n1512.03385.\\n\\n150\\n\\n|\\n\\nChapter 5: Paint\\n\\ny = Conv2D(filters, kernel_size=(3, 3), strides=1, padding=\\'same\\')(y) y = InstanceNormalization(axis = -1, center = False, scale = False)(y)\\n\\nreturn add([shortcut, y])\\n\\nOn either side of the residual blocks, our ResNet generator also contains downsam‐ pling and upsampling layers. The overall architecture of the ResNet is shown in Figure 5-11.\\n\\nFigure 5-11. A ResNet generator\\n\\nIt has been shown that ResNet architectures can be trained to hundreds and even thousands of layers deep and not suffer from the vanishing gradient problem, where the gradients at early layers are tiny and therefore train very slowly. This is due to the fact that the error gradients can backpropagate freely through the network through the skip connections that are part of the residual blocks. Furthermore, it is believed that adding additional layers never results in a drop in model accuracy, as the skip connections ensure that it is always possible to pass through the identity mapping from the previous layer, if no further informative features can be extracted.\\n\\nAnalysis of the CycleGAN In the original CycleGAN paper, the model was trained for 200 epochs to achieve state-of-the-art results for artist-to-photograph style transfer. In Figure 5-12 we show the output from each generator at various stages of the early training process, to show the progression as the model begins to learn how to convert Monet paintings into photographs and vice versa.\\n\\nIn the top row, we can see that gradually the distinctive colors and brushstrokes used by Monet are transformed into the more natural colors and smooth edges that would be expected in a photograph. Similarly, the reverse is happening in the bottom row, as the generator learns how to convert a photograph into a scene that Monet might have painted himself.\\n\\nCreating a CycleGAN to Paint Like Monet\\n\\n|\\n\\n151\\n\\nFigure 5-12. Output at various stages of the training process\\n\\nFigure 5-13 shows some of the results from the original paper achieved by the model after it was trained for 200 epochs.\\n\\nFigure 5-13. Output after 200 epochs of training11\\n\\n11 Source: Zhu et al., 2017, https://junyanz.github.io/CycleGAN.\\n\\n152\\n\\n|\\n\\nChapter 5: Paint\\n\\nNeural Style Transfer So far, we have seen how a CycleGAN can transpose images between two domains, where the images in the training set are not necessarily paired. Now we shall look at a different application of style transfer, where we do not have a training set at all, but instead wish to transfer the style of one single image onto another, as shown in Figure 5-14. This is known as neural style transfer.12\\n\\nFigure 5-14. An example of neural style transfer13\\n\\nThe idea works on the premise that we want to minimize a loss function that is a weighted sum of three distinct parts:\\n\\nContent loss\\n\\nWe would like the combined image to contain the same content as the base image.\\n\\nStyle loss\\n\\nWe would like the combined image to have the same general style as the style image.\\n\\nTotal variance loss\\n\\nWe would like the combined image to appear smooth rather than pixelated.\\n\\nWe minimize this loss via gradient descent—that is, we update each pixel value by an amount proportional to the negative gradient of the loss function, over many\\n\\n12 Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A Neural Algorithm of Artistic Style,” 26 August\\n\\n2015, https://arxiv.org/abs/1508.06576.\\n\\n13 Source: Gatys, et al. 2015, https://arxiv.org/abs/1508.06576.\\n\\nNeural Style Transfer\\n\\n|\\n\\n153\\n\\niterations. This way, the loss gradually decreases with each iteration and we end up with an image that merges the content of one image with the style of another.\\n\\nOptimizing the generated output via gradient descent is different to how we have tackled generative modeling problems thus far. Previously we have trained a deep neural network such as a VAE or GAN by backpropagating the error through the entire network to learn from a training set of data and generalize the information learned to generate new images. Here, we cannot take this approach as we only have two images to work with, the base image and the style image. However, as we shall see, we can still make use of a pretrained deep neural network to provide vital infor‐ mation about each image inside the loss functions.\\n\\nWe’ll start by defining the three individual loss functions, as they are the core of the neural style transfer engine.\\n\\nContent Loss The content loss measures how different two images are in terms of the subject mat‐ ter and overall placement of their content. Two images that contain similar-looking scenes (e.g., a photo of a row of buildings and another photo of the same buildings taken in different light from a different angle) should have a smaller loss than two images that contain completely different scenes. Simply comparing the pixel values of the two images won’t do, because even in two distinct images of the same scene, we wouldn’t expect individual pixel values to be similar. We don’t really want the content loss to care about the values of individual pixels; we’d rather that it scores images based on the presence and approximate position of higher-level features such as buildings, sky, or river.\\n\\nWe’ve seen this concept before. It’s the whole premise behind deep learning—a neural network trained to recognize the content of an image naturally learns higher-level features at deeper layers of the network by combining simpler features from previous layers. Therefore, what we need is a deep neural network that has already been suc‐ cessfully trained to identify the content of an image, so that we can tap into a deep layer of the network to extract the high-level features of a given input image. If we measure the mean squared error between this output for the base image and the cur‐ rent combined image, we have our content loss function!\\n\\nThe pretrained network that we shall be using is called VGG19. This is a 19-layer convolutional neural network that has been trained to classify images into one thou‐ sand object categories on more than one million images from the ImageNet dataset. A diagram of the network is shown in Figure 5-15.\\n\\n154\\n\\n|\\n\\nChapter 5: Paint\\n\\nFigure 5-15. The VGG19 model\\n\\nExample 5-9 is a code snippet that calculates the content loss between two images, adapted from the neural style transfer example in the official Keras repository. If you want to re-create this technique and experiment with the parameters, I suggest work‐ ing from this repository as a starting point.\\n\\nExample 5-9. The content loss function\\n\\nfrom keras.applications import vgg19 from keras import backend as K\\n\\nbase_image_path = \\'/path_to_images/base_image.jpg\\' style_reference_image_path = \\'/path_to_images/styled_image.jpg\\'\\n\\ncontent_weight = 0.01\\n\\nbase_image = K.variable(preprocess_image(base_image_path)) style_reference_image = K.variable(preprocess_image(style_reference_image_path)) combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\\n\\ninput_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0)\\n\\nmodel = vgg19.VGG19(input_tensor=input_tensor, weights=\\'imagenet\\', include_top=False)\\n\\nNeural Style Transfer\\n\\n|\\n\\n155\\n\\noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) layer_features = outputs_dict[\\'block5_conv2\\']\\n\\nbase_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :]\\n\\ndef content_loss(content, gen): return K.sum(K.square(gen - content))\\n\\ncontent_loss = content_weight * content_loss(base_image_features , combination_features)\\n\\nThe Keras library contains a pretrained VGG19 model that can be imported.\\n\\nWe define two Keras variables to hold the base image and style image and a placeholder that will contain the generated combined image.\\n\\nThe input tensor to the VGG19 model is a concatenation of the three images.\\n\\nHere we create an instance of the VGG19 model, specifying the input tensor and the weights that we would like to preload. The include_top = False parameter specifies that we do not need to load the weights for the final dense layers of the networks that result in the classification of the image. This is because we are only interested in the preceding convolutional layers, which capture the high-level fea‐ tures of an input image, not the actual probabilities that the original model was trained to output.\\n\\nThe layer that we use to calculate the content loss is the second convolutional layer of the fifth block. Choosing a layer at a shallower or deeper point in the net‐ work affects how the loss function defines “content” and therefore alters the properties of the generated combined image.\\n\\nHere we extract the base image features and combined image features from the input tensor that has been fed through the VGG19 network.\\n\\nThe content loss is the sum of squares distance between the outputs of the chosen layer for both images, multiplied by a weighting parameter.\\n\\nStyle Loss Style loss is slightly more difficult to quantify—how can we measure the similarity in style between two images?\\n\\nThe solution given in the neural style transfer paper is based on the idea that images that are similar in style typically have the same pattern of correlation between feature maps in a given layer. We can see this more clearly with an example.\\n\\n156\\n\\n|\\n\\nChapter 5: Paint\\n\\nSuppose in the VGG19 network we have some layer where one channel has learned to identify parts of the image that are colored green, another channel has learned to identify spikiness, and another has learned to identify parts of the image that are brown.\\n\\nThe output from these channels (feature maps) for three inputs is shown in Figure 5-16.\\n\\nFigure 5-16. The output from three channels (feature maps) for three given input images —the darker orange colors represent larger values\\n\\nWe can see that A and B are similar in style—both are grassy. Image C is slightly dif‐ ferent in style to images A and B. If we look at the feature maps, we can see that the green and spikiness channels often fire strongly together at the same spatial point in images A and B, but not in image C. Conversely, the brown and spikiness channels are often activated together at the same point in image C, but not in images A and B. To numerically measure how much two feature maps are jointly activated together, we can flatten them and calculate the dot product.14 If the resulting value is high, the fea‐ ture maps are highly correlated; if the value is low, the feature maps are not correlated.\\n\\n14 To calculate the dot product between two vectors, multiply the values of the two vectors in each position and\\n\\nsum the results.\\n\\nNeural Style Transfer\\n\\n|\\n\\n157\\n\\nWe can define a matrix that contains the dot product between all possible pairs of fea‐ tures in the layer. This is called a Gram matrix. Figure 5-17 shows the Gram matrices for the three features, for each of the images.\\n\\nFigure 5-17. Parts of the Gram matrices for the three images—the darker blue colors rep‐ resent larger values\\n\\nIt is clear that images A and B, which are similar in style, have similar Gram matrices for this layer. Even though their content may be very different, the Gram matrix—a measure of correlation between all pairs of features in the layer—is similar.\\n\\nTherefore to calculate the style loss, all we need to do is calculate the Gram matrix (GM) for a set of layers throughout the network for both the base image and the com‐ bined image and compare their similarity using sum of squared errors. Algebraically, the style loss between the base image (S) and the generated image (G) for a given layer (l) of size Ml (height x width) with Nl channels can be written as follows:\\n\\nLGM S, G, l =\\n\\n1 2Ml 4Nl\\n\\n2 ∑ i j\\n\\nGM l S i j − GM l G i j\\n\\n2\\n\\nNotice how this is scaled to account for the number of channels (Nl) and size of the layer (Ml). This is because we calculate the overall style loss as a weighted sum across\\n\\n158\\n\\n|\\n\\nChapter 5: Paint\\n\\nseveral layers, all of which have different sizes. The total style loss is then calculated as follows:\\n\\nL Lstyle S, G = ∑ l = 0\\n\\nwlLGM S, G, l\\n\\nIn Keras, the style loss calculations can be coded as shown in Example 5-10.15\\n\\nExample 5-10. The style loss function\\n\\nstyle_loss = 0.0\\n\\ndef gram_matrix(x): features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram\\n\\ndef style_loss(style, combination): S = gram_matrix(style) C = gram_matrix(combination) channels = 3 size = img_nrows * img_ncols return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\\n\\nfeature_layers = [\\'block1_conv1\\', \\'block2_conv1\\', \\'block3_conv1\\', \\'block4_conv1\\', \\'block5_conv1\\']\\n\\nfor layer_name in feature_layers: layer_features = outputs_dict[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] sl = style_loss(style_reference_features, combination_features) style_loss += (style_weight / len(feature_layers)) * sl\\n\\nThe style loss is calculated over five layers—the first convolutional layer in each of the five blocks of the VGG19 model.\\n\\nHere we extract the style image features and combined image features from the input tensor that has been fed through the VGG19 network.\\n\\nThe style loss is scaled by a weighting parameter and the number of layers that it is calculated over.\\n\\n15 Source: GitHub, http://bit.ly/2FlVU0P.\\n\\nNeural Style Transfer\\n\\n|\\n\\n159\\n\\nTotal Variance Loss The total variance loss is simply a measure of noise in the combined image. To judge how noisy an image is, we can shift it one pixel to the right and calculate the sum of the squared difference between the translated and original images. For balance, we can also do the same procedure but shift the image one pixel down. The sum of these two terms is the total variance loss.\\n\\nExample 5-11 shows how we can code this in Keras.16\\n\\nExample 5-11. The variance loss function\\n\\ndef total_variation_loss(x): a = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25))\\n\\ntv_loss = total_variation_weight * total_variation_loss(combination_image)\\n\\nloss = content_loss + style_loss + tv_loss\\n\\nThe squared difference between the image and the same image shifted one pixel down.\\n\\nThe squared difference between the image and the same image shifted one pixel to the right.\\n\\nThe total variance loss is scaled by a weighting parameter.\\n\\nThe overall loss is the sum of the content, style, and total variance losses.\\n\\nRunning the Neural Style Transfer The learning process involves running gradient descent to minimize this loss func‐ tion, with respect to the pixels in the combined image. The full code for this is included in the, neural_style_transfer.py script included as part of the official Keras repository. Example 5-12 is a code snippet showing the training loop.\\n\\n16 Source: GitHub, http://bit.ly/2FlVU0P.\\n\\n160\\n\\n|\\n\\nChapter 5: Paint\\n\\nExample 5-12. The training loop for the neural style transfer model\\n\\nfrom scipy.optimize import fmin_l_bfgs_b\\n\\niterations = 1000 x = preprocess_image(base_image_path)\\n\\nfor i in range(iterations): x, min_val, info = fmin_l_bfgs_b( evaluator.loss , x.flatten() , fprime=evaluator.grads , maxfun=20 )\\n\\nThe process is initialized with the base image as the starting combined image.\\n\\nAt each iteration we pass the current combined image (flattened) into a optimiza‐ tion function, fmin_l_bfgs_b from the scipy.optimize package, that performs one gradient descent step according to the L-BFGS-B algorithm.\\n\\nHere, evaluator is an object that contains methods that calculate the overall loss, as described previously, and gradients of the loss with respect to the input image.\\n\\nAnalysis of the Neural Style Transfer Model Figure 5-18 shows the output of the neural style transfer process at three different stages in the learning process, with the following parameters:\\n\\ncontent_weight: 1 • style_weight: 100 • total_variation_weight: 20\\n\\nNeural Style Transfer\\n\\n|\\n\\n161\\n\\nFigure 5-18. Output from the neural style transfer process at 1, 200, and 400 iterations\\n\\nWe can see that with each training step, the algorithm becomes stylistically closer to the style image and loses the detail of the base image, while retaining the overall con‐ tent structure.\\n\\nThere are many ways to experiment with this architecture. You can try changing the weighting parameters in the loss function or the layer that is used to determine the content similarity, to see how this affects the combined output image and training speed. You can also try decaying the weight given to each layer in the style loss func‐ tion, to bias the model toward transferring finer or coarser style features.\\n\\nSummary In this chapter, we have explored two different ways to generate novel artwork: Cycle‐ GAN and neural style transfer.\\n\\nThe CycleGAN methodology allows us to train a model to learn the general style of an artist and transfer this over to a photograph, to generate output that looks as if the artist had painted the scene in the photo. The model also gives us the reverse process for free, converting paintings into realistic photographs. Crucially, paired images from each domain aren’t required for a CycleGAN to work, making it an extremely powerful and flexible technique.\\n\\n162\\n\\n|\\n\\nChapter 5: Paint\\n\\nThe neural style transfer technique allows us to transfer the style of a single image onto a base image, using a cleverly chosen loss function that penalizes the model for straying too far from the content of the base image and artistic style of the style image, while retaining a degree of smoothness to the output. This technique has been commercialized by many high-profile apps to blend a user’s photographs with a given set of stylistic paintings.\\n\\nIn the next chapter we shall move away from image-based generative modeling to a domain that presents new challenges: text-based generative modeling.\\n\\nSummary\\n\\n|\\n\\n163\\n\\nCHAPTER 6 Write\\n\\nIn this chapter we shall explore methods for building generative models on text data. There are several key differences between text and image data that mean that many of the methods that work well for image data are not so readily applicable to text data. In particular:\\n\\nText data is composed of discrete chunks (either characters or words), whereas pixels in an image are points in a continuous color spectrum. We can easily make a green pixel more blue, but it is not obvious how we should go about making the word cat more like the word dog, for example. This means we can easily apply backpropagation to image data, as we can calculate the gradient of our loss func‐ tion with respect to individual pixels to establish the direction in which pixel col‐ ors should be changed to minimize the loss. With discrete text data, we can’t apply backpropagation in the usual manner, so we need to find a way around this problem.\\n\\nText data has a time dimension but no spatial dimension, whereas image data has two spatial dimensions but no time dimension. The order of words is highly important in text data and words wouldn’t make sense in reverse, whereas images can usually be flipped without affecting the content. Furthermore, there are often long-term sequential dependencies between words that need to be captured by the model: for example, the answer to a question or carrying forward the context of a pronoun. With image data, all pixels can be processed simultaneously.\\n\\nText data is highly sensitive to small changes in the individual units (words or characters). Image data is generally less sensitive to changes in individual pixel units—a picture of a house would still be recognizable as a house even if some pixels were altered. However, with text data, changing even a few words can dras‐ tically alter the meaning of the passage, or make it nonsensical. This makes it\\n\\n165\\n\\nvery difficult to train a model to generate coherent text, as every word is vital to the overall meaning of the passage.\\n\\nText data has a rules-based grammatical structure, whereas image data doesn’t follow set rules about how the pixel values should be assigned. For example, it wouldn’t make grammatical sense in any content to write “The cat sat on the hav‐ ing.” There are also semantic rules that are extremely difficult to model; it wouldn’t make sense to say “I am in the beach,” even though grammatically, there is nothing wrong with this statement.\\n\\nGood progress has been made in text modeling, but solutions to the above problems are still ongoing areas of research. We’ll start by looking at one of the most utilized and established models for generating sequential data such as text, the recurrent neu‐ ral network (RNN), and in particular, the long short-term memory (LSTM) layer. In this chapter we will also explore some new techniques that have led to promising results in the field of question-answer pair generation.\\n\\nFirst, a trip to the local prison, where the inmates have formed a literary society…\\n\\nThe Literary Society for Troublesome Miscreants Edward Sopp hated his job as a prison warden. He spent his days watching over the prisoners and had no time to follow his true passion of writing short stories. He was running low on inspiration and needed to find a way to generate new content.\\n\\nOne day, he came up with a brilliant idea that would allow him to produce new works of fiction in his style, while also keeping the inmates occupied—he would get the inmates to collectively write the stories for him! He branded the new society the LSTM (Literary Society for Troublesome Miscreants).\\n\\nThe prison is particularly strange because it only consists of one large cell, containing 256 prisoners. Each prisoner has an opinion on how Edward’s current story should continue. Every day, Edward posts the latest word from his novel into the cell, and it is the job of the inmates to individually update their opinions on the current state of the story, based on the new word and the opinions of the inmates from the previous day.\\n\\nEach prisoner uses a specific thought process to update their own opinion, which involves balancing information from the new incoming word and other prisoners’ opinions with their own prior beliefs. First, they decide how much of yesterday’s opinion they wish to forget, using the information from the new word and the opin‐ ions of other prisoners in the cell. They also use this information to form new thoughts and decide how much of this they want to mix into the old beliefs that they have chosen to carry forward from the previous day. This then forms the prisoner’s new opinion for the day.\\n\\n166\\n\\n|\\n\\nChapter 6: Write\\n\\nHowever, the prisoners are secretive and don’t always tell their fellow inmates every‐ thing that they believe. They also use the latest chosen word and the opinions of the other inmates to decide how much of their opinion they wish to disclose.\\n\\nWhen Edward wants the cell to generate the next word in the sequence, the prisoners tell their disclosable opinions to the somewhat dense guard at the door, who com‐ bines this information to ultimately decide on the next word to be appended to the end of the novel. This new word is then fed back into the cell as usual, and the pro‐ cess continues until the full story is completed.\\n\\nTo train the inmates and the guard, Edward feeds short sequences of words that he has written previously into the cell and monitors if the inmates’ chosen next word is correct. He updates them on their accuracy, and gradually they begin to learn how to write stories in his own unique style.\\n\\nAfter many iterations of this process, Edward finds that the system has become quite accomplished at generating realistic-looking text. While it is somewhat lacking in semantic structure, it certainly displays similar characteristics to his previous stories.\\n\\nSatisfied with the results, he publishes a collection of the generated tales in his new book, entitled E. Sopp’s Fables.\\n\\nLong Short-Term Memory Networks The story of Mr. Sopp and his crowdsourced fables is an analogy for one of the most utilized and successful deep learning techniques for sequential data such as text: the long short-term memory (LSTM) network.\\n\\nAn LSTM network is a particular type of recurrent neural network (RNN). RNNs con‐ tain a recurrent layer (or cell) that is able to handle sequential data by making its own output at a particular timestep form part of the input to the next timestep, so that information from the past can affect the prediction at the current timestep. We say LSTM network to mean a neural network with an LSTM recurrent layer.\\n\\nWhen RNNs were first introduced, recurrent layers were very simple and consisted solely of a tanh operator that ensured that the information passed between timesteps was scaled between –1 and 1. However, this was shown to suffer from the vanishing gradient problem and didn’t scale well to long sequences of data.\\n\\nLSTM cells were first introduced in 1997 in a paper by Sepp Hochreiter and Jürgen Schmidhuber.1 In the paper, the authors describe how LSTMs do not suffer from the same vanishing gradient problem experienced by vanilla RNNs and can be trained on\\n\\n1 Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9 (1997): 1735–\\n\\n1780, http://bit.ly/2In7NnH.\\n\\nLong Short-Term Memory Networks\\n\\n|\\n\\n167\\n\\nsequences that are hundreds of timesteps long. Since then, the LSTM architecture has been adapted and improved, and variations such as gated recurrent units (GRUs) are now widely utilized and available as layers in Keras.\\n\\nLet’s begin by taking a look at how to build a very simple LSTM network in Keras that can generate text in the style of Aesop’s Fables.\\n\\nYour First LSTM Network As usual, you first need to get set up with the data.\\n\\nYou can download a collection of Aesop’s Fables from Project Gutenberg. This is a collection of free ebooks that can be downloaded as plain text files. This is a great resource for sourcing data that can be used to train text-based deep learning models.\\n\\nTo download the data, from inside the book repository run the following command:\\n\\nbash ./scripts/download_gutenburg_data.sh 11339 aesop\\n\\nLet’s now take a look at the steps we need to take in order to get the data in the right shape the is 06_01_lstm_text_train.ipynb notebook in the book repository.\\n\\nto\\n\\ntrain an LSTM network. The\\n\\ncode\\n\\ncontained\\n\\nTokenization The first step is to clean up and tokenize the text. Tokenization is the process of split‐ ting the text up into individual units, such as words or characters.\\n\\nHow you tokenize your text will depend on what you are trying to achieve with your text generation model. There are pros and cons to using both word and character tokens, and your choice will affect how you need to clean the text prior to modeling and the output from your model.\\n\\nIf you use word tokens:\\n\\nAll text can be converted to lowercase, to ensure capitalized words at the start of sentences are tokenized the same way as the same words appearing in the middle of a sentence. In some cases, however, this may not be desirable; for example, some proper nouns, such as names or places, may benefit from remaining capi‐ talized so that they are tokenized independently.\\n\\nThe text vocabulary (the set of distinct words in the training set) may be very large, with some words appearing very sparsely or perhaps only once. It may be wise to replace sparse words with a token for unknown word, rather than includ‐ ing them as separate tokens, to reduce the number of weights the neural network needs to learn.\\n\\n168\\n\\n|\\n\\nChapter 6: Write\\n\\nin\\n\\nWords can be stemmed, meaning that they are reduced to their simplest form, so that different tenses of a verb remained tokenized together. For example, browse, browsing, browses, and browsed would all be stemmed to brows.\\n\\nYou will need to either tokenize the punctuation, or remove it altogether.\\n\\nUsing word tokenization means that the model will never be able to predict words outside of the training vocabulary.\\n\\nIf you use character tokens:\\n\\nThe model may generate sequences of characters that form new words outside of the training vocabulary—this may be desirable in some contexts, but not in others.\\n\\nCapital letters can either be converted to their lowercase counterparts, or remain as separate tokens.\\n\\nThe vocabulary is usually much smaller when using character tokenization. This is beneficial for model training speed as there are fewer weights to learn in the final output layer.\\n\\nFor this example, we’ll use lowercase word tokenization, without word stemming. We’ll also tokenize punctuation marks, as we would like the model to predict when it should end sentences or open/close speech marks, for example. Finally, we’ll replace the multiple newlines between stories with a block of new story characters, ||||||||||||||||||||. This way, when we generate text using the model, we can seed the model with this block of characters, so that the model knows to start a new story from scratch.\\n\\nThe code in Example 6-1 cleans and tokenizes the text.\\n\\nExample 6-1. Tokenization\\n\\nimport re from keras.preprocessing.text import Tokenizer\\n\\nfilename = \"./data/aesop/data.txt\"\\n\\nwith open(filename, encoding=\\'utf-8-sig\\') as f: text = f.read()\\n\\nseq_length = 20 start_story = \\'| \\' * seq_length\\n\\n# CLEANUP text = text.lower() text = start_story + text text = text.replace(\\'\\\\n\\\\n\\\\n\\\\n\\\\n\\', start_story)\\n\\nYour First LSTM Network\\n\\n|\\n\\n169\\n\\ntext = text.replace(\\'\\\\n\\', \\' \\') text = re.sub(\\' +\\', \\'. \\', text).strip() text = text.replace(\\'..\\', \\'.\\')\\n\\ntext = re.sub(\\'([!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~])\\', r\\' \\\\1 \\', text) text = re.sub(\\'\\\\s{2,}\\', \\' \\', text)\\n\\n# TOKENIZATION tokenizer = Tokenizer(char_level = False, filters = \\'\\') tokenizer.fit_on_texts([text]) total_words = len(tokenizer.word_index) + 1 token_list = tokenizer.texts_to_sequences([text])[0]\\n\\nAn extract of the raw text after cleanup is shown in Figure 6-1.\\n\\nFigure 6-1. The text after cleanup\\n\\nIn Figure 6-2, we can see the dictionary of tokens mapped to their respective indices and also a snippet of tokenized text, with the corresponding words shown in green.\\n\\n170\\n\\n|\\n\\nChapter 6: Write\\n\\nFigure 6-2. The mapping dictionary between words and indices (left) and the text after tokenization (right)\\n\\nBuilding the Dataset Our LSTM network will be trained to predict the next word in a sequence, given a sequence of words preceding this point. For example, we could feed the model the tokens for the greedy cat and the and would expect the model to output a suitable next word (e.g., dog, rather than in).\\n\\nThe sequence length that we use to train the model is a parameter of the training pro‐ cess. In this example we choose to use a sequence length of 20, so we split the text into 20-word chunks. A total of 50,416 such sequences can be constructed, so our training dataset X is an array of shape [50416, 20].\\n\\nThe response variable for each sequence is the subsequent word, one-hot encoded into a vector of length 4,169 (the number of distinct words in the vocabulary). There‐ fore, our response y is a binary array of shape [50416, 4169].\\n\\nThe dataset generation step can be achieved with the code in Example 6-2.\\n\\nExample 6-2. Generating the dataset\\n\\nimport numpy as np from keras.utils import np_utils\\n\\ndef generate_sequences(token_list, step):\\n\\nX = [] y = []\\n\\nYour First LSTM Network\\n\\n|\\n\\n171\\n\\nfor i in range(0, len(token_list) - seq_length, step): X.append(token_list[i: i + seq_length]) y.append(token_list[i + seq_length])\\n\\ny = np_utils.to_categorical(y, num_classes = total_words)\\n\\nnum_seq = len(X) print(\\'Number of sequences:\\', num_seq, \"\\\\n\")\\n\\nreturn X, y, num_seq\\n\\nstep = 1 seq_length = 20 X, y, num_seq = generate_sequences(token_list, step)\\n\\nX = np.array(X) y = np.array(y)\\n\\nThe LSTM Architecture The architecture of the overall model is shown in Figure 6-3. The input to the model is a sequence of integer tokens and the output is the probability of each word in the vocabulary appearing next in the sequence. To understand how this works in detail, we need to introduce two new layer types, Embedding and LSTM.\\n\\nFigure 6-3. LSTM model architecture\\n\\nThe Embedding Layer An embedding layer is essentially a lookup table that converts each token into a vec‐ tor of length embedding_size (Figure 6-4). The number of weights learned by this layer is therefore equal to the size of the vocabulary, multiplied by embedding_size.\\n\\n172\\n\\n|\\n\\nChapter 6: Write\\n\\nFigure 6-4. An embedding layer is a lookup table for each integer token\\n\\nThe Input layer passes a tensor of integer sequences of shape [batch_size, seq_length] to the Embedding layer, which outputs a tensor of shape [batch_size, seq_length, embedding_size]. This is then passed on to the LSTM layer (Figure 6-5).\\n\\nFigure 6-5. A single sequence as it flows through an embedding layer\\n\\nWe embed each integer token into a continuous vector because it enables the model to learn a representation for each word that is able to be updated through backpropa‐ gation. We could also just one-hot encode each input token, but using an embedding layer is preferred because it makes the embedding itself trainable, thus giving the model more flexibility in deciding how to embed each token to improve model performance.\\n\\nYour First LSTM Network\\n\\n|\\n\\n173\\n\\nThe LSTM Layer To understand the LSTM layer, we must first look at how a general recurrent layer works.\\n\\nA recurrent layer has the special property of being able to process sequential input data [x1,…,xn]. It consists of a cell that updates its hidden state, ht, as each element of the sequence xt is passed through it, one timestep at a time. The hidden state is a vec‐ tor with length equal to the number of units in the cell—it can be thought of as the cell’s current understanding of the sequence. At timestep t, the cell uses the previous value of the hidden state ht–1 together with the data from the current timestep xt to produce an updated hidden state vector ht. This recurrent process continues until the end of the sequence. Once the sequence is finished, the layer outputs the final hidden state of the cell, hn, which is then passed on to the next layer of the network. This process is shown in Figure 6-6.\\n\\nFigure 6-6. A simple diagram of a recurrent layer\\n\\nTo explain this in more detail, let’s unroll the process so that we can see exactly how a single sequence is fed through the layer (Figure 6-7).\\n\\n174\\n\\n|\\n\\nChapter 6: Write\\n\\nFigure 6-7. How a single sequence flows through a recurrent layer\\n\\nHere, we represent the recurrent process by drawing a copy of the cell at each time‐ step and show how the hidden state is constantly being updated as it flows through the cells. We can clearly see how the previous hidden state is blended with the current sequential data point (i.e., the current embedded word vector) to produce the next hidden state. The output from the layer is the final hidden state of the cell, after each word in the input sequence has been processed. It’s important to remember that all of the cells in this diagram share the same weights (as they are really the same cell). There is no difference between this diagram and Figure 6-6; it’s just a different way of drawing the mechanics of a recurrent layer.\\n\\nYour First LSTM Network\\n\\n|\\n\\n175\\n\\nThe fact that the output from the cell is called a hidden state is an unfortunate naming convention—it’s not really hidden, and you shouldn’t think of it as such. Indeed, the last hidden state is the overall output from the layer, and we will be making use of the fact that we can access the hidden state at each individual timestep later in this chapter.\\n\\nThe LSTM Cell Now that we have seen how a generic recurrent layer works, let’s take a look inside an individual LSTM cell.\\n\\nThe job of the LSTM cell is to output a new hidden state, ht, given its previous hidden state, ht–1, and the current word embedding, xt. To recap, the length of ht is equal to the number of units in the LSTM. This is a parameter that is set when you define the layer and has nothing to do with the length of the sequence. Make sure you do not confuse the term cell with unit. There is one cell in an LSTM layer that is defined by the number of units it contains, in the same way that the prisoner cell from our ear‐ lier story contained many prisoners. We often draw a recurrent layer as a chain of cells unrolled, as it helps to visualize how the hidden state is updated at each timestep.\\n\\nAn LSTM cell maintains a cell state, Ct, which can be thought of as the cell’s internal beliefs about the current status of the sequence. This is distinct from the hidden state, ht, which is ultimately output by the cell after the final timestep. The cell state is the same length as the hidden state (the number of units in the cell).\\n\\nLet’s look more closely at a single cell and how the hidden state is updated (Figure 6-8).\\n\\n176\\n\\n|\\n\\nChapter 6: Write\\n\\nFigure 6-8. An LSTM cell\\n\\nThe hidden state is updated in six steps:\\n\\n1. The hidden state of the previous timestep, ht–1, and the current word embedding, xt, are concatenated and passed through the forget gate. This gate is simply a dense layer with weights matrix Wf, bias bf, and a sigmoid activation function. The resulting vector, ft, has a length equal to the number of units in the cell and contains values between 0 and 1 that determine how much of the previous cell state, Ct–1, should be retained.\\n\\n2. The concatenated vector is also passed through an input gate which, like the for‐ get gate, is a dense layer with weights matrix Wi, bias bi, and a sigmoid activation function. The output from this gate, it, has length equal to the number of units in\\n\\nYour First LSTM Network\\n\\n|\\n\\n177\\n\\nthe cell and contains values between 0 and 1 that determine how much new information will be added to the previous cell state, Ct–1.\\n\\n3. The concatenated vector is passed through a dense layer with weights matrix WC, bias bC, and a tanh activation function to generate a vector Ct that contains the new information that the cell wants to consider keeping. It also has length equal to the number of units in the cell and contains values between –1 and 1.\\n\\n4. ft and Ct–1 are multiplied element-wise and added to the element-wise multiplica‐ tion of it and Ct. This represents forgetting parts of the previous cell state and then adding new relevant information to produce the updated cell state, Ct.\\n\\n5. The original concatenated vector is also passed through an output gate: a dense layer with weights matrix Wo, bias bo, and a sigmoid activation. The resulting vec‐ tor, ot, has a length equal to the number of units in the cell and stores values between 0 and 1 that determine how much of the updated cell state, Ct, to output from the cell.\\n\\n6. ot is multiplied element-wise with the updated cell state Ct after a tanh activation has been applied to produce the new hidden state, ht.\\n\\nThe code to build the LSTM network is given in Example 6-3.\\n\\nExample 6-3. Building the LSTM network\\n\\nfrom keras.layers import Dense, LSTM, Input, Embedding, Dropout from keras.models import Model from keras.optimizers import RMSprop\\n\\nn_units = 256 embedding_size = 100\\n\\ntext_in = Input(shape = (None,)) x = Embedding(total_words, embedding_size)(text_in) x = LSTM(n_units)(x) x = Dropout(0.2)(x) text_out = Dense(total_words, activation = \\'softmax\\')(x)\\n\\nmodel = Model(text_in, text_out)\\n\\nopti = RMSprop(lr = 0.001) model.compile(loss=\\'categorical_crossentropy\\', optimizer=opti)\\n\\nepochs = 100 batch_size = 32 model.fit(X, y, epochs=epochs, batch_size=batch_size, shuffle = True)\\n\\n178\\n\\n|\\n\\nChapter 6: Write\\n\\nGenerating New Text Now that we have compiled and trained the LSTM network, we can start to use it to generate long strings of text by applying the following process:\\n\\n1. Feed the network with an existing sequence of words and ask it to predict the fol‐ lowing word.\\n\\n2. Append this word to the existing sequence and repeat.\\n\\nThe network will output a set of probabilities for each word that we can sample from. Therefore, we can make the text generation stochastic, rather than deterministic. Moreover, we can introduce a temperature parameter to the sampling process to indicate how deterministic we would like the process to be.\\n\\nThis is achieved with the code in Example 6-4.\\n\\nExample 6-4. Generating text with an LSTM network\\n\\ndef sample_with_temp(preds, temperature=1.0): # helper function to sample an index from a probability array preds = np.asarray(preds).astype(\\'float64\\') preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probs = np.random.multinomial(1, preds, 1) return np.argmax(probs)\\n\\ndef generate_text(seed_text, next_words, model, max_sequence_len, temp): output_text = seed_text seed_text = start_story + seed_text\\n\\nfor _ in range(next_words): token_list = tokenizer.texts_to_sequences([seed_text])[0] token_list = token_list[-max_sequence_len:] token_list = np.reshape(token_list, (1, max_sequence_len))\\n\\nprobs = model.predict(token_list, verbose=0)[0] y_class = sample_with_temp(probs, temperature = temp)\\n\\noutput_word = tokenizer.index_word[y_class] if y_class > 0 else \\'\\'\\n\\nif output_word == \"|\": break\\n\\nseed_text += output_word + \\' \\' output_text += output_word + \\' \\'\\n\\nreturn output_text\\n\\nGenerating New Text\\n\\n|\\n\\n179\\n\\nThis function weights the logits with a temperature scaling factor before reap‐ plying the softmax function. A temperature close to zero makes the sampling more deterministic (i.e., the word with highest probability is very likely to be chosen), whereas a temperature of 1 means each word is chosen with the proba‐ bility output by the model.\\n\\nThe seed text is a string of words that you would like to give the model to start the generation process (it can be blank). This is prepended with the block of characters we use to indicate the start of a story (||||||||||||||||||||).\\n\\nThe words are converted to a list of tokens.\\n\\nOnly the last max_sequence_len tokens are kept. The LSTM layer can accept any length of sequence as input, but the longer the sequence is the more time it will take to generate the next word, so the sequence length should be capped.\\n\\nThe model outputs the probabilities of each word being next in the sequence.\\n\\nThe probabilities are passed through the sampler to output the next word, para‐ meterized by temperature.\\n\\nIf the output word is the start story token, we stop generating any more words as this is the model telling us it wants to end this story and start the next one!\\n\\nOtherwise, we append the new word to the seed text, ready for the next iteration of the generative process.\\n\\nLet’s take a look at this in action, at two different temperature values (Figure 6-9).\\n\\n180\\n\\n|\\n\\nChapter 6: Write\\n\\nFigure 6-9. Example of LSTM-generated passages, at two different temperature values\\n\\nThere are a few things to note about these two passages. First, both are stylistically similar to a fable from the original training set. They both open with the familiar statement of the characters in the story, and generally the text within speech marks is more dialogue-like, using personal pronouns and prepared by the occurrence of the word said.\\n\\nSecond, the text generated at temperature = 0.2 is less adventurous but more coher‐ ent in its choice of words than the text generated at temperature = 1.0, as lower temperature values result in more deterministic sampling.\\n\\nLast, it is clear that neither flows particularly well as a story across multiple sentences, because the LSTM network cannot grasp the semantic meaning of the words that it is generating. In order to generate passages that have greater chance of being semanti‐ cally reasonable, we can build a human-assisted text generator, where the model out‐ puts the top 10 words with the highest probabilities and it is then ultimately up to a human to choose the next word from among this list. This is similar to predictive text on your mobile phone, where you are given the choice of a few words that might fol‐ low on from what you have already typed.\\n\\nTo demonstrate this, Figure 6-10 shows the top 10 words with the highest probabili‐ ties to follow various sequences (not from the training set).\\n\\nGenerating New Text\\n\\n|\\n\\n181\\n\\nFigure 6-10. Distribution of word probabilities following various sequences\\n\\nThe model is able to generate a suitable distribution for the next most likely word across a range of contexts. For example, even though the model was never told about parts of speech such as nouns, verbs, adjectives, and prepositions, it is generally able to separate words into these classes and use them in a way that is grammatically cor‐ rect. It can also guess that the article that begins a story about an eagle is more likely to be an, rather than a.\\n\\nThe punctuation example from Figure 6-10 shows how the model is also sensitive to subtle changes in the input sequence. In the first passage (the lion said ,), the model guesses that speech marks follow with 98% likelihood, so that the clause precedes the spoken dialogue. However, if we instead input the next word as and, it is able to understand that speech marks are now unlikely, as the clause is more likely to have superseded the dialogue and the sentence will more likely continue as descriptive prose.\\n\\n182\\n\\n|\\n\\nChapter 6: Write\\n\\nRNN Extensions The network in the preceding section is a simple example of how an LSTM network can be trained to learn how to generate text in a given style. In this section we will explore several extensions to this idea.\\n\\nStacked Recurrent Networks The network we just looked at contained a single LSTM layer, but we can also train networks with stacked LSTM layers, so that deeper features can be learned from the text.\\n\\nTo achieve this, we set the return_sequences parameter within the first LSTM layer to True. This makes the layer output the hidden state from every timestep, rather than just the final timestep. The second LSTM layer can then use the hidden states from the first layer as its input data. This is shown in Figure 6-11, and the overall model architecture is shown in Figure 6-12.\\n\\nRNN Extensions\\n\\n|\\n\\n183\\n\\n184\\n\\n|\\n\\nChapter 6: Write\\n\\nr e y a l\\n\\nd n o c e s\\n\\ne h t\\n\\nf o s e t a t s n e d d i h s e t o n e d\\n\\nt\\n\\nh d n a r e y a l\\n\\nt s r fi e h t\\n\\nf o s e t a t s n e d d i h s e t o n e d\\n\\nt\\n\\ng\\n\\n:\\n\\nN N R r e y a l i t l u m a f o m a r g a i D\\n\\n.\\n\\n1 1 - 6 e r u g i F\\n\\nFigure 6-12. A stacked LSTM network\\n\\nThe code to build the stacked LSTM network is given in Example 6-5.\\n\\nExample 6-5. Building a stacked LSTM network\\n\\ntext_in = Input(shape = (None,)) embedding = Embedding(total_words, embedding_size) x = embedding(text_in) x = LSTM(n_units, return_sequences = True)(x) x = LSTM(n_units)(x) x = Dropout(0.2)(x) text_out = Dense(total_words, activation = \\'softmax\\')(x)\\n\\nmodel = Model(text_in, text_out)\\n\\nGated Recurrent Units Another type of commonly used RNN layer is the gated recurrent unit (GRU).2 The key differences from the LSTM unit are as follows:\\n\\n1. The forget and input gates are replaced by reset and update gates.\\n\\n2 Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical\\n\\nMachine Translation,” 3 June 2014, https://arxiv.org/abs/1406.1078.\\n\\nRNN Extensions\\n\\n|\\n\\n185\\n\\n2. There is no cell state or output gate, only a hidden state that is output from the cell.\\n\\nThe hidden state is updated in four steps, as illustrated in Figure 6-13.\\n\\nFigure 6-13. A single GRU cell\\n\\nThe process is as follows:\\n\\n1. The hidden state of the previous timestep, ht–1, and the current word embedding, xt, are concatenated and used to create the reset gate. This gate is a dense layer, with weights matrix Wr and a sigmoid activation function. The resulting vector, rt, has a length equal to the number of units in the cell and stores values between 0 and 1 that determine how much of the previous hidden state, ht–1, should be carried forward into the calculation for the new beliefs of the cell.\\n\\n2. The reset gate is applied to the hidden state, ht–1, and concatenated with the cur‐ rent word embedding, xt. This vector is then fed to a dense layer with weights matrix W and a tanh activation function to generate a vector, ht, that stores the\\n\\n186\\n\\n|\\n\\nChapter 6: Write\\n\\nnew beliefs of the cell. It has length equal to the number of units in the cell and stores values between –1 and 1.\\n\\n3. The concatenation of the hidden state of the previous timestep, ht–1, and the cur‐ rent word embedding, xt, are also used to create the update gate. This gate is a dense layer with weights matrix Wz and a sigmoid activation. The resulting vec‐ tor, zt, has length equal to the number of units in the cell and stores values between 0 and 1, which are used to determine how much of the new beliefs, ht, to blend into the current hidden state, ht–1.\\n\\n4. The new beliefs of the cell ht and the current hidden state, ht–1, are blended in a proportion determined by the update gate, zt, to produce the updated hidden state, ht, that is output from the cell.\\n\\nBidirectional Cells For prediction problems where the entire text is available to the model at inference time, there is no reason to process the sequence only in the forward direction—it could just as well be processed backwards. A bidirectional layer takes advantage of this by storing two sets of hidden states: one that is produced as a result of the sequence being processed in the usual forward direction and another that is pro‐ duced when the sequence is processed backwards. This way, the layer can learn from information both preceding and succeeding the given timestep.\\n\\nIn Keras, this is implemented as a wrapper around a recurrent layer, as shown here:\\n\\nlayer = Bidirectional(GRU(100))\\n\\nThe hidden states in the resulting layer are vectors of length equal to double the num‐ ber of units in the wrapped cell (a concatenation of the forward and backward hidden states). Thus, in this example the hidden states of the layer are vectors of length 200.\\n\\nEncoder–Decoder Models So far, we have looked at using LSTM networks for generating the continuation of an existing text sequence. We have seen how a single LSTM layer can process the data sequentially to update a hidden state that represents the layer’s current understanding of the sequence. By connecting the final hidden state to a dense layer, the network can output a probability distribution for the next word.\\n\\nFor some tasks, the goal isn’t to predict the single next word in the existing sequence; instead we wish to predict a completely different sequence of words that is in some way related to the input sequence. Some examples of this style of task are:\\n\\nEncoder–Decoder Models\\n\\n|\\n\\n187\\n\\nLanguage translation\\n\\nThe network is fed a text string in the source language and the goal is to output the text translated into a target language.\\n\\nQuestion generation\\n\\nThe network is fed a passage of text and the goal is to generate a viable question that could be asked about the text.\\n\\nText summarization\\n\\nThe network is fed a long passage of text and the goal is to generate a short sum‐ mary of the passage.\\n\\nFor these kinds of problems, we can use a type of network known as an encoder– decoder. We have already seen one type of encoder–decoder network in the context of image generation: the variational autoencoder. For sequential data, the encoder– decoder process works as follows:\\n\\n1. The original input sequence is summarized into a single vector by the encoder RNN.\\n\\n2. This vector is used to initialize the decoder RNN.\\n\\n3. The hidden state of the decoder RNN at each timestep is connected to a dense layer that outputs a probability distribution over the vocabulary of words. This way, the decoder can generate a novel sequence of text, having been initialized with a representation of the input data produced by the encoder.\\n\\nThis process is shown in Figure 6-14, in the context of translation between English and German.\\n\\n188\\n\\n|\\n\\nChapter 6: Write\\n\\nEncoder–Decoder Models\\n\\n|\\n\\nk r o w t e n r e d o c e d – r e d o c n e n A\\n\\n.\\n\\n4 1 - 6 e r u g i F\\n\\n189\\n\\nThe final hidden state of the encoder can be thought of as a representation of the entire input document. The decoder then transforms this representation into sequen‐ tial output, such as the translation of the text into another language, or a question relating to the document.\\n\\nDuring training, the output distribution produced by the decoder at each timestep is compared against the true next word, to calculate the loss. The decoder doesn’t need to sample from these distributions to generate words during the training process, as the subsequent cell is fed with the ground-truth next word, rather than a word sam‐ pled from the previous output distribution. This way of training encoder–decoder networks is known as teacher forcing. We can imagine that the network is a student sometimes making erroneous distribution predictions, but no matter what the net‐ work outputs at each timestep, the teacher provides the correct response as input to the network for the attempt at the next word.\\n\\nA Question and Answer Generator We’re now going to put everything together and build a model that can generate question and answer pairs from a block of text. This project is inspired by the qgen- workshop TensorFlow codebase and the model proposed by Tong Wang, Xingdi Yuan, and Adam Trischler.3\\n\\nThe model consists of two parts:\\n\\nAn RNN that identifies candidate answers from the block of text\\n\\nAn encoder–decoder network that generates a suitable question, given one of the candidate answers highlighted by the RNN\\n\\nFor example, consider the following opening to a passage of text about a football match:\\n\\nThe winning goal was scored by 23-year-old striker Joe Bloggs during the match between Arsenal and Barcelona . Arsenal recently signed the striker for 50 million pounds . The next match is in two weeks time, on July 31st 2005 . \"\\n\\nWe would like our first network to be able to identify potential answers such as:\\n\\n\"Joe Bloggs\" \"Arsenal\" \"Barcelona\" \"50 million pounds\" \"July 31st 2005\"\\n\\n3 Tong Wang, Xingdi Yuan, and Adam Trischler, “A Joint Model for Question Answering and Question Genera‐\\n\\ntion,” 5 July 2017, https://arxiv.org/abs/1706.01450.\\n\\n190\\n\\n|\\n\\nChapter 6: Write\\n\\nAnd our second network should be able to generate a question, given each of the answers, such as:\\n\\n\"Who scored the winning goal?\" \"Who won the match?\" \"Who were Arsenal playing?\" \"How much did the striker cost?\" \"When is the next match?\"\\n\\nLet’s first take a look at the dataset we shall be using in more detail.\\n\\nA Question-Answer Dataset We’ll be using the Maluuba NewsQA dataset, which you can download by following the set of instructions on GitHub.\\n\\nThe resulting train.csv, test.csv, and dev.csv files should be placed in the ./data/qa/ folder inside the book repository. These files all have the same column structure, as follows:\\n\\nstory_id\\n\\nA unique identifier for the story.\\n\\nstory_text\\n\\nThe text of the story (e.g., “The winning goal was scored by 23-year-old striker Joe Bloggs during the match…”).\\n\\nquestion\\n\\nA question that could be asked about the story text (e.g., “How much did the striker cost?”).\\n\\nanswer_token_ranges\\n\\nThe token positions of the answer in the story text (e.g., 24:27). There might be multiple ranges (comma separated) if the answer appears multiple times in the story.\\n\\nThis raw data is processed and tokenized so that is it able to be used as input to our model. After this transformation, each observation in the training set consists of the following five features:\\n\\ndocument_tokens\\n\\nThe tokenized story text (e.g., [1, 4633, 7, 66, 11, ...]), clipped/padded with zeros to be of length max_document_length (a parameter).\\n\\nquestion_input_tokens\\n\\nThe tokenized question (e.g., [2, 39, 1, 52, ...]), padded with zeros to be of length max_question_length (another parameter).\\n\\nA Question and Answer Generator\\n\\n|\\n\\n191\\n\\nquestion_output_tokens\\n\\nThe tokenized question, offset by one timestep (e.g., [39, 1, 52, 1866, ...], padded with zeros to be of length max_question_length.\\n\\nanswer_masks\\n\\nA binary mask matrix having shape [max_answer_length, max_docu ment_length]. The [i, j] value of the matrix is 1 if the ith word of the answer to the question is located at the jth word of the document and 0 otherwise.\\n\\nanswer_labels\\n\\nA binary vector of length max_document_length (e.g., [0, 1, 1, 0, ...]). The ith element of the vector is 1 if the ith word of the document could be consid‐ ered part of an answer and 0 otherwise.\\n\\nLet’s now take a look at the model architecture that is able to generate question- answer pairs from a given block of text.\\n\\nModel Architecture Figure 6-15 shows the overall model architecture that we will be building. Don’t worry if this looks intimidating! It’s only built from elements that we have seen already and we will be walking through the architecture step by step in this section.\\n\\n192\\n\\n|\\n\\nChapter 6: Write\\n\\nA Question and Answer Generator\\n\\n|\\n\\ns e x o b d e r e d r o b n e e r g n i n w o h s\\n\\ns i a t a d t u p n i\\n\\n; s r i a p r e w s n a – n o i t s e u q g n i t a r e n e g\\n\\nr o f\\n\\ne r u t c e t i h c r a e\\n\\nTh\\n\\n.\\n\\n5 1 - 6 e r u g i F\\n\\n193\\n\\nLet’s start by taking a look at the Keras code that builds the part of the model at the top of the diagram, which predicts if each word in the document is part of an answer or not. This code is shown in Example 6-6. You can also follow along with the accom‐ panying notebook in the book repository, 06_02_qa_train.ipynb.\\n\\nExample 6-6. Model architecture for generating question–answer pairs\\n\\nfrom keras.layers import Input, Embedding, GRU, Bidirectional, Dense, Lambda from keras.models import Model, load_model import keras.backend as K from qgen.embedding import glove\\n\\n#### PARAMETERS ####\\n\\nVOCAB_SIZE = glove.shape[0] # 9984 EMBEDDING_DIMENS = glove.shape[1] # 100\\n\\nGRU_UNITS = 100 DOC_SIZE = None ANSWER_SIZE = None Q_SIZE = None\\n\\ndocument_tokens = Input(shape=(DOC_SIZE,), name=\"document_tokens\")\\n\\nembedding = Embedding(input_dim = VOCAB_SIZE, output_dim = EMBEDDING_DIMENS\\n\\n, weights=[glove], mask_zero = True, name = \\'embedding\\')\\n\\ndocument_emb = embedding(document_tokens)\\n\\nanswer_outputs = Bidirectional(GRU(GRU_UNITS, return_sequences=True)\\n\\n, name = \\'answer_outputs\\')(document__emb)\\n\\nanswer_tags = Dense(2, activation = \\'softmax\\' , name = \\'answer_tags\\')(answer_outputs)\\n\\nThe document tokens are provided as input to the model. Here, we use the vari‐ able DOC_SIZE to describe the size of this input, but the variable is actually set to None. This is because the architecture of the model isn’t dependent on the length of the input sequence—the number of cells in the layer will adapt to equal the length of the input sequence, so we don’t need to specify it explicitly.\\n\\nThe embedding layer is initialized with GloVe word vectors (explained in the fol‐ lowing sidebar).\\n\\nThe recurrent layer is a bidirectional GRU that returns the hidden state at each timestep.\\n\\n194\\n\\n|\\n\\nChapter 6: Write\\n\\nThe output Dense layer is connected to the hidden state at each timestep and has only two units, with a softmax activation, representing the probability that each word is part of an answer (1) or is not part of an answer (0).\\n\\nGloVe Word Vectors The embedding layer is initialized with a set of pretrained word embeddings, rather than random vectors as we have seen previously. These word vectors have been cre‐ ated as part of the Stanford GloVe (“Global Vectors”) project, which uses unsuper‐ vised learning to obtain representative vectors for a large set of words.\\n\\nThese vectors have many beneficial properties, such as the similarity of vectors between connected words. For example, the vector between embeddings for the words man and woman is approximately the same as the vector between the words king and queen. It is as if the gender of the word is encoded into the latent space in which the word vectors exist. Initializing an embedding layer with GloVe is often bet‐ ter than training from scratch because a lot of the hard work of capturing the repre‐ sentation of a word has already been achieved by the GloVe training process. Your algorithm can then tweak the word embeddings to suit the particular context of your machine learning problem.\\n\\nTo work with the GloVe word vectors within this project, download the file glove.6B. 100d.txt (6 billion words each with an embedding of length 100) from the GloVe project website and then run the following Python script from the book repository to trim this file to only include words that are present in the training corpus:\\n\\npython ./utils/write.py\\n\\nThe second part of the model is the encoder–decoder network that takes a given answer and tries to formulate a matching question (the bottom part of Figure 6-15).\\n\\nThe Keras code for this part of the network is given in Example 6-7.\\n\\nExample 6-7. Model architecture for the encoder–decoder network that formulates a question given an answer\\n\\nencoder_input_mask = Input(shape=(ANSWER_SIZE, DOC_SIZE)\\n\\n, name=\"encoder_input_mask\")\\n\\nencoder_inputs = Lambda(lambda x: K.batch_dot(x[0], x[1])\\n\\n, name=\"encoder_inputs\")([encoder_input_mask, answer_outputs])\\n\\nencoder_cell = GRU(2 * GRU_UNITS, name = \\'encoder_cell\\')(encoder_inputs)\\n\\ndecoder_inputs = Input(shape=(Q_SIZE,), name=\"decoder_inputs\") decoder_emb = embedding(decoder_inputs) decoder_emb.trainable = False decoder_cell = GRU(2 * GRU_UNITS, return_sequences = True, name = \\'decoder_cell\\')\\n\\nA Question and Answer Generator\\n\\n|\\n\\n195\\n\\ndecoder_states = decoder_cell(decoder_emb, initial_state = [encoder_cell])\\n\\ndecoder_projection = Dense(VOCAB_SIZE, name = \\'decoder_projection\\'\\n\\n, activation = \\'softmax\\', use_bias = False)\\n\\ndecoder_outputs = decoder_projection(decoder_states)\\n\\ntotal_model = Model([document_tokens, decoder_inputs, encoder_input_mask]\\n\\n, [answer_tags, decoder_outputs])\\n\\nanswer_model = Model(document_tokens, [answer_tags]) decoder_initial_state_model = Model([document_tokens, encoder_input_mask]\\n\\n, [encoder_cell])\\n\\nThe answer mask is passed as an input to the model—this allows us to pass the hidden states from a single answer range through to the encoder–decoder. This is achieved using a Lambda layer.\\n\\nThe encoder is a GRU layer that is fed the hidden states for the given answer range as input data.\\n\\nThe input data to the decoder is the question matching the given answer range.\\n\\nThe question word tokens are passed through the same embedding layer used in the answer identification model.\\n\\nThe decoder is a GRU layer and is initialized with the final hidden state from the encoder.\\n\\nThe hidden states of the decoder are passed through a Dense layer to generate a distribution over the entire vocabulary for the next word in the sequence.\\n\\nThis completes our network for question–answer pair generation. To train the net‐ work, we pass the document text, question text, and answer masks as input data in batches and minimize the cross-entropy loss on both the answer position prediction and question word generation, weighted equally.\\n\\nInference To test the model on an input document sequence that it has never seen before, we need to run the following process:\\n\\n1. Feed the document string to the answer generator to produce sample positions for answers in the document.\\n\\n2. Choose one of these answer blocks to carry forward to the encoder–decoder question generator (i.e., create the appropriate answer mask).\\n\\n196\\n\\n|\\n\\nChapter 6: Write\\n\\n3. Feed the document and answer mask to the encoder to generate the initial state for the decoder.\\n\\n4. Initialize the decoder with this initial state and feed in the <START> token to gen‐ erate the first word of the question. Continue this process, feeding in generated words one by one until the <END> token is predicted by the model.\\n\\nAs discussed previously, during training the model uses teacher forcing to feed the ground-truth words (rather than the predicted next words) back into the decoder cell. However during inference the model must generate a question by itself, so we want to be able to feed the predicted words back into the decoder cell while retaining its hidden state.\\n\\nOne way we can achieve this is by defining an additional Keras model (question_model) that accepts the current word token and current decoder hidden state as input, and outputs the predicted next word distribution and updated decoder hidden state. This is shown in Example 6-8.\\n\\nExample 6-8. Inference models\\n\\ndecoder_inputs_dynamic = Input(shape=(1,), name=\"decoder_inputs_dynamic\") decoder_emb_dynamic = embedding(decoder_inputs_dynamic) decoder_init_state_dynamic = Input(shape=(2 * GRU_UNITS,)\\n\\n, name = \\'decoder_init_state_dynamic\\')\\n\\ndecoder_states_dynamic = decoder_cell(decoder_emb_dynamic\\n\\n, initial_state = [decoder_init_state_dynamic])\\n\\ndecoder_outputs_dynamic = decoder_projection(decoder_states_dynamic)\\n\\nquestion_model = Model([decoder_inputs_dynamic, decoder_init_state_dynamic] , [decoder_outputs_dynamic, decoder_states_dynamic])\\n\\nWe can then use this model in a loop to generate the output question word by word, as shown in Example 6-9.\\n\\nExample 6-9. Generating question–answer pairs from a given document\\n\\ntest_data_gen = test_data() batch = next(test_data_gen) answer_preds = answer_model.predict(batch[\"document_tokens\"])\\n\\nidx = 0 start_answer = 37 end_answer = 39\\n\\nanswers = [[0] * len(answer_preds[idx])] for i in range(start_answer, end_answer + 1): answers[idx][i] = 1\\n\\nA Question and Answer Generator\\n\\n|\\n\\n197\\n\\nanswer_batch = expand_answers(batch, answers)\\n\\nnext_decoder_init_state = decoder_initial_state_model.predict(\\n\\n[answer_batch[\\'document_tokens\\'][[idx]], answer_batch[\\'answer_masks\\'][[idx]]])\\n\\nword_tokens = [START_TOKEN] questions = [look_up_token(START_TOKEN)]\\n\\nended = False\\n\\nwhile not ended:\\n\\nword_preds, next_decoder_init_state = question_model.predict(\\n\\n[word_tokens, next_decoder_init_state])\\n\\nnext_decoder_init_state = np.squeeze(next_decoder_init_state, axis = 1) word_tokens = np.argmax(word_preds, 2)[0]\\n\\nquestions.append(look_up_token(word_tokens[0]))\\n\\nif word_tokens[0] == END_TOKEN: ended = True\\n\\nquestions = \\' \\'.join(questions)\\n\\nModel Results Sample results from the model are shown in Figure 6-16 (see also the accompanying notebook in the book repository, 06_03_qa_analysis.ipynb). The chart on the right shows the probability of each word in the document forming part of an answer, according to the model. These answer phrases are then fed to the question generator and the output of this model is shown on the lefthand side of the diagram (“Predicted Question”).\\n\\nFirst, notice how the answer generator is able to accurately identify which words in the document are most likely to be contained in an answer. This is already quite impressive given that it has never seen this text before and also may not have seen certain words from the document that are included in the answer, such as Bloggs. It is able to understand from the context that this is likely to be the surname of a person and therefore likely to form part of an answer.\\n\\n198\\n\\n|\\n\\nChapter 6: Write\\n\\nFigure 6-16. Sample results from the model\\n\\nThe encoder extracts the context from each of these possible answers, so that the decoder is able to generate suitable questions. It is remarkable that the encoder is able to capture that the person mentioned in the first answer, 23-year-old striker Joe Bloggs, would probably have a matching question relating to his goal-scoring abilities, and is able to pass this context on to the decoder so that it can generate the question “who scored the <UNK> ?” rather than, for example, “who is the president ?”\\n\\nThe decoder has finished this question with the tag <UNK>, but not because it doesn’t know what to do next—it is predicting that the word that follows is likely to be from outside the core vocabulary. We shouldn’t be surprised that the model resorts to using the tag <UNK> in this context, as many of the niche words in the original corpus would be tokenized this way.\\n\\nWe can see that in each case, the decoder chooses the correct “type” of question— who, how much, or when—depending on the type of answer. There are still some problems though, such as asking how much money did he lose ? rather than how much\\n\\nA Question and Answer Generator\\n\\n|\\n\\n199\\n\\nmoney was paid for the striker ?. This is understandable, as the decoder only has the final encoder state to work with and cannot reference the original document for extra information.\\n\\nThere are several extensions to encoder–decoder networks that improve the accuracy and generative power of the model. Two of the most widely used are pointer networks4 and attention mechanisms.5 Pointer networks give the model the ability to “point” at specific words in the input text to include in the generated question, rather than only relying on the words in the known vocabulary. This helps to solve the <UNK> prob‐ lem mentioned earlier. We shall explore attention mechanisms in detail in the next chapter.\\n\\nSummary In this chapter we have seen how recurrent neural networks can be applied to gener‐ ate text sequences that mimic a particular style of writing, and also generate plausible question–answer pairs from a given document.\\n\\nWe explored two different types of recurrent layer, long short-term memory and GRU, and saw how these cells can be stacked or made bidirectional to form more complex network architectures. The encoder–decoder architecture introduced in this chapter is an important generative tool as it allows sequence data to be condensed into a single vector that can then be decoded into another sequence. This is applicable to a range of problems aside from question–answer pair generation, such as transla‐ tion and text summarization.\\n\\nIn both cases we have seen how it is important to understand how to transform unstructured text data to a structured format that can be used with recurrent neural network layers. A good understanding of how the shape of the tensor changes as data flows through the network is also pivotal to building successful networks, and recur‐ rent layers require particular care in this regard as the time dimension of sequential data adds additional complexity to the transformation process.\\n\\nIn the next chapter we will see how many of the same ideas around RNNs can be applied to another type of sequential data: music.\\n\\n4 Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly, “Pointer Networks,” 9 July 2015, https://arxiv.org/abs/\\n\\n1506.03134.\\n\\n5 Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural Machine Translation by Jointly Learning to\\n\\nAlign and Translate,” 1 September 2014, https://arxiv.org/abs/1409.0473.\\n\\n200\\n\\n|\\n\\nChapter 6: Write\\n\\nCHAPTER 7 Compose\\n\\nAlongside visual art and creative writing, musical composition is another core act of creativity that we consider to be uniquely human.\\n\\nFor a machine to compose music that is pleasing to our ear, it must master many of the same technical challenges that we saw in the previous chapter in relation to text. In particular, our model must be able to learn from and re-create the sequential struc‐ ture of music and must also be able to choose from a discrete set of possibilities for subsequent notes.\\n\\nHowever, musical generation presents additional challenges that are not required for text generation, namely pitch and rhythm. Music is often polyphonic—that is, there are several streams of notes played simultaneously on different instruments, which combine to create harmonies that are either dissonant (clashing) or consonant (har‐ monious). Text generation only requires us to handle a single stream of text, rather than the parallel streams of chords that are present in music.\\n\\nAlso, text generation can be handled one word at a time. We must consider carefully if this is an appropriate way to process musical data, as much of the interest that stems from listening to music is in the interplay between different rhythms across the ensemble. A guitarist might play a flurry of quicker notes while the pianist holds a longer sustained chord, for example. Therefore, generating music note by note is complex, because we often do not want all the instruments to change note simultaneously.\\n\\nWe will start this chapter by simplifying the problem to focus on music generation for a single (monophonic) line of music. We will see that many of the RNN techniques from the previous chapter on text generation can also be used for music generation as the two tasks share many common themes. This chapter will also introduce the atten‐ tion mechanism that will allow us to build RNNs that are able to choose which\\n\\n201\\n\\nprevious notes to focus on in order to predict which notes will appear next. Lastly, we’ll tackle the task of polyphonic music generation and explore how we can deploy an architecture based around GANs to create music for multiple voices.\\n\\nPreliminaries Anyone tackling the task of music generation must first have a basic understanding of musical theory. In this section we’ll go through the essential notation required to read music and how we can represent this numerically, in order to transform music into the input data required to train a generative model.\\n\\nWe’ll work through the notebook 07_01_notation_compose.ipynb in the book reposi‐ tory. Another excellent resource for getting started with music generation using Python is Sigurður Skúli’s blog post and accompanying GitHub repository.\\n\\nThe raw dataset that we shall be using is a set of MIDI files for the Cello Suites by J.S. Bach. You can use any dataset you wish, but if you want to work with this dataset, you can find instructions for downloading the MIDI files in the notebook.\\n\\nTo view and listen to the music generated by the model, you’ll need some software that can produce musical notation. MuseScore is a great tool for this purpose and can be downloaded for free.\\n\\nMusical Notation We’ll be using the Python library music21 to load the MIDI files into Python for pro‐ cessing. Example 7-1 shows how to load a MIDI file and visualize it (Figure 7-1), both as a score and as structured data.\\n\\nExample 7-1. Importing a MIDI file\\n\\nfrom music21 import converter\\n\\ndataset_name = \\'cello\\' filename = \\'cs1-2all\\' file = \"./data/{}/{}.mid\".format(dataset_name, filename)\\n\\noriginal_score = converter.parse(file).chordify()\\n\\n202\\n\\n|\\n\\nChapter 7: Compose\\n\\nPreliminaries\\n\\n|\\n\\nn o i t a t o n\\n\\nl a c i s u M\\n\\n.\\n\\n1 - 7 e r u g i F\\n\\n203\\n\\nWe use the chordify method to squash all simultaneously played notes into chords within a single part, rather than them being split between many parts. Since this piece is performed by one instrument (a cello), we are justified in doing this, though some‐ times we may wish to keep the parts separate to generate music that is polyphonic in nature. This presents additional challenges that we shall tackle later on in this chapter.\\n\\nThe code in Example 7-2 loops through the score and extracts the pitch and duration for each note (and rest) in the piece into two lists. Individual notes in chords are sep‐ arated by a dot, so that the whole chord can be stored as a single string. The number after each note name indicates the octave that the note is in—since the note names (A to G) repeat, this is needed to uniquely identify the pitch of the note. For example, G2 is an octave below G3.\\n\\nExample 7-2. Extracting the data\\n\\nnotes = [] durations = []\\n\\nfor element in original_score.flat:\\n\\nif isinstance(element, chord.Chord): notes.append(\\'.\\'.join(n.nameWithOctave for n in element.pitches)) durations.append(element.duration.quarterLength)\\n\\nif isinstance(element, note.Note): if element.isRest: notes.append(str(element.name)) durations.append(element.duration.quarterLength) else: notes.append(str(element.nameWithOctave)) durations.append(element.duration.quarterLength)\\n\\nThe output from this process is shown in Table 7-1.\\n\\nThe resulting dataset now looks a lot more like the text data that we have dealt with previously. The words are the pitches, and we should try to build a model that pre‐ dicts the next pitch, given a sequence of previous pitches. The same idea can also be applied to the list of durations. Keras gives us the flexibility to be able to build a model that can handle the pitch and duration prediction simultaneously.\\n\\n204\\n\\n|\\n\\nChapter 7: Compose\\n\\nTable 7-1. The pitch and duration of each note, stored as lists\\n\\nDuration Pitch 0.25 1.0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\\n\\nB3 G2.D3.B3 B3 A3 G3 F#3 G3 D3 E3 F#3 G3 A3\\n\\nYour First Music-Generating RNN To create the dataset that will train the model, we first need to give each pitch and duration an integer value (Figure 7-2), exactly as we have done previously for each word in a text corpus. It doesn’t matter what these values are as we shall be using an embedding layer to transform the integer lookup values into vectors.\\n\\nFigure 7-2. The integer lookup dictionaries for pitch and duration\\n\\nWe then create the training set by splitting the data into small chunks of 32 notes, with a response variable of the next note in the sequence (one-hot encoded), for both pitch and duration. One example of this is shown in Figure 7-3.\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\n205\\n\\nFigure 7-3. The inputs and outputs for the musical generative model\\n\\nThe model we will be building is a stacked LSTM network with an attention mecha‐ nism. In the previous chapter, we saw how we are able to stack LSTM layers by pass‐ ing the hidden states of the previous layer as input to the next LSTM layer. Stacking layers in this way gives the model freedom to learn more sophisticated features from the data. In this section we will introduce the attention mechanism1 that now forms an integral part of most sophisticated sequential generative models. It has ultimately given rise to the transformer, a type of model based entirely on attention that doesn’t even require recurrent or convolutional layers. We will introduce the transformer architecture in more detail in Chapter 9.\\n\\nFor now, let’s focus on incorporating attention into a stacked LSTM network to try to predict the next note, given a sequence of previous notes.\\n\\nAttention The attention mechanism was originally applied to text translation problems—in par‐ ticular, translating English sentences into French.\\n\\nIn the previous chapter, we saw how encoder–decoder networks can solve this kind of problem, by first passing the input sequence through an encoder to generate a\\n\\n1 Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural Machine Translation by Jointly Learning to\\n\\nAlign and Translate,” 1 September 2014, https://arxiv.org/abs/1409.0473.\\n\\n206\\n\\n|\\n\\nChapter 7: Compose\\n\\ncontext vector, then passing this vector through the decoder network to output the translated text. One observed problem with this approach is that the context vector can become a bottleneck. Information from the start of the source sentence can become diluted by the time it reaches the context vector, especially for long sentences. Therefore these kinds of encoder–decoder networks sometimes struggle to retain all the required information for the decoder to accurately translate the source.\\n\\nAs an example, suppose we want the model to translate the following sentence into German: I scored a penalty in the football match against England.\\n\\nClearly, the meaning of the entire sentence would be changed by replacing the word scored with missed. However, the final hidden state of the encoder may not be able to sufficiently retain this information, as the word scored appears early in the sentence.\\n\\nThe correct translation of the sentence is: Ich habe im Fußballspiel gegen England einen Elfmeter erzielt.\\n\\nIf we look at the correct German translation, we can see that the word for scored (erzielt) actually appears right at the end of the sentence! So not only would the model have to retain the fact that the penalty was scored rather than missed through the encoder, but also all the way through the decoder as well.\\n\\nExactly the same principle is true in music. To understand what note or sequence of notes is likely to follow from a particular given passage of music, it may be crucial to use information from far back in the sequence, not just the most recent information. For example, take the opening passage of the Prelude to Bach’s Cello Suite No. 1 (Figure 7-4).\\n\\nFigure 7-4. The opening of Bach’s Cello Suite No. 1 (Prelude)\\n\\nWhat note do you think comes next? Even if you have no musical training you may still be able to guess. If you said G (the same as the very first note of the piece), then you’d be correct. How did you know this? You may have been able to see that every bar and half bar starts with the same note and used this information to inform your decision. We want our model to be able to perform the same trick—in particular, we want it to not only care about the hidden state of the network now, but also to pay\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\n207\\n\\nparticular attention to the hidden state of the network eight notes ago, when the pre‐ vious low G was registered.\\n\\nThe attention mechanism was proposed to solve this problem. Rather than only using the final hidden state of the encoder RNN as the context vector, the attention mecha‐ nism allows the model to create the context vector as a weighted sum of the hidden states of the encoder RNN at each previous timestep. The attention mechanism is just a set of layers that converts the previous encoder hidden states and current decoder hidden state into the summation weights that generate the context vector.\\n\\nIf this sounds confusing, don’t worry! We’ll start by seeing how to apply an attention mechanism after a simple recurrent layer (i.e., to solve the problem of predicting the next note of Bach’s Cello Suite No. 1), before we see how this extends to encoder– decoder networks, where we want to predict a whole sequence of subsequent notes, rather than just one.\\n\\nBuilding an Attention Mechanism in Keras First, let’s remind ourselves of how a standard recurrent layer can be used to predict the next note given a sequence of previous notes. Figure 7-5 shows how the input sequence (x1,…,xn) is fed to the layer one step at a time, continually updating the hid‐ den state of the layer. The input sequence could be the note embeddings, or the hid‐ den state sequence from a previous recurrent layer. The output from the recurrent layer is the final hidden state, a vector with the same length as the number of units. This can then be fed to a Dense layer with softmax output to predict a distribution for the next note in the sequence.\\n\\nFigure 7-5. A recurrent layer for predicting the next note in a sequence, without attention\\n\\nFigure 7-6 shows the same network, but this time with an attention mechanism applied to the hidden states of the recurrent layer.\\n\\n208\\n\\n|\\n\\nChapter 7: Compose\\n\\nFigure 7-6. A recurrent layer for predicting the next note in a sequence, with attention\\n\\nLet’s walk through this process step by step:\\n\\n1. First, each hidden state hj (a vector of length equal to the number of units in the recurrent layer) is passed through an alignment function a to generate a scalar, ej. In this example, this function is simply a densely connected layer with one output unit and a tanh activation function.\\n\\n2. Next, the softmax function is applied to the vector e1,…,en to produce the vector of weights α1,…,αn.\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\n209\\n\\n3. Lastly, each hidden state vector hj is multiplied by its respective weight αj, and the results are then summed to give the context vector c (thus c has the same length as a hidden state vector).\\n\\nThe context vector can then be passed to a Dense layer with softmax output as usual, to output a distribution for the potential next note.\\n\\nThis network can be built in Keras as shown in Example 7-3.\\n\\nExample 7-3. Building the RNN with attention\\n\\nnotes_in = Input(shape = (None,)) durations_in = Input(shape = (None,))\\n\\nx1 = Embedding(n_notes, embed_size)(notes_in) x2 = Embedding(n_durations, embed_size)(durations_in)\\n\\nx = Concatenate()([x1,x2])\\n\\nx = LSTM(rnn_units, return_sequences=True)(x) x = LSTM(rnn_units, return_sequences=True)(x)\\n\\ne = Dense(1, activation=\\'tanh\\')(x) e = Reshape([-1])(e)\\n\\nalpha = Activation(\\'softmax\\')(e)\\n\\nc = Permute([2, 1])(RepeatVector(rnn_units)(alpha)) c = Multiply()([x, c]) c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c)\\n\\nnotes_out = Dense(n_notes, activation = \\'softmax\\', name = \\'pitch\\')(c) durations_out = Dense(n_durations, activation = \\'softmax\\', name = \\'duration\\')(c)\\n\\nmodel = Model([notes_in, durations_in], [notes_out, durations_out])\\n\\natt_model = Model([notes_in, durations_in], alpha)\\n\\nopti = RMSprop(lr = 0.001) model.compile(loss=[\\'categorical_crossentropy\\', \\'categorical_crossentropy\\'], optimizer=opti)\\n\\nThere are two inputs to the network: the sequence of previous note names and duration values. Notice how the sequence length isn’t specified—the attention mechanism does not require a fixed-length input, so we can leave this as variable.\\n\\nThe Embedding layers convert the integer values of the note names and durations into vectors.\\n\\n210\\n\\n|\\n\\nChapter 7: Compose\\n\\nThe vectors are concatenated to form one long vector that will be used as input into the recurrent layers.\\n\\nTwo stacked LSTM layers are used as the recurrent part of the network. Notice how we set return_sequences to True to make each layer pass the full sequence of hidden states to the next layer, rather than just the final hidden state.\\n\\nThe alignment function is just a Dense layer with one output unit and tanh acti‐ vation. We can use a Reshape layer to squash the output to a single vector, of length equal to the length of the input sequence (seq_length).\\n\\nThe weights are calculated through applying a softmax activation to the align‐ ment values.\\n\\nTo get the weighted sum of the hidden states, we need to use a RepeatVector layer to copy the weights rnn_units times to form a matrix of shape [rnn_units, seq_length], then transpose this matrix using a Permute layer to get a matrix of shape [seq_length, rnn_units]. We can then multiply this matrix pointwise with the hidden states from the final LSTM layer, which also has shape [seq_length, rnn_units]. Finally, we use a Lambda layer to perform the summation along the seq_length axis, to give the context vector of length rnn_units.\\n\\nThe network has a double-headed output, one for the next note name and one for the next note length.\\n\\nThe final model accepts the previous note names and note durations as input and outputs a distribution for the next note name and next note duration.\\n\\nWe also create a model that outputs the alpha layer vector, so that we will be able to understand how the network is attributing weights to previous hidden states.\\n\\nThe model is compiled using categorical_crossentropy for both the note name and note duration output heads, as this is a multiclass classification problem.\\n\\nA diagram of the full model built in Keras is shown in Figure 7-7.\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\n211\\n\\nFigure 7-7. The LSTM model with attention for predicting the next note in a sequence\\n\\nYou can train this LSTM with attention by running the notebook called 07_02_lstm_compose_train.ipynb in the book repository.\\n\\n212\\n\\n|\\n\\nChapter 7: Compose\\n\\nAnalysis of the RNN with Attention The following analysis can be produced by running the notebook 07_03_lstm_com‐ pose_analysis.ipynb from the book repository, once you have trained your network.\\n\\nWe’ll start by generating some music from scratch, by seeding the network with only a sequence of <START> tokens (i.e., we are telling the model to assume it is starting from the beginning of the piece). Then we can generate a musical passage using the same iterative technique we used in Chapter 6 for generating text sequences, as follows:\\n\\n1. Given the current sequence (of note names and note durations), the model pre‐ dicts two distributions, for the next note name and duration.\\n\\n2. We sample from both of these distributions, using a temperature parameter to control how much variation we would like in the sampling process.\\n\\n3. The chosen note is stored and its name and duration are appended to the respec‐ tive sequences.\\n\\n4. If the length of the sequence is now greater than the sequence length that the model was trained on, we remove one element from the start of the sequence.\\n\\n5. The process repeats with the new sequence, and so on, for as many notes as we wish to generate.\\n\\nFigure 7-8 shows examples of music generated from scratch by the model at various epochs of the training process.\\n\\nMost of our analysis in this section will focus on the note pitch predictions, rather than rhythms, as for Bach’s Cello Suites the harmonic intricacies are more difficult to capture and therefore more worthy of investigation. However, you can also apply the same analysis to the rhythmic predictions of the model, which may be particularly relevant for other styles of music that you could use to train this model (such as a drum track).\\n\\nThere are several points to note about the generated passages in Figure 7-8. First, see how the music is becoming more sophisticated as training progresses. To begin with, the model plays it safe by sticking to the same group of notes and rhythms. By epoch 10, the model has begun to generate small runs of notes, and by epoch 20 it is pro‐ ducing interesting rhythms and is firmly established in a set key (E-flat major).\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\n213\\n\\nFigure 7-8. Some examples of passages generated by the model when seeded only with a sequence of <START> tokens; here we use a temperature of 0.5 for the note names and durations\\n\\nSecond, we can analyze the distribution of note pitches over time by plotting the pre‐ dicted distribution at each timestep as a heatmap. Figure 7-9 shows this heatmap for the example from epoch 20 in Figure 7-8.\\n\\n214\\n\\n|\\n\\nChapter 7: Compose\\n\\nFigure 7-9. The distribution of possible next notes over time (at epoch 20): the darker the square, the more certain the model is that the next note is at this pitch\\n\\nAn interesting point to note here is that the model has clearly learned which notes belong to particular keys, as there are gaps in the distribution at notes that do not belong to the key. For example, there is a gray gap along the row for note 54 (corre‐ sponding to Gb/F#). This note is highly unlikely to appear in a piece of music in the key of E-flat major. Early on in the generation process (the lefthand side of the dia‐ gram) the key is not yet firmly established and therefore there is more uncertainty in how to choose the next note. As the piece progresses, the model settles on a key and certain notes become almost certain not to appear. What is remarkable is that the model hasn’t explicitly decided to set the music in a certain key at the beginning, but instead is literally making it up as it goes along, trying to choose the note that best fits with those it has chosen previously.\\n\\nIt is also worth pointing out that the model has learned Bach’s characteristic style of dropping to a low note on the cello to end a phrase and bouncing back up again to start the next. See how around note 20, the phrase ends on a low E-flat—it is com‐ mon in the Bach Cello Suites to then return to a higher, more sonorous range of the instrument for the start of next phrase, which is exactly what the model predicts. There is a large gray gap between the low E-flat (pitch number 39) and the next note, which is predicted to be around pitch number 50, rather than continuing to rumble around the depths of the instrument.\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\n215\\n\\nLastly, we should check to see if our attention mechanism is working as expected. Figure 7-10 shows the values of the alpha vector elements calculated by the network at each point in the generated sequence. The horizontal axis shows the generated sequence of notes; the vertical axis shows where the attention of the network was aimed when predicting each note along the horizontal axis (i.e., the alpha vector). The darker the square, the greater the attention placed on the hidden state corre‐ sponding to this point in the sequence.\\n\\nFigure 7-10. Each square in the matrix indicates the amount of attention given to the hidden state of the network corresponding to the note on the vertical axis, at the point of predicting the note on the horizontal axis; the more red the square, the more attention was given\\n\\n216\\n\\n|\\n\\nChapter 7: Compose\\n\\nWe can see that for the second note of the piece (B-3 = B-flat), the network chose to place almost all of its attention on the fact that the first note of the piece was also B-3. This makes sense; if you know that the first note is a B-flat, you will probably use this information to inform your decision about the next note.\\n\\nAs we move through the next few notes, the network spreads its attention roughly equally among previous notes—however, it rarely places any weight on notes more than six notes ago. Again, this makes sense; there is probably enough information contained in the previous six hidden states to understand how the phrase should continue.\\n\\nThere are also examples of where the network has chosen to ignore a certain note nearby, as it doesn’t add any additional information to its understanding of the phrase. For example, take a look inside the white box marked in the center of the dia‐ gram, and note how there is a strip of boxes in the middle that cuts through the usual pattern of looking back at the previous four to six notes. Why would the network willingly choose to ignore this note when deciding how to continue the phrase?\\n\\nIf you look across to see which note this corresponds to, you can see that it is the first of three E-3 (E-flat) notes. The model has chosen to ignore this because the note prior to this is also an E-flat, an octave lower (E-2). The hidden state of the network at this point will provide ample information for the model to understand that E-flat is an important note in this passage, and therefore the model does not need to pay attention to the subsequent higher E-flat, as it doesn’t add any extra information.\\n\\nAdditional evidence that the model has started to understand the concept of an octave can be seen inside the green box below and to the right. Here the model has chosen to ignore the low G (G2) because the note prior to this was also a G (G3), an octave higher. Remember we haven’t told the model anything about which notes are related through octaves—it has worked this out for itself just by studying the music of J.S. Bach, which is remarkable.\\n\\nAttention in Encoder–Decoder Networks The attention mechanism is a powerful tool that helps the network decide which pre‐ vious states of the recurrent layer are important for predicting the continuation of a sequence. So far, we have seen this for one-note-ahead predictions. However, we may also wish to build attention into encoder–decoder networks, where we predict a sequence of future notes by using an RNN decoder, rather than building up sequen‐ ces one note at a time.\\n\\nTo recap, Figure 7-11 shows how a standard encoder–decoder model for music gen‐ eration might look, without attention—the kind that we introduced in Chapter 6.\\n\\nFigure 7-12 shows the same network, but with an attention mechanism between the encoder and the decoder.\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\n217\\n\\n218\\n\\n|\\n\\nChapter 7: Compose\\n\\nl e d o m\\n\\nr e d o c e d – r e d o c n e d r a d n a t s\\n\\ne\\n\\nTh\\n\\n.\\n\\n1 1 - 7 e r u g i F\\n\\nYour First Music-Generating RNN\\n\\n|\\n\\nn o i t n e t t a h t i\\n\\nw\\n\\nl e d o m\\n\\nr e d o c e d – r e d o c n e n A\\n\\n.\\n\\n2 1 - 7 e r u g i F\\n\\n219\\n\\nThe attention mechanism works in exactly the same way as we have seen previously, with one alteration: the hidden state of the decoder is also rolled into the mechanism so that the model is able to decide where to focus its attention not only through the previous encoder hidden states, but also from the current decoder hidden state. Figure 7-13 shows the inner workings of an attention module within an encoder– decoder framework.\\n\\nFigure 7-13. An attention mechanism within the context of an encoder-decoder network, connected to decoder cell i\\n\\nWhile there are many copies of the attention mechanism within the encoder–decoder network, they all share the same weights, so there is no extra overhead in the number of parameters to be learned. The only change is that now, the decoder hidden state is rolled into the attention calculations (the red lines in the diagram). This slightly\\n\\n220\\n\\n|\\n\\nChapter 7: Compose\\n\\nchanges the equations to incorporate an extra index (i) to specify the step of the decoder.\\n\\nAlso notice how in Figure 7-11 we use the final state of the encoder to initialize the hidden state of the decoder. In an encoder–decoder with attention, we instead initial‐ ize the decoder using the built-in standard initializers for a recurrent layer. The con‐ text vector ci is concatenated with the incoming data yi–1 to form an extended vector of data into each cell of the decoder. Thus, we treat the context vectors as additional data to be fed into the decoder.\\n\\nGenerating Polyphonic Music The RNN with attention mechanism framework that we have explored in this section works well for single-line (monophonic) music, but could it be adapted to multiline (polyphonic) music?\\n\\nThe RNN framework is certainly flexible enough to conceive of an architecture whereby multiple lines of music are generated simultaneously, through a recurrent mechanism. But as it stands, our current dataset isn’t well set up for this, as we are storing chords as single entities rather than parts that consist of multiple individual notes. There is no way for our current RNN to know, for example, that a C-major chord (C, E, and G) is actually very close to an A-minor chord (A, C, and E)—only one note would need to change, the G to an A. Instead, it treats both as two distinct elements to be predicted independently.\\n\\nIdeally, we would like to design a network that can accept multiple channels of music as individual streams and learn how these streams should interact with each other to generate beautiful-sounding music, rather than disharmonious noise.\\n\\nDoesn’t this sound a bit like generating images? For image generation we have three channels (red, green, and blue), and we want the network to learn how to combine these channels to generate beautiful-looking images, rather than random pixelated noise.\\n\\nIn fact, as we shall see in the next section, we can treat music generation directly as an image generation problem. This means that instead of using recurrent networks we can apply the same convolutional-based techniques that worked so well for image generation problems to music—in particular, GANs.\\n\\nBefore we explore this new architecture, there is just enough time to visit the concert hall, where a performance is about to begin…\\n\\nThe Musical Organ The conductor taps his baton twice on the podium. The performance is about to begin. In front of him sits an orchestra. However, this orchestra isn’t about to launch\\n\\nThe Musical Organ\\n\\n|\\n\\n221\\n\\ninto a Beethoven symphony or a Tchaikovsky overture. This orchestra composes original music live during the performance and is powered entirely by a set of players giving instructions to a huge Musical Organ (MuseGAN for short) in the middle of the stage, which converts these instructions into beautiful music for the pleasure of the audience. The orchestra can be trained to generate music in a particular style, and no two performances are ever the same.\\n\\nThe 128 players in the orchestra are divided into 4 equal sections of 32 players. Each section gives instructions to the MuseGAN and has a distinct responsibility within the orchestra.\\n\\nThe style section is in charge of producing the overall musical stylistic flair of the per‐ formance. In many ways, it has the easiest job of all the sections as each player simply has to generate a single instruction at the start of the concert that is then continually fed to the MuseGAN throughout the performance.\\n\\nThe groove section has a similar job, but each player produces several instructions: one for each of the distinct musical tracks that are output by the MuseGAN. For example, in one concert, each member of the groove section produced five instruc‐ tions, one for each of the vocal, piano, string, bass, and drum tracks. Thus, their job is to provide the groove for each individual instrumental sound that is then constant throughout the performance.\\n\\nThe style and groove sections do not change their instructions throughout the piece. The dynamic element of the performance is provided by the final two sections, which ensure that the music is constantly changing with each bar that goes by. A bar (or measure) is a small unit of music that contains a fixed, small number of beats. For example, if you can count 1, 2, 1, 2 along to a piece of music, then there are two beats in each bar and you’re probably listening to a march. If you can count 1, 2, 3, 1, 2, 3, then there are three beats to each bar and you may be listening to a waltz.\\n\\nThe players in the chords section change their instructions at the start of each bar. This has the effect of giving each bar a distinct musical character, for example, through a change of chord. The players in the chords section only produce one instruction per bar that then applies to every instrumental track.\\n\\nThe players in the melody section have the most exhausting job, because they give dif‐ ferent instructions to each instrumental track at the start of every bar throughout the piece. These players have the most fine-grained control over the music, and this can therefore be thought of as the section that provides the melodic interest.\\n\\nThis completes the description of the orchestra. We can summarize the responsibili‐ ties of each section as shown in Table 7-2.\\n\\n222\\n\\n|\\n\\nChapter 7: Compose\\n\\nTable 7-2. Sections of the MuseGAN orchestra\\n\\nInstructions change with each bar? Different instruction per track?\\n\\nStyle Groove Chords Melody\\n\\nＸ Ｘ ✓ ✓\\n\\nＸ ✓ Ｘ ✓\\n\\nIt is up to the MuseGAN to generate the next bar of music, given the current set of 128 instructions (one from each player). Training the MuseGAN to do this isn’t easy. Initially the instrument only produces horrendous noise, as it has no way to under‐ stand how it should interpret the instructions to produce bars that are indistinguisha‐ ble from genuine music.\\n\\nThis is where the conductor comes in. The conductor tells the MuseGAN when the music it is producing is clearly distinguishable from real music, and the MuseGAN then adapts its internal wiring to be more likely to fool the conductor the next time around. The conductor and the MuseGAN use exactly the same process as we saw in Chapter 4, when Di and Gene worked together to continuously improve the photos of ganimals taken by Gene.\\n\\nThe MuseGAN players tour the world giving concerts in any style where there is suf‐ ficient existing music to train the MuseGAN. In the next section we’ll see how we can build a MuseGAN using Keras, to learn how to generate realistic polyphonic music.\\n\\nYour First MuseGAN The MuseGAN was introduced in the 2017 paper “MuseGAN: Multi-Track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompani‐ ment.”2 The authors show how it is possible to train a model to generate polyphonic, multitrack, multibar music through a novel GAN framework. Moreover, they show how, by dividing up the responsibilities of the noise vectors that feed the generator, they are able to maintain fine-grained control over the high-level temporal and track- based features of the music.\\n\\nTo begin this project, you’ll first need to download the MIDI files that we’ll be using to train the MuseGAN. We’ll use a dataset of 229 J.S. Bach chorales for four voices, available on GitHub. Download this dataset and place it inside the data folder of the book repository, in a folder called chorales. The dataset consists of an array of four numbers for each timestep: the MIDI note pitches of each of the four voices. A time‐ step in this dataset is equal to a 16th note (a semiquaver). So, for example, in a single\\n\\n2 Hao-Wen Dong et al., “MuseGAN: Multi-Track Sequential Generative Adversarial Networks for Symbolic\\n\\nMusic Generation and Accompaniment,” 19 September 2017, https://arxiv.org/abs/1709.06298.\\n\\nYour First MuseGAN\\n\\n|\\n\\n223\\n\\nbar of 4 quarter (crotchet) beats, there would be 16 timesteps. Also, the dataset is automatically split into train, validation, and test sets. We will be using the train data‐ set to train the MuseGAN.\\n\\nWe first need to get the data into the correct shape to feed the GAN. In this example, we’ll generate two bars of music, so we’ll first extract only the first two bars of each chorale. Figure 7-14 shows how two bars of raw data are converted into the trans‐ formed dataset that will feed the GAN with the corresponding musical notation.\\n\\nEach bar consists of 16 timesteps and there are a potential 84 pitches across the 4 tracks. Therefore, a suitable shape for the transformed data is:\\n\\n[batch_size, n_bars, n_steps_per_bar, n_pitches, n_tracks]\\n\\nwhere\\n\\nn_bars = 2 n_steps_per_bar = 16 n_pitches = 84 n_tracks = 4\\n\\nTo get the data into this shape, we one-hot encode the pitch numbers into a vector of length 84 and split each sequence of notes into two groups of 16, to replicate 2 bars.3\\n\\nNow that we have transformed our dataset, let’s take a look at the overall structure of the MuseGAN, starting with the generator.\\n\\n3 We are making the assumption here that each chorale in the dataset has four beats in each bar, which is rea‐\\n\\nsonable, and even if this were not the case it would not adversely affect the training of the model.\\n\\n224\\n\\n|\\n\\nChapter 7: Compose\\n\\nFigure 7-14. Example of MuseGAN raw data\\n\\nYour First MuseGAN\\n\\n|\\n\\n225\\n\\nThe MuseGAN Generator Like all GANs, the MuseGAN consists of a generator and a critic. The generator tries to fool the critic with its musical creations, and the critic tries to prevent this from happening by ensuring it is able to tell the difference between the generator’s forged Bach chorales and the real thing.\\n\\nWhere the MuseGAN is different is the fact that the generator doesn’t just accept a single noise vector as input, but instead has four separate inputs, which correspond to the four sections of the orchestra in the story—chords, style, melody, and groove. By manipulating each of these inputs independently we can change high-level properties of the generated music.\\n\\nA high-level view of the generator is shown in Figure 7-15.\\n\\nFigure 7-15. High-level diagram of the MuseGAN generator\\n\\nThe diagram shows how the chords and melody inputs are first passed through a temporal network that outputs a tensor with one of the dimensions equal to the num‐ ber of bars to be generated. The style and groove inputs are not stretched temporally in this way, as they remain constant through the piece.\\n\\nThen, to generate a particular bar for a particular track, the relevant vectors from the chords, style, melody, and groove parts of the network are concatenated to form a longer vector. This is then passed to a bar generator, which ultimately outputs the specified bar for the specified track.\\n\\nBy concatenating the generated bars for all tracks, we create a score that can be com‐ pared with real scores by the critic. You can start training the MuseGAN using the notebook 07_04_musegan_train.ipynb in the book repository. The parameters to the model are given in Example 7-4.\\n\\n226\\n\\n|\\n\\nChapter 7: Compose\\n\\nExample 7-4. Defining the MuseGAN\\n\\nBATCH_SIZE = 64 n_bars = 2 n_steps_per_bar = 16 n_pitches = 84 n_tracks = 4 z_dim = 32\\n\\ngan = MuseGAN(input_dim = data_binary.shape[1:] , critic_learning_rate = 0.001 , generator_learning_rate = 0.001 , optimiser = \\'adam\\' , grad_weight = 10 , z_dim = 32 , batch_size = 64 , n_tracks = 4 , n_bars = 2 , n_steps_per_bar = 16 , n_pitches = 84 )\\n\\nChords, Style, Melody, and Groove Let’s now take a closer look at the four different inputs that feed the generator.\\n\\nChords\\n\\nThe chords input is a vector of length 32 (z_dim). We need to output a different vec‐ tor for every bar, as its job is to control the general dynamic nature of the music over time. Note that while this is labeled chords_input, it really could control anything about the music that changes per bar, such as general rhythmic style, without being specific to any particular track.\\n\\nThe way this is achieved is with a neural network consisting of convolutional trans‐ pose layers that we call the temporal network. The Keras code to build this is shown in Example 7-5.\\n\\nExample 7-5. Building the temporal network\\n\\ndef conv_t(self, x, f, k, s, a, p, bn): x = Conv2DTranspose( filters = f , kernel_size = k , padding = p , strides = s , kernel_initializer = self.weight_init )(x)\\n\\nif bn:\\n\\nThe MuseGAN Generator\\n\\n|\\n\\n227\\n\\nx = BatchNormalization(momentum = 0.9)(x)\\n\\nif a == \\'relu\\': x = Activation(a)(x) elif a == \\'lrelu\\': x = LeakyReLU()(x)\\n\\nreturn x\\n\\ndef TemporalNetwork(self):\\n\\ninput_layer = Input(shape=(self.z_dim,), name=\\'temporal_input\\')\\n\\nx = Reshape([1,1,self.z_dim])(input_layer) x = self.conv_t(x, f=1024, k=(2,1), s=(1,1), a= \\'relu\\', p = \\'valid\\', bn = True) x = self.conv_t(x, f=self.z_dim, k=(self.n_bars - 1,1) , s=(1,1), a= \\'relu\\', p = \\'valid\\', bn = True)\\n\\noutput_layer = Reshape([self.n_bars, self.z_dim])(x)\\n\\nreturn Model(input_layer, output_layer)\\n\\nThe input to the temporal network is a vector of length 32 (z_dim).\\n\\nWe reshape this vector to a 1 × 1 tensor with 32 channels, so that we can apply convolutional transpose operations to it.\\n\\nWe apply Conv2DTranspose layers to expand the size of the tensor along one axis, so that it is the same length as n_bars.\\n\\nWe remove the unnecessary extra dimension with a Reshape layer.\\n\\nThe reason we use convolutional operations rather than requiring two independent chord vectors into the network is because we would like the network to learn how one bar should follow on from another in a consistent way. Using a neural network to expand the input vector along the time axis means the model has a chance to learn how music flows across bars, rather than treating each bar as completely independent of the last.\\n\\nStyle\\n\\nThe style input is also a vector of length z_dim. This is carried across to the bar gener‐ ator without any change, as it is independent of the track and bar. In other words, the bar generator should use this vector to establish consistency between bars and tracks.\\n\\n228\\n\\n|\\n\\nChapter 7: Compose\\n\\nMelody\\n\\nThe melody input is an array of shape [n_tracks, z_dim]—that is, we provide the model with a random noise vector of length z_dim for each track.\\n\\nEach of these vectors is passed through its own copy of the temporal network speci‐ fied previously. Note that the weights of these copies are not shared. The output is therefore a vector of length z_dim for every track of every bar. This way, the bar gen‐ erator will be able to use this vector to fine-tune the content of every single bar and track independently.\\n\\nGroove\\n\\nThe groove input is also an array of shape [n_tracks, z_dim]—a random noise vec‐ tor of length z_dim for each track. Unlike the melody input, these are not passed through the temporal network but instead are fed straight through to the bar genera‐ tor unchanged, just like the style vector. However, unlike in the style vector there is a distinct groove input for every track, meaning that we can use these vectors to adjust the overall output for each track independently.\\n\\nThe Bar Generator The bar generator converts a vector of length 4 * z_dim to a single bar for a single track—i.e., a tensor of shape [1, n_steps_per_bar, n_pitches, 1]. The input vec‐ tor is created through the concatenation of the four relevant chord, style, melody, and groove vectors, each of length z_dim.\\n\\nThe bar generator is a neural network that uses convolutional transpose layers to expand the time and pitch dimensions. We will be creating one bar generator for every track, and weights are not shared. The Keras code to build a bar generator is given in Example 7-6.\\n\\nExample 7-6. Building the bar generator\\n\\ndef BarGenerator(self):\\n\\ninput_layer = Input(shape=(self.z_dim * 4,), name=\\'bar_generator_input\\')\\n\\nx = Dense(1024)(input_layer) x = BatchNormalization(momentum = 0.9)(x) x = Activation(\\'relu\\')(x)\\n\\nx = Reshape([2,1,512])(x) x = self.conv_t(x, f=512, k=(2,1), s=(2,1), a= \\'relu\\', p = \\'same\\', bn = True) x = self.conv_t(x, f=256, k=(2,1), s=(2,1), a= \\'relu\\', p = \\'same\\', bn = True) x = self.conv_t(x, f=256, k=(2,1), s=(2,1), a= \\'relu\\', p = \\'same\\', bn = True) x = self.conv_t(x, f=256, k=(1,7), s=(1,7), a= \\'relu\\', p = \\'same\\',bn = True)\\n\\nThe MuseGAN Generator\\n\\n|\\n\\n229\\n\\nx = self.conv_t(x, f=1, k=(1,12), s=(1,12), a= \\'tanh\\', p = \\'same\\', bn = False)\\n\\noutput_layer = Reshape([1, self.n_steps_per_bar , self.n_pitches ,1])(x)\\n\\nreturn Model(input_layer, output_layer)\\n\\nThe input to the bar generator is a vector of length 4 * z_dim.\\n\\nAfter passing through a Dense layer, we reshape the tensor to prepare it for the convolutional transpose operations.\\n\\nFirst we expand the tensor along the timestep axis…\\n\\n…then along the pitch axis.\\n\\nThe final layer has a tanh activation applied, as we will be using a WGAN-GP (which requires tanh output activation) to train the network.\\n\\nThe tensor is reshaped to add two extra dimensions of size 1, to prepare it for concatenation with other bars and tracks.\\n\\nPutting It All Together Ultimately the MuseGAN has one single generator that incorporates all of the tempo‐ ral networks and bar generators. This network takes the four input tensors and con‐ verts them into a multitrack, multibar score. The Keras code to build the overall generator is provided in Example 7-7.\\n\\nExample 7-7. Building the MuseGAN generator\\n\\nchords_input = Input(shape=(self.z_dim,), name=\\'chords_input\\') style_input = Input(shape=(self.z_dim,), name=\\'style_input\\') melody_input = Input(shape=(self.n_tracks, self.z_dim), name=\\'melody_input\\') groove_input = Input(shape=(self.n_tracks, self.z_dim), name=\\'groove_input\\')\\n\\n# CHORDS -> TEMPORAL NETWORK self.chords_tempNetwork = self.TemporalNetwork() self.chords_tempNetwork.name = \\'temporal_network\\' chords_over_time = self.chords_tempNetwork(chords_input) # [n_bars, z_dim]\\n\\n# MELODY -> TEMPORAL NETWORK melody_over_time = [None] * self.n_tracks # list of n_tracks [n_bars, z_dim] tensors self.melody_tempNetwork = [None] * self.n_tracks for track in range(self.n_tracks): self.melody_tempNetwork[track] = self.TemporalNetwork() melody_track = Lambda(lambda x: x[:,track,:])(melody_input) melody_over_time[track] = self.melody_tempNetwork[track](melody_track)\\n\\n230\\n\\n|\\n\\nChapter 7: Compose\\n\\n# CREATE BAR GENERATOR FOR EACH TRACK self.barGen = [None] * self.n_tracks for track in range(self.n_tracks): self.barGen[track] = self.BarGenerator()\\n\\n# CREATE OUTPUT FOR EVERY TRACK AND BAR bars_output = [None] * self.n_bars for bar in range(self.n_bars): track_output = [None] * self.n_tracks\\n\\nc = Lambda(lambda x: x[:,bar,:]\\n\\n, name = \\'chords_input_bar_\\' + str(bar))(chords_over_time)\\n\\ns = style_input\\n\\nfor track in range(self.n_tracks):\\n\\nm = Lambda(lambda x: x[:,bar,:])(melody_over_time[track]) g = Lambda(lambda x: x[:,track,:])(groove_input)\\n\\nz_input = Concatenate(axis = 1\\n\\n, name = \\'total_input_bar_{}_track_{}\\'.format(bar, track) )([c,s,m,g])\\n\\ntrack_output[track] = self.barGen[track](z_input)\\n\\nbars_output[bar] = Concatenate(axis = -1)(track_output)\\n\\ngenerator_output = Concatenate(axis = 1, name = \\'concat_bars\\')(bars_output)\\n\\nself.generator = Model([chords_input, style_input, melody_input, groove_input]\\n\\n, generator_output)\\n\\nThe inputs to the generator are defined.\\n\\nPass the chords input through the temporal network.\\n\\nPass the melody input through the temporal network.\\n\\nCreate an independent bar generator network for every track.\\n\\nLoop over the tracks and bars, creating a generated bar for each combination.\\n\\nConcatenate everything together to form a single output tensor.\\n\\nThe MuseGAN model takes four distinct noise tensors as input and outputs a generated multitrack, multibar score.\\n\\nThe MuseGAN Generator\\n\\n|\\n\\n231\\n\\nThe Critic In comparison to the generator, the critic architecture is much more straightforward (as is often the case with GANs).\\n\\nThe critic tries to distinguish full multitrack, multibar scores created by the generator from real excepts from the Bach chorales. It is a convolutional neural network, con‐ sisting mostly of Conv3D layers that collapse the score into a single output prediction. So far, we have only worked with Conv2D layers, applicable to three-dimensional input images (width, height, channels). Here we have to use Conv3D layers, which are analo‐ gous to Conv2D input tensors (n_bars, n_steps_per_bar, n_pitches, n_tracks).\\n\\nlayers but accept four-dimensional\\n\\nAlso, we do not use batch normalization layers in the critic as we will be using the WGAN-GP framework for training the GAN, which forbids this.\\n\\nThe Keras code to build the critic is given in Example 7-8.\\n\\nExample 7-8. Building the MuseGAN critic\\n\\ndef conv(self, x, f, k, s, a, p): x = Conv3D( filters = f , kernel_size = k , padding = p , strides = s , kernel_initializer = self.weight_init )(x)\\n\\nif a ==\\'relu\\': x = Activation(a)(x) elif a== \\'lrelu\\': x = LeakyReLU()(x)\\n\\nreturn x\\n\\ncritic_input = Input(shape=self.input_dim, name=\\'critic_input\\')\\n\\nx = critic_input x = self.conv(x, f=128, k = (2,1,1), s = (1,1,1), a = \\'lrelu\\', p = \\'valid\\') x = self.conv(x, f=128, k = (self.n_bars - 1,1,1) , s = (1,1,1), a = \\'lrelu\\', p = \\'valid\\')\\n\\nx = self.conv(x, f=128, k = (1,1,12), s = (1,1,12), a = \\'lrelu\\', p = \\'same\\') x = self.conv(x, f=128, k = (1,1,7), s = (1,1,7), a = \\'lrelu\\', p = \\'same\\') x = self.conv(x, f=128, k = (1,2,1), s = (1,2,1), a = \\'lrelu\\', p = \\'same\\') x = self.conv(x, f=128, k = (1,2,1), s = (1,2,1), a = \\'lrelu\\', p = \\'same\\') x = self.conv(x, f=256, k = (1,4,1), s = (1,2,1), a = \\'lrelu\\', p = \\'same\\')\\n\\n232\\n\\n|\\n\\nChapter 7: Compose\\n\\nx = self.conv(x, f=512, k = (1,3,1), s = (1,2,1), a = \\'lrelu\\', p = \\'same\\')\\n\\nx = Flatten()(x)\\n\\nx = Dense(1024, kernel_initializer = self.weight_init)(x) x = LeakyReLU()(x) critic_output = Dense(1, activation=None\\n\\n, kernel_initializer = self.weight_init)(x)\\n\\nself.critic = Model(critic_input, critic_output)\\n\\nThe input to the critic is an array of multitrack, multibar scores, each of shape [n_bars, n_steps_per_bar, n_pitches, n_tracks].\\n\\nFirst, we collapse the tensor along the bar axis. We apply Conv3D layers through‐ out the critic as we are working with 4D tensors.\\n\\nNext, we collapse the tensor along the pitch axis.\\n\\nFinally, we collapse the tensor along the timesteps axis.\\n\\nThe output is a Dense layer with a single unit and no activation function, as required by the WGAN-GP framework.\\n\\nAnalysis of the MuseGAN We can perform some experiments with our MuseGAN by generating a score, then tweaking some of the input noise parameters to see the effect on the output.\\n\\nThe output from the generator is an array of values in the range [–1, 1] (due to the tanh activation function of the final layer). To convert this to a single note for each track, we choose the note with the maximum value over all 84 pitches for each time‐ step. In the original MuseGAN paper the authors use a threshold of 0, as each track can contain multiple notes; however, in this setting we can simply take the maximum, to guarantee exactly one note per timestep per track, as is the case for the Bach chorales.\\n\\nFigure 7-16 shows a score that has been generated by the model from random nor‐ mally distributed noise vectors (top left). We can find the closest score in the dataset (by Euclidean distance) and check that our generated score isn’t a copy of a piece of music that already exists in the dataset—the closest score is shown just below it, and we can see that it does not resemble our generated score.\\n\\nAnalysis of the MuseGAN\\n\\n|\\n\\n233\\n\\nFigure 7-16. Example of a MuseGAN predicted score, showing the closest real score in the training data and how the generated score is affected by changing the input noise\\n\\nLet’s now play around with the input noise to tweak our generated score. First, we can try changing the noise vector—the bottom-left score in Figure 7-16 shows the result. We can see that every track has changed, as expected, and also that the two bars exhibit different properties. In the second bar, the baseline is more dynamic and the top line is higher in pitch than in the first bar.\\n\\nWhen we change the style vector (top right), both bars change in a similar way. There is no great difference in style between the two bars, but the whole passage has changed from the original generated score.\\n\\nWe can also alter tracks individually, through the melody and groove inputs. In Figure 7-16 we can see the effect of changing just the melody noise input for the top line of music. All other parts remain unaffected, but the top-line notes change signifi‐ cantly. Also, we can see a rhythmic change between the two bars in the top line: the second bar is more dynamic, containing faster notes than the first bar.\\n\\n234\\n\\n|\\n\\nChapter 7: Compose\\n\\nLastly, the bottom-right score in the diagram shows the predicted score when we alter the groove input parameter for only the baseline. Again, all other parts remain unaf‐ fected, but the baseline is different. Moreover, the overall pattern of the baseline remains similar between bars, as we would expect.\\n\\nThis shows how each of the input parameters can be used to directly influence high- level features of the generated musical sequence, in much the same way as we were able to adjust the latent vectors of VAEs and GANs in previous chapters to alter the appearance of a generated image. One drawback to the model is that the number of bars to generate must be specified up front. The tackle this, the authors show a exten‐ sion to the model that allows previous bars to be fed in as input, therefore allowing the model to generate long-form scores by continually feeding the most recent pre‐ dicted bars back into the model as additional input.\\n\\nSummary In this chapter we have explored two different kinds of model for music generation: a stacked LSTM with attention and a MuseGAN.\\n\\nThe stacked LSTM is similar in design to the networks we saw in Chapter 6 for text generation. Music and text generation share a lot of features in common, and often similar techniques can be used for both. We enhanced the recurrent network with an attention mechanism that allows the model to focus on specific previous timesteps in order to predict the next note and saw how the model was able to learn about con‐ cepts such as octaves and keys, simply by learning to accurately generate the music of Bach.\\n\\nThen we saw that generating sequential data does not always require a recurrent model—the MuseGAN uses convolutions to generate polyphonic musical scores with multiple tracks, by treating the score as a kind of image where the tracks are individ‐ ual channels of the image. The novelty of the MuseGAN lies in the way the four input noise vectors (chords, style, melody, and groove) are organized so that it is possible to maintain full control over high-level features of the music. While the underlying har‐ monization is still not as perfect or varied as Bach, it is a good attempt at what is an extremely difficult problem to master and highlights the power of GANs to tackle a wide variety of problems.\\n\\nIn the next chapter we shall introduce one of the most remarkable models developed in recent years, the world model. In their groundbreaking paper describing it, the authors show how it possible to build a model that enables a car to drive around a simulated racetrack by first testing out strategies in its own generated “dream” of the environment. This allows the car to excel at driving around the track without ever having attempted the task, as it has already imagined how to do this successfully in its own imagined world model.\\n\\nSummary\\n\\n|\\n\\n235\\n\\nCHAPTER 8 Play\\n\\nIn March 2018, David Ha and Jürgen Schmidhuber published their “World Models” paper.1 The paper showed how it is possible to train a model that can learn how to perform a particular task through experimentation within its own generative halluci‐ nated dreams, rather than inside the environment itself. It is an excellent example of how generative modeling can be used to solve practical problems, when applied alongside other machine learning techniques such as reinforcement learning.\\n\\nA key component of the architecture is a generative model that can construct a prob‐ ability distribution for the next possible state, given the current state and action. Hav‐ ing built up an understanding of the underlying physics of the environment through random movements, the model is then able to train itself from scratch on a new task, entirely within its own internal representation of the environment. This approach led to world-best scores for both of the tasks on which it was tested.\\n\\nIn this chapter, we will explore the model in detail and show how it is possible to cre‐ ate your own version of this amazing cutting-edge technology.\\n\\nBased on the original paper, we will be building a reinforcement learning algorithm that learns how to drive a car around a racetrack as fast as possible. While we will be using a 2D computer simulation as our environment, the same technique could also be applied to real-world scenarios where testing strategies in the live environment is expensive or infeasible.\\n\\nBefore we start building the model, however, we need to take a closer look at the con‐ cept of reinforcement learning and the OpenAI Gym platform.\\n\\n1 David Ha and Jürgen Schmidhuber, “World Models,” 27 March 2018, https://arxiv.org/abs/1803.10122.\\n\\n237\\n\\nReinforcement Learning Reinforcement learning can be defined as follows:\\n\\nReinforcement learning (RL) is a field of machine learning that aims to train an agent to perform optimally within a given environment, with respect to a particular goal.\\n\\nWhile both discriminative modeling and generative modeling aim to minimize a loss function over a dataset of observations, reinforcement learning aims to maximize the long-term reward of an agent in a given environment. It is often described as one of the three major branches of machine learning, alongside supervised learning (predict‐ ing using labeled data) and unsupervised learning (learning structure from unlabeled data).\\n\\nLet’s first introduce some key terminology relating to reinforcement learning:\\n\\nEnvironment\\n\\nThe world in which the agent operates. It defines the set of rules that govern the game state update process and reward allocation, given the agent’s previous action and current game state. For example, if we were teaching a reinforcement learning algorithm to play chess, the environment would consist of the rules that govern how a given action (e.g., the move e4) affects the next game state (the new positions of the pieces on the board) and would also specify how to assess if a given position is checkmate and allocate the winning player a reward of 1 after the winning move.\\n\\nAgent\\n\\nThe entity that takes actions in the environment.\\n\\nGame state\\n\\nThe data that represents a particular situation that the agent may encounter (also just called a state), for example, a particular chessboard configuration with accompanying game information such as which player will make the next move.\\n\\nAction\\n\\nA feasible move that an agent can make.\\n\\nReward\\n\\nThe value given back to the agent by the environment after an action has been taken. The agent aims to maximize the long-term sum of its rewards. For exam‐ ple, in a game of chess, checkmating the opponent’s king has a reward of 1 and every other move has a reward of 0. Other games have rewards constantly awar‐ ded throughout the episode (e.g., points in a game of Space Invaders).\\n\\nEpisode\\n\\nOne run of an agent in the environment; this is also called a rollout.\\n\\n238\\n\\n|\\n\\nChapter 8: Play\\n\\nTimestep\\n\\nFor a discrete event environment, all states, actions, and rewards are subscripted to show their value at timestep t.\\n\\nThe relationship between these definitions is shown in Figure 8-1.\\n\\nFigure 8-1. Reinforcement learning diagram\\n\\nThe environment is first initialized with a current game state, s0. At timestep t, the agent receives the current game state st and uses this to decide its next best action at, which it then performs. Given this action, the environment then calculates the next state st + 1 and reward rt + 1 and passes these back to the agent, for the cycle to begin again. The cycle continues until the end criterion of the episode is met (e.g., a given number of timesteps elapse or the agent wins/loses).\\n\\nHow can we design an agent to maximize the sum of rewards in a given environ‐ ment? We could build an agent that contains a set of rules for how to respond to any given game state. However, this quickly becomes infeasible as the environment becomes more complex and doesn’t ever allow us to build an agent that has superhu‐ man ability in a particular task, as we are hardcoding the rules. Reinforcement learn‐ ing involves creating an agent that can learn optimal strategies by itself in complex environments through repeated play—this is what we will be using in this chapter to build our agent.\\n\\nI’ll now introduce OpenAI Gym, home of the CarRacing environment that we will use to simulate a car driving around a track.\\n\\nOpenAI Gym OpenAI Gym is a toolkit for developing reinforcement learning algorithms that is available as a Python library.\\n\\nContained within the library are several classic reinforcement learning environments, such as CartPole and Pong, as well as environments that present more complex chal‐ lenges, such as training an agent to walk on uneven terrain or win an Atari game. All of the environments provide a step method through which you can submit a given action; the environment will return the next state and the reward. By repeatedly\\n\\nReinforcement Learning\\n\\n|\\n\\n239\\n\\ncalling the step method with the actions chosen by the agent, you can play out an episode in the environment.\\n\\nIn addition to the abstract mechanics of each environment, OpenAI Gym also pro‐ vides graphics that allow you to watch your agent perform in a given environment. This is useful for debugging and finding areas where your agent could improve.\\n\\nWe will make use of the CarRacing environment within OpenAI Gym. Let’s see how the game state, action, reward, and episode are defined for this environment:\\n\\nGame state\\n\\nA 64 × 64–pixel RGB image depicting an overhead view of the track and car.\\n\\nAction\\n\\nA set of three values: the steering direction (–1 to 1), acceleration (0 to 1), and braking (0 to 1). The agent must set all three values at each timestep.\\n\\nReward\\n\\nA negative penalty of –0.1 for each timestep taken and a positive reward of 1000/N if a new track tile is visited, where N is the total number of tiles that make up the track.\\n\\nEpisode\\n\\nThe episode ends when either the car completes the track, drives off the edge of the environment, or 3,000 timesteps have elapsed.\\n\\nThese concepts are shown on a graphical representation of a game state in Figure 8-2. Note that the car doesn’t see the track from its point of view, but instead we should imagine an agent floating above the track controlling the car from a bird’s-eye view.\\n\\n240\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-2. A graphical representation of one game state in the CarRacing environment\\n\\nWorld Model Architecture We’ll now cover a high-level overview of the entire architecture that we will be using to build the agent that learns through reinforcement learning, before we explore the detailed steps required to build each component.\\n\\nThe solution consists of three distinct parts, as shown in Figure 8-3, that are trained separately:\\n\\nV\\n\\nA variational autoencoder.\\n\\nM\\n\\nA recurrent neural network with a mixture density network (MDN-RNN).\\n\\nC\\n\\nA controller.\\n\\nWorld Model Architecture\\n\\n|\\n\\n241\\n\\nFigure 8-3. World model architecture diagram\\n\\nThe Variational Autoencoder When you make decisions while driving, you don’t actively analyze every single pixel in your view—instead, you condense the visual information into a smaller number of latent entities, such as the straightness of the road, upcoming bends, and your posi‐ tion relative to the road, to inform your next action.\\n\\nWe saw in Chapter 3 how a VAE can take a high-dimensional input image and con‐ dense it into a latent random variable that approximately follows a standard multi‐ variate normal distribution, through minimization of the reconstruction error and KL divergence. This ensures that the latent space is continuous and that we are able to easily sample from it to to generate meaningful new observations.\\n\\nIn the car racing example, the VAE condenses the 64 × 64 × 3 (RGB) input image into a 32-dimensional normally distributed random variable, parameterized by two vari‐ ables, mu and log_var. Here, log_var is the logarithm of the variance of the distribution.\\n\\nWe can sample from this distribution to produce a latent vector z that represents the current state. This is passed on to the next part of the network, the MDN-RNN.\\n\\n242\\n\\n|\\n\\nChapter 8: Play\\n\\nThe MDN-RNN As you drive, each subsequent observation isn’t a complete surprise to you. If the cur‐ rent observation suggests a left turn in the road ahead and you turn the wheel to the left, you expect the next observation to show that you are still in line with the road.\\n\\nIf you didn’t have this ability, your driving would probably snake all over the road as you wouldn’t be able see that a slight deviation from the center is going to be worse in the next timestep unless you do something about it now.\\n\\nThis forward thinking is the job of the MDN-RNN, a network that tries to predict the distribution of the next latent state based on the previous latent state and the previous action.\\n\\nSpecifically, the MDN-RNN is an LSTM layer with 256 hidden units followed by a mixture density network (MDN) output layer that allows for the fact that the next latent state could actually be drawn from any one of several normal distributions.\\n\\nThe same technique was applied by one of the authors of the “World Models” paper, David Ha, to a handwriting generation task, as shown in Figure 8-4, to describe the fact that the next pen point could land in any one of the distinct red areas.\\n\\nFigure 8-4. MDN for handwriting generation\\n\\nIn the car racing example, we allow for each element of the next observed latent state to be drawn from any one of five normal distributions.\\n\\nThe Controller Until this point, we haven’t mentioned anything about choosing an action. That responsibility lies with the controller.\\n\\nThe controller is a densely connected neural network, where the input is a concatena‐ tion of z (the current latent state sampled from the distribution encoded by the VAE) and the hidden state of the RNN. The three output neurons correspond to the three actions (turn, accelerate, brake) and are scaled to fall in the appropriate ranges.\\n\\nWorld Model Architecture\\n\\n|\\n\\n243\\n\\nWe will need to train the controller using reinforcement learning as there is no train‐ ing dataset that will tell us that a certain action is good and another is bad. Instead, the agent will need to discover this for itself through repeated experimentation.\\n\\nAs we shall see later in the chapter, the crux of the “World Models” paper is that it demonstrates how this reinforcement learning can take place within the agent’s own generative model of the environment, rather than the OpenAI Gym environment. In other words, it takes place in the agent’s hallucinated version of how the environment behaves, rather than the real thing.\\n\\nTo understand the different roles of the three components and how they work together, we can imagine a dialogue between them:\\n\\nVAE (looking at latest 64 × 64 × 3 observation): This looks like a straight road, with a slight left bend approaching, with the car facing in the direction of the road (z). RNN: Based on that description (z) and the fact that the controller chose to accelerate hard at the last timestep (action), I will update my hidden state so that the next obser‐ vation is predicted to still be a straight road, but with slightly more left turn in view. Controller: Based on the description from the VAE (z) and the current hidden state from the RNN (h), my neural network outputs [0.34, 0.8, 0] as the next action.\\n\\nThe action from the controller is then passed to the environment, which returns an updated observation, and the cycle begins again.\\n\\nFor further information on the model, there is also an excellent interactive explana‐ tion available online.\\n\\nSetup We are now ready to start exploring how to build and train this model in Keras. If you’ve got a high-spec laptop, you can run the solution locally, but I’d recommend using cloud resources such as Google Cloud Compute Engine for access to powerful machines that you can use in short bursts.\\n\\nThe following code has been tested on Ubuntu 16.04, so it is spe‐ cific to a Linux terminal.\\n\\nFirst install the following libraries:\\n\\nsudo apt-get install cmake swig python3-dev \\\\ zlib1g-dev python-opengl mpich xvfb \\\\ xserver-xephyr vnc4server\\n\\nThen clone the following repository:\\n\\n244\\n\\n|\\n\\nChapter 8: Play\\n\\ngit clone https://github.com/AppliedDataSciencePartners/WorldModels.git\\n\\nAs the codebase for this project is stored separately from the book repository, I sug‐ gest creating a separate virtual environment to work in:\\n\\nmkvirtualenv worldmodels cd WorldModels pip install -r requirements.txt\\n\\nNow you’re good to go!\\n\\nTraining Process Overview Here’s an overview of the five-step training process:\\n\\n1. Collect random rollout data Here, the agent does not care about the given task, but instead simply explores the environment at random. This will be conducted using OpenAI Gym to simulate multiple episodes and store the observed state, action, and reward at each timestep. The idea here is to build up a dataset of how the physics of the environment works, which the VAE can then learn from to capture the states efficiently as latent vectors. The MDN-RNN can then subse‐ quently learn how the latent vectors evolve over time.\\n\\n2. Train the VAE Using the randomly collected data, we train a VAE on the observa‐ tion images.\\n\\n3. Collect data to train the MDN-RNN Once we have a trained VAE, we use it to encode each of the collected observations into mu and log_var vectors, which are saved alongside the current action and reward.\\n\\n4. Train the MDN-RNN We take batches of 100 episodes and load the correspond‐ ing mu, log_var, action, and reward variables at each timestep that were gener‐ ated in step 3. We then sample a z vector from the mu and log_var vectors. Given the current z vector, action, and reward, the MDN-RNN is then trained to pre‐ dict the subsequent z vector and reward.\\n\\n5. Train the controller With a trained VAE and RNN, we can now train the control‐ ler to output an action given the current z and hidden state, h, of the RNN. The controller uses an evolutionary algorithm, CMA-ES (Covariance Matrix Adapta‐ tion Evolution Strategy), as its optimizer. The algorithm rewards matrix weight‐ ings that generate actions that lead to overall high scores on the task, so that future generations are also likely to inherit this desired behavior.\\n\\nLet’s now take a closer look at each of these steps in more detail.\\n\\nCollecting Random Rollout Data To start collecting data, run the following command from your terminal:\\n\\nTraining Process Overview\\n\\n|\\n\\n245\\n\\nbash 01_generate_data.sh <env_name> <parallel_process> <episodes_per_process> \\\\ <render> <action_refresh_rate>\\n\\nwhere the parameters are as follows:\\n\\n<env_name>\\n\\nThe name of the environment used by the make_env function (e.g., car_racing).\\n\\n<parallel_process>\\n\\nThe number of processes to run (e.g., 8 for an 8-core machine).\\n\\n<episodes_per_process>\\n\\nHow many episodes each process should run (e.g., 125, so 8 processes would cre‐ ate 1,000 episodes overall).\\n\\n<max_timesteps>\\n\\nThe maximum number of timesteps per episode (e.g., 300).\\n\\n<render>\\n\\n1 to render the rollout process in a window (otherwise 0).\\n\\n<action_refresh_rate>\\n\\nThe number of timesteps to freeze the current action for before changing. This prevents the action from changing too rapidly for the car to make progress.\\n\\nFor example, on an 8-core machine, you could run:\\n\\nbash 01_generate_data.sh car_racing 8 125 300 0 5\\n\\nThis would start 8 processes running in parallel, each simulating 125 episodes, with a maximum of 300 timesteps each and an action refresh rate of 5 timesteps.\\n\\nEach process calls the Python file 01_generate_data.py. The key part of the script is outlined in Example 8-1.\\n\\nExample 8-1. 01_generate_data.py excerpt\\n\\n# ...\\n\\nDIR_NAME = \\'./data/rollout/\\'\\n\\nenv = make_env(current_env_name) s = 0 while s < total_episodes: episode_id = random.randint(0, 2**31-1) filename = DIR_NAME + str(episode_id)+\".npz\" observation = env.reset() env.render() t = 0 obs_sequence = []\\n\\n246\\n\\n|\\n\\nChapter 8: Play\\n\\naction_sequence = [] reward_sequence = [] done_sequence = [] reward = -0.1 done = False\\n\\nwhile t < time_steps: if t % action_refresh_rate == 0: action = config.generate_data_action(t, env) observation = config.adjust_obs(observation) obs_sequence.append(observation) action_sequence.append(action) reward_sequence.append(reward) done_sequence.append(done) observation, reward, done, info = env.step(action) t = t + 1\\n\\nprint(\"Episode {} finished after {} timesteps\".format(s, t)) np.savez_compressed(filename\\n\\n, obs=obs_sequence , action=action_sequence , reward = reward_sequence , done = done_sequence)\\n\\ns = s + 1 env.close()\\n\\nmake_env is a custom function in the repository that creates the appropriate OpenAI Gym environment. In this case, we are creating the CarRacing environ‐ ment, with a few tweaks. The environment file is stored in the custom_envs folder.\\n\\ngenerate_data_action is a custom function that stores the rules for generating random actions.\\n\\nThe observations that are returned by the environment are scaled between 0 and 255. We want observations that are scaled between 0 and 1, so this function is simply a division by 255.\\n\\nEvery OpenAI Gym environment includes a step method. This returns the next observation, reward, and done flag, given an action.\\n\\nWe save each episode as an individual file inside the ./data/rollout/ directory.\\n\\nFigure 8-5 shows an excerpt from frames 40 to 59 of one episode, as the car approaches a corner, alongside the randomly chosen action and reward. Note how the reward changes to 3.22 as the car rolls over new track tiles but is otherwise –0.1. Also, the action changes every five frames as the action_refresh_rate is 5.\\n\\nCollecting Random Rollout Data\\n\\n|\\n\\n247\\n\\nFigure 8-5. Frames 40 to 59 of one episode\\n\\nTraining the VAE We can now build a generative model (a VAE) on this collected data.\\n\\nRemember, the aim of the VAE is to allow us to collapse one 64 × 64 × 3 image into a normally distributed random variable, whose distribution is parameterized by two vectors, mu and log_var. Each of these vectors is of length 32.\\n\\nTo start training the VAE, run the following command from your terminal:\\n\\npython 02_train_vae.py --new_model [--N] [--epochs]\\n\\n248\\n\\n|\\n\\nChapter 8: Play\\n\\nwhere the parameters are as follows:\\n\\n--new_model\\n\\nWhether the model should be trained from scratch. Set this flag initially; if it’s not set, the code will look for a ./vae/vae.json file and continue training a previous model.\\n\\n--N (optional)\\n\\nThe number of episodes to use when training the VAE (e.g., 1000—the VAE does not need to use all the episodes to achieve good results, so to speed up training you can use only a sample of the episodes).\\n\\n--epochs (optional)\\n\\nThe number of training epochs (e.g., 3).\\n\\nThe output of the training process should be as shown in Figure 8-6. A file storing the weights of the trained network is saved to ./vae/vae.json every epoch.\\n\\nFigure 8-6. Training the VAE\\n\\nThe VAE Architecture As we have seen previously, the Keras functional API allows us to not only define the full VAE model that will be trained, but also additional models that reference the encoder and decoder parts of the trained network separately. These will be useful when we want to encode a specific image, or decode a given z vector, for example.\\n\\nIn this example, we define four different models on the VAE:\\n\\nfull_model\\n\\nThis is the full end-to-end model that is trained.\\n\\nencoder\\n\\nThis accepts a 64 × 64 × 3 observation as input and outputs a sampled z vector. If you run the predict method of this model for the same input multiple times, you\\n\\nTraining the VAE\\n\\n|\\n\\n249\\n\\nwill get different output, since even though the mu and log_var values are con‐ stant, the randomly sampled z vector will be different each time.\\n\\nencoder_mu_log_var\\n\\nThis accepts a 64 × 64 × 3 observation as input and outputs the mu and log_var vectors corresponding to this input. Unlike with the vae_encoder model, if you run the predict method of this model multiple times, you will always get the same output: a mu vector of length 32 and a log_var vector of length 32.\\n\\ndecoder\\n\\nThis accepts a z vector as input and returns the reconstructed 64 × 64 × 3 obser‐ vation.\\n\\nA diagram of the VAE is given in Figure 8-7. You can play around with the VAE architecture by editing the ./vae/arch.py file. This is where the VAE class and parame‐ ters of the neural network are defined.\\n\\n250\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-7. The VAE architecture for the “World Models” paper\\n\\nTraining the VAE\\n\\n|\\n\\n251\\n\\nExploring the VAE We’ll now take a look at the output from the predict methods of the different models built on the VAE to see how they differ, and then see how the VAE can be used to generate completely new track observations.\\n\\nThe full model\\n\\nIf we feed the full_model with an observation, it is able to reconstruct an accurate representation of the image, as shown in Figure 8-8. This is useful to visually check that the VAE is working correctly.\\n\\nFigure 8-8. The input and output from the full VAE model\\n\\nThe encoder models\\n\\nIf we feed the encoder_mu_log_var model with an observation, the output is the gen‐ erated mu and log_var vectors describing a multivariate normal distribution.\\n\\nThe encoder model goes one step further by sampling a particular z vector from this distribution.\\n\\nThe output from the two encoder models is shown in Figure 8-9.\\n\\n252\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-9. The output from the encoder models\\n\\nIt is interesting to plot the value of mu and log_var for each of the 32 dimensions (Figure 8-10), for a particular observation. Notice how only 12 of the 32 dimensions differ significantly from the standard normal distribution (mu = 0, log_var = 0). This is because the VAE is trying to minimize the KL divergence, so it tries to differ from the standard normal distribution in as few dimensions as possible. It has deci‐ ded that 12 dimensions are enough to capture sufficient information about the obser‐ vations for accurate reconstruction.\\n\\nTraining the VAE\\n\\n|\\n\\n253\\n\\nFigure 8-10. A plot of mu (blue line) and log_var (orange line) for each of the 32 dimen‐ sions of a particular observation\\n\\nThe decoder model\\n\\nThe decoder model accepts a z vector as input and reconstructs the original image. In Figure 8-11 we linearly interpolate two of the dimensions of z to show how each dimension appears to encode a particular aspect of the track—for example, z[4] con‐ trols the immediate left/right direction of the track nearest the car and z[7] controls the sharpness of the approaching left turn.\\n\\n254\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-11. A linear interpolation of two dimensions of z\\n\\nThis shows that the latent space that the VAE has learned is continuous and can be used to generate new track segments that have never before been observed by the agent.\\n\\nCollecting Data to Train the RNN Now that we have a trained VAE, we can use this to generate training data for our RNN.\\n\\nIn this step, we pass all of the random rollout data through the encoder_mu_log_var model and store the mu and log_var vectors corresponding to each observation. This\\n\\nCollecting Data to Train the RNN\\n\\n|\\n\\n255\\n\\nencoded data, along with the already collected actions and rewards, will be used to train the MDN-RNN.\\n\\nTo start collecting data, run the following command from your terminal:\\n\\npython 03_generate_rnn_data.py\\n\\nExample 8-2 contains an excerpt from the 03_generate_data.py file that shows how the MDN-RNN training data is generated.\\n\\nExample 8-2. Excerpt from 03_generate_data.py\\n\\ndef encode_episode(vae, episode):\\n\\nobs = episode[\\'obs\\'] action = episode[\\'action\\'] reward = episode[\\'reward\\'] done = episode[\\'done\\']\\n\\nmu, log_var = vae.encoder_mu_log_var.predict(obs)\\n\\ndone = done.astype(int) reward = np.where(reward>0, 1, 0) * np.where(done==0, 1, 0)\\n\\ninitial_mu = mu[0, :] initial_log_var = log_var[0, :]\\n\\nreturn (mu, log_var, action, reward, done, initial_mu, initial_log_var)\\n\\nvae = VAE() vae.set_weights(\\'./vae/weights.h5\\')\\n\\nfor file in filelist: rollout_data = np.load(ROLLOUT_DIR_NAME + file) mu, log_var, action, reward, done, initial_mu , initial_log_var = encode_episode(vae, rollout_data)\\n\\nnp.savez_compressed(SERIES_DIR_NAME + file, mu=mu, log_var=log_var , action = action, reward = reward, done = done) initial_mus.append(initial_mu) initial_log_vars.append(initial_log_var)\\n\\nnp.savez_compressed(ROOT_DIR_NAME + \\'initial_z.npz\\', initial_mu=initial_mus , initial_log_var=initial_log_vars)\\n\\nHere, we’re using the encoder_mu_log_var model of the VAE to get the mu and log_var vectors for a particular observation.\\n\\nThe reward value is transformed to be either 0 or 1, so that it can be used as input into the MDN-RNN.\\n\\n256\\n\\n|\\n\\nChapter 8: Play\\n\\nWe also save the initial mu and log_var vectors into a separate file—this will be useful later, for initializing the dream environment.\\n\\nTraining the MDN-RNN We can now train the MDN-RNN to predict the distribution of the next z vector and reward, given the current z value, current action, and previous reward.\\n\\nThe aim of the MDN-RNN is to predict one timestep ahead into the future—we can then use the internal hidden state of the LSTM as part of the input into the controller.\\n\\nTo start training the MDN-RNN, run the following command from your terminal:\\n\\npython 04_train_rnn.py (--new_model) (--batch_size) (--steps)\\n\\nwhere the parameters are as follows:\\n\\nnew_model\\n\\nWhether the model should be trained from scratch. Set this flag initially; if it’s not set, the code will look for a ./rnn/rnn.json file and continue training a previous model.\\n\\nbatch_size\\n\\nThe number of episodes fed to the MDN-RNN in each training iteration.\\n\\nsteps\\n\\nThe total number of training iterations.\\n\\nThe output of the training process is shown in Figure 8-12. A file storing the weights of the trained network is saved to ./rnn/rnn.json every 10 steps.\\n\\nFigure 8-12. Training the MDN-RNN\\n\\nTraining the MDN-RNN\\n\\n|\\n\\n257\\n\\nThe MDN-RNN Architecture The architecture of the MDN-RNN is shown in Figure 8-13.\\n\\nFigure 8-13. The MDN-RNN architecture\\n\\nIt consists of an LSTM layer (the RNN), followed by a densely connected layer (the MDN) that transforms the hidden state of the LSTM into the parameters of mixture distribution. Let’s walk through the network step by step.\\n\\nThe input to the LSTM layer is a vector of length 36—a concatenation of the encoded z vector (length 32) from the VAE, the current action (length 3), and the previous reward (length 1).\\n\\nThe output from the LSTM layer is a vector of length 256—one value for each LSTM cell in the layer. This is passed to the MDN, which is just a densely connected layer that transforms the vector of length 256 into a vector of length 481.\\n\\nWhy 481? Figure 8-14 explains the composition of the output from the MDN-RNN. Remember, the aim of a mixture density network is to model the fact that our next z could be drawn from one of several possible distributions with a certain probability. In the car racing example, we choose five normal distributions. How many parame‐ ters do we need to define these distributions? For each of the five mixtures, we need a mu and a log_sigma (to define the distribution) and a probability of this mixture being chosen (log_pi), for each of the 32 dimensions of z. This makes 5 × 3 × 32 = 480 parameters. The one extra parameter is for the reward prediction—more specifi‐ cally, the log odds of reward at the next timestep.\\n\\n258\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-14. The output from the mixture density network\\n\\nSampling the Next z and Reward from the MDN-RNN We can sample from the MDN output to generate a prediction for the next z and reward at the following timestep, through the following process:\\n\\n1. Split the 481-dimensional output vector into the 3 variables (log_pi, mu, log_sigma) and the reward value.\\n\\n2. Exponentiate and scale log_pi so that it can be interpreted as 32 probability dis‐ tributions over the 5 mixture indices.\\n\\n3. For each of the 32 dimensions of z, sample from the distributions created from log_pi (i.e., choose which of the 5 distributions should be used for each dimen‐ sion of z).\\n\\n4. Fetch the corresponding values of mu and log_sigma for this distribution. 5. Sample a value for each dimension of z from the normal distribution parameter‐ ized by the chosen parameters of mu and log_sigma for this dimension.\\n\\n6. If the reward log odds value is greater than 0, predict 1 for the reward; otherwise, predict 0.\\n\\nThe MDN-RNN Loss Function The loss function for the MDN-RNN is the sum of the z vector reconstruction loss and the reward loss.\\n\\nThe excerpt from the rnn/arch.py file for the MDN-RNN in Example 8-3 shows how we construct the custom loss function in Keras.\\n\\nExample 8-3. Excerpt from rnn/arch.py\\n\\ndef get_responses(self, y_true):\\n\\nz_true = y_true[:,:,:Z_DIM] rew_true = y_true[:,:,-1]\\n\\nTraining the MDN-RNN\\n\\n|\\n\\n259\\n\\nreturn z_true, rew_true\\n\\ndef get_mixture_coef(self, z_pred):\\n\\nlog_pi, mu, log_sigma = tf.split(z_pred, 3, 1) log_pi = log_pi - K.log(K.sum(K.exp(log_pi), axis = 1, keepdims = True)) return log_pi, mu, log_sigma\\n\\ndef tf_lognormal(self, z_true, mu, log_sigma):\\n\\nlogSqrtTwoPI = np.log(np.sqrt(2.0 * np.pi)) return -0.5 * ((z_true - mu) / K.exp(log_sigma)) ** 2 - log_sigma - logSqrtTwoPI\\n\\ndef rnn_z_loss(y_true, y_pred):\\n\\nz_true, rew_true = self.get_responses(y_true)\\n\\nd = normal distribution_MIXTURES * Z_DIM z_pred = y_pred[:,:,:(3*d)] z_pred = K.reshape(z_pred, [-1, normal distribution_MIXTURES * 3])\\n\\nlog_pi, mu, log_sigma = self.get_mixture_coef(z_pred)\\n\\nflat_z_true = K.reshape(z_true,[-1, 1])\\n\\nz_loss = log_pi + self.tf_lognormal(flat_z_true, mu, log_sigma) z_loss = -K.log(K.sum(K.exp(z_loss), 1, keepdims=True))\\n\\nz_loss = K.mean(z_loss)\\n\\nreturn z_loss\\n\\ndef rnn_rew_loss(y_true, y_pred):\\n\\nz_true, rew_true = self.get_responses(y_true) #, done_true\\n\\nd = normal distribution_MIXTURES * Z_DIM reward_pred = y_pred[:,:,-1]\\n\\nrew_loss = K.binary_crossentropy(rew_true, reward_pred, from_logits = True)\\n\\nrew_loss = K.mean(rew_loss)\\n\\nreturn rew_loss\\n\\ndef rnn_loss(y_true, y_pred):\\n\\nz_loss = rnn_z_loss(y_true, y_pred) rew_loss = rnn_rew_loss(y_true, y_pred)\\n\\n260\\n\\n|\\n\\nChapter 8: Play\\n\\nreturn Z_FACTOR * z_loss + REWARD_FACTOR * rew_loss\\n\\nopti = Adam(lr=LEARNING_RATE) model.compile(loss=rnn_loss, optimizer=opti, metrics = [rnn_z_loss, rnn_rew_loss])\\n\\nSplit the 481-dimensional output vector into the 3 variables (log_pi, mu, log_sigma) and the reward value.\\n\\nThis is the calculation of the z vector reconstruction loss: the negative log- likelihood of observing the true z, under the mixture distribution parameterized by the output from the MDN-RNN. We want this value to be as large as possible, or equivalently, we seek to minimize the negative log likelihood.\\n\\nFor the reward loss, we simply use the binary cross entropy between the true reward and the predicted log odds from the network.\\n\\nThe loss is the sum of the z reconstruction loss and the reward loss—we set the weighting parameters Z_FACTOR and REWARD_FACTOR both to 1, though these can be adjusted to prioritize reconstruction loss or reward loss.\\n\\nNotice that to train the MDN-RNN, we do not need to sample specific z vectors from the MDN output, but instead calculate the loss directly using the 481-dimensional output vector.\\n\\nTraining the Controller The final step is to train the controller (the network that outputs the chosen action) using an evolutionary algorithm called CMA-ES (Covariance Matrix Adaptation Evo‐ lution Strategy).\\n\\nTo start training the controller, run the following command from your terminal (all on one line):\\n\\nxvfb-run -a -s \"-screen 0 1400x900x24\" python 05_train_controller.py car_racing -n 16 -t 2 -e 4 --max_length 1000\\n\\nwhere the parameters are as follows:\\n\\nn\\n\\nThe number of workers that will test solutions in parallel (this should be no greater than the number of cores on your machine)\\n\\nt\\n\\nThe number of solutions that each worker will be given to test at each generation\\n\\nTraining the Controller\\n\\n|\\n\\n261\\n\\ne\\n\\nThe number of episodes that each solution will be tested against to calculate the average reward\\n\\nmax_length\\n\\nThe maximum number of timeframes in each episode\\n\\neval_steps\\n\\nThe number of generations between evaluations of the current best parameter set\\n\\nThe above command uses a virtual frame buffer (xvfb) to render the frames, so that the code can run on a Linux machine without a physical screen. The population size, pop_size = n * t.\\n\\nThe Controller Architecture The architecture of the controller is very simple. It is a densely connected neural net‐ work with no hidden layer; it connects the input vector directly to the action vector.\\n\\nThe input vector is a concatenation of the current z vector (length 32) and the current hidden state of the LSTM (length 256), giving a vector of length 288. Since we are connecting each input unit directly to the 3 output action units, the total number of weights to tune is 288 × 3 = 864, plus 3 bias weights, giving 867 in total.\\n\\nHow should we train this network? Notice that this is not a supervised learning prob‐ lem—we are not trying to predict the correct action. There is no training set of correct actions, as we do not know what the optimal action is for a given state of the environ‐ ment. This is what distinguishes this as a reinforcement learning problem. We need the agent to discover the optimal values for the weights itself by experimenting within the environment and updating its weights based on received feedback.\\n\\nEvolutionary strategies are becoming a popular choice for solving reinforcement learning problems, due to their simplicity, efficiency, and scalability. We shall use one particular strategy, known as CMA-ES.\\n\\nCMA-ES Evolutionary strategies generally adhere to the following process:\\n\\n1. Create a population of agents and randomly initialize the parameters to be opti‐ mized for each agent.\\n\\n2. Loop over the following:\\n\\na. Evaluate each agent in the environment, returning the average reward over\\n\\nmultiple episodes.\\n\\n262\\n\\n|\\n\\nChapter 8: Play\\n\\nb. Breed the agents with the best scores to create new members of the popula‐\\n\\ntion.\\n\\nc. Add randomness to the parameters of the new members.\\n\\nd. Update the population pool by adding the newly created agents and removing\\n\\npoorly performing agents.\\n\\nThis is similar to the process through which animals evolve in nature—hence the name evolutionary strategies. “Breeding” in this context simply means combining the existing best-scoring agents such that the next generation are more likely to produce high-quality results, similar to their parents. As with all reinforcement learning solu‐ tions, there is a balance to be found between greedily searching for locally optimal solutions and exploring unknown areas of the parameter space for potentially better solutions. This is why it is important to add randomness to the population, to ensure we are not too narrow in our search field.\\n\\nCMA-ES is just one form of evolutionary strategy. In short, it works by maintaining a normal distribution from which it can sample the parameters of new agents. At each generation, it updates the mean of the distribution to maximize the likelihood of sampling the high-scoring agents from the previous timestep. At the same time, it updates the covariance matrix of the distribution to maximize the likelihood of sam‐ pling the high-scoring agents, given the previous mean. It can be thought of as a form of naturally arising gradient descent, but with the added benefit that it is derivative- free, meaning that we do not need to calculate or estimate costly gradients.\\n\\nOne generation of the algorithm demonstrated on a toy example is shown in Figure 8-15. Here we are trying to find the minimum point of a highly nonlinear function in two dimensions—the value of the function in the red/black areas of the image is greater than the value of the function in the white/yellow parts of the image.\\n\\nFigure 8-15. One update step from the CMA-ES algorithm2\\n\\n2 Reproduced with permission from David Ha, 2017, http://bit.ly/2XufRwq.\\n\\nTraining the Controller\\n\\n|\\n\\n263\\n\\nThe steps are as follows:\\n\\n1. We start with a randomly generated 2D normal distribution and sample a popu‐ lation of candidates, shown in blue.\\n\\n2. We then calculate the value of the function for each candidate and isolate the best 25%, shown in purple—we’ll call this set of points P.\\n\\n3. We set the mean of the new normal distribution to be the mean of the points in P. This can be thought of as the breeding stage, wherein we only use the best candi‐ dates to generate a new mean for the distribution. We also set the covariance matrix of the new normal distribution to be the covariance matrix of the points in P, but use the existing mean in the covariance calculation rather than the cur‐ rent mean of the points in P. The larger the difference between the existing mean and the mean of the points in P, the wider the variance of the next normal distri‐ bution. This has the effect of naturally creating momentum in the search for the optimal parameters.\\n\\n4. We can then sample a new population of candidates from our new normal distri‐ bution with an updated mean and covariance matrix.\\n\\nFigure 8-16 shows several generations of the process. See how the covariance widens as the mean moves in large steps toward the minimum, but narrows as the mean set‐ tles into the true minimum.\\n\\n264\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-16. CMA-ES3\\n\\nFor the car racing task, we do not have a well-defined function to maximize, but instead an environment where the 867 parameters to be optimized determine how well the agent scores. Initially, some sets of parameters will, by random chance, gen‐ erate scores that are higher than others and the algorithm will gradually move the normal distribution in the direction of those parameters that score highest in the environment.\\n\\nParallelizing CMA-ES One of the great benefits of CMA-ES is that it can be easily parallelized using a Python library created by David Ha called es.py. The most time-consuming part of the algorithm is calculating the score for a given set of parameters, since it needs to simulate an agent with these parameters in the environment. However, this process can be parallelized, since there are no dependencies between individual simulations. In the codebase, we use a master/slave setup, where there is a master process that sends out parameter sets to be tested to many slave processes in parallel. The slave nodes return the results to the master, which accumulates the results and then passes the overall result of the generation to the CMA-ES object. This object updates the\\n\\n3 Source: https://en.wikipedia.org/wiki/CMA-ES.\\n\\nTraining the Controller\\n\\n|\\n\\n265\\n\\nmean and covariance matrix of the normal distribution as per Figure 8-15 and pro‐ vides the master with a new population to test. The loop then starts again. Figure 8-17 explains this in a diagram.\\n\\nFigure 8-17. Parallelizing CMA-ES—here there is a population size of 8 and 4 slave nodes (so t = 2, the number of trials that each slave is responsible for)\\n\\nThe master asks the CMA-ES object (es) for a set of parameters to trial.\\n\\nThe master divides the parameters into the number of slave nodes available. Here, each of the four slave processes gets two parameter sets to trial.\\n\\nThe slave nodes run a worker process that loops over each set of parameters and runs several episodes for each. Here we run three episodes for each set of parameters.\\n\\n266\\n\\n|\\n\\nChapter 8: Play\\n\\nThe rewards from each episode are averaged to give a single score for each set of parameters.\\n\\nThe slave node returns the list of scores to the master.\\n\\nThe master groups all the scores together and sends this list to the es object.\\n\\nThe es object uses this list of rewards to calculate the new normal distribution as per Figure 8-15.\\n\\nOutput from the Controller Training The output of the training process is shown in Figure 8-18. A file storing the weights of the trained network is saved every eval_steps generations.\\n\\nFigure 8-18. Training the controller\\n\\nEach line of the output represents one generation of training. The reported statistics for each generation are as follows:\\n\\n1. Environment name (e.g., car_racing) 2. Generation number (e.g., 16) 3. Current elapsed time in seconds (e.g., 2395) 4. Average reward of the generation (e.g., 136.44) 5. Minimum reward of the generation (e.g., 33.28) 6. Maximum reward of the generation (e.g., 246.12) 7. Standard deviation of the rewards (e.g., 62.78) 8. Current standard deviation factor of the ES process (initialized at 0.5 and decays each timestep; e.g., 0.4604)\\n\\nTraining the Controller\\n\\n|\\n\\n267\\n\\n9. Minimum timesteps taken before termination (e.g., 1000.0) 10. Maximum timesteps taken before termination (e.g., 1000)\\n\\nAfter eval_steps timesteps, each slave node evaluates the current best-scoring parameter set and returns the average rewards across several episodes. These rewards are again averaged to return the overall score for the parameter set.\\n\\nAfter around 200 timesteps, the training process achieves an average reward score of 840 for the car racing task.\\n\\nIn-Dream Training So far, the controller training has been conducted using the OpenAI Gym CarRacing environment to implement the step method that moves the simulation from one state to the next. This function calculates the next state and reward, given the current state of the environment and chosen action.\\n\\nNotice how the step method performs a very similar function to the MDN-RNN in our model. Sampling from the MDN-RNN outputs a prediction for the next z and reward, given the current z and chosen action.\\n\\nIn fact, the MDN-RNN can be thought of as an environment in its own right, but operating in z-space rather than in the original image space. Incredibly, this means that we can actually substitute the real environment with a copy of the MDN-RNN and train the controller entirely within an MDN-RNN-inspired dream of how the environment should behave.\\n\\nIn other words, the MDN-RNN has learned enough about the general physics of the real environment from the original random movement dataset that it can be used as a proxy for the real environment when training the controller. This is quite remarkable —it means that the agent can train itself to learn a new task by thinking about how it can maximize reward in its dream environment, without ever having to test out strategies in the real world. It can then perform well at the task first time, having never attempted the task in reality.\\n\\nThis is one reason why the “World Models” paper is highly important and why gener‐ ative modeling will almost certainly form a key component of artificial intelligence in the future.\\n\\nA comparison of the architectures for training in the real environment and the dream environment follows: the real-world architecture is shown in Figure 8-19 and the in- dream training setup is illustrated in Figure 8-20.\\n\\n268\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-19. Training the controller in the OpenAI Gym environment\\n\\nNotice how in the dream architecture, the training of the controller is performed entirely in z-space without the need to ever decode the z vectors back into recogniza‐ ble track images. We can of course do so, in order to visually inspect the performance of the agent, but it is not required for training.\\n\\nIn-Dream Training\\n\\n|\\n\\n269\\n\\nFigure 8-20. Training the controller in the MDN-RNN dream environment\\n\\nIn-Dream Training the Controller To train the controller using the dream environment, run the following command from your terminal (on one line):\\n\\nxvfb-run -a -s \"-screen 0 1400x900x24\" python 05_train_controller.py car_racing -n 16 -t 2 -e 4 --max_length 1000 --dream_mode 1\\n\\nThis is the same command used to train the controller in the real environment, but with the added flag --dream_mode 1.\\n\\nThe output of the training process is shown in Figure 8-21.\\n\\n270\\n\\n|\\n\\nChapter 8: Play\\n\\nFigure 8-21. Output from in-dream training\\n\\nWhen training in the dream environment, the scores of each generation are given in terms of the average sum of the dream rewards (i.e., 0 or 1 at each timestep). How‐ ever, the evaluation performed after every 10 generations is still conducted in the real environment and is therefore scored based on the sum of rewards from the OpenAI Gym environment, so that we can compare training methods.\\n\\nAfter just 10 generations of training in the dream environment, the agent scores an average of 586.6 in the real environment. The car is able to drive accurately around the track and can handle most corners, except those that are especially sharp.\\n\\nThis is an amazing achievement—remember, when the controller was evaluated after 10 generations it had never attempted the task of driving fast around the track in the real environment. It had only ever driven around the environment randomly (to train the VAE and MDN-RNN) and then in its own dream environment to train the controller.\\n\\nAs a comparison, after 10 generations the agent trained in the real environment is barely able to move off the start line. Moreover, each generation of training in the dream environment is around 3–4 times faster than training in the real environment, since z and reward prediction by the MDN-RNN is faster than z and reward calcula‐ tion by the OpenAI Gym environment.\\n\\nIn-Dream Training\\n\\n|\\n\\n271\\n\\nChallenges of In-Dream Training One of the challenges of training agents entirely within the MDN-RNN dream envi‐ ronment is overfitting. This occurs when the agent finds a strategy that is rewarding in the dream environment, but does not generalize well to the real environment, due to the MDN-RNN not fully capturing how the true environment behaves under cer‐ tain conditions.\\n\\nWe can see this happening in Figure 8-21: after 20 generations, even though the in- dream scores continue to rise, the agent only scores 363.7 in the real environment, which is worse than its score after 10 generations.\\n\\nThe authors of the original “World Models” paper highlight this challenge and show how including a temperature parameter to control model uncertainty can help alle‐ viate the problem. Increasing this parameter magnifies the variance when sampling z through the MDN-RNN, leading to more volatile rollouts when training in the dream environment. The controller receives higher rewards for safer strategies that encounter well-understood states and therefore tend to generalize better to the real environment. Increased temperature, however, needs to be balanced against not making the environment so volatile that the controller cannot learn any strategy, as there is not enough consistency in how the dream environment evolves over time.\\n\\nIn the original paper, the authors show this technique successfully applied to a differ‐ ent environment: DoomTakeCover, based around the computer game Doom. Figure 8-22 shows how changing the temperature parameter affects both the virtual (dream) score and the actual score in the real environment.\\n\\nFigure 8-22. Using temperature to control dream environment volatility4\\n\\n4 Source: Ha and Schmidhuber, 2018.\\n\\n272\\n\\n|\\n\\nChapter 8: Play\\n\\nSummary In this chapter we have seen how a generative model (a VAE) can be utilized within a reinforcement learning setting to enable an agent to learn an effective strategy by test‐ ing policies within its own generated dreams, rather than within the real environment.\\n\\nThe VAE is trained to learn a latent representation of the environment, which is then used as input to a recurrent neural network that forecasts future trajectories within the latent space.\\n\\nAmazingly, the agent can then use this generative model as a pseudoenvironment to iteratively test policies, using an evolutionary methodology, which generalize well to the real environment.\\n\\nSummary\\n\\n|\\n\\n273\\n\\nCHAPTER 9 The Future of Generative Modeling\\n\\nI started writing this book in May 2018, shortly after the “World Models” paper dis‐ cussed in Chapter 8 was published. I knew at the time that I wanted this paper to be the focus of the final core chapter of the book, as it is the first practical example of how generative models can facilitate a deeper form of learning that takes place inside the agent’s own world model of the environment. To this day, I still find this example completely astonishing. It is a glimpse into a future where agents learn not only through maximizing a single reward in an environment of our choice, but by generat‐ ing their own internal representation of an environment and therefore having the capability to create their own reward functions to optimize. In this chapter, we will run with this idea and see where it takes us.\\n\\nFirst, we must place ourselves at the very edge of the generative modeling landscape, among the most radical, innovative, and leading ideas in the field. Since the inception of this book, significant advancements in GAN and attention-based methodologies have taken us to the point where we can now generate images, text, and music that is practically indistinguishable from human-generated content. We shall start by fram‐ ing these advancements alongside examples that we have already explored and walk‐ ing through the most cutting-edge architectures available today.\\n\\nFive Years of Progress The history of generative modeling in its current form is short in comparison to the more widely studied discriminative modeling—the invention of the GAN in 2014 can perhaps be thought of as the spark that lit the touchpaper. Figure 9-1 shows a sum‐ mary of the key developments in generative modeling, many of which we have already explored together in this book.\\n\\n275\\n\\nFigure 9-1. A brief history of generative modeling: green marks represent ideas that are covered in this book and red marks are ideas that we shall explore in this chapter\\n\\n276\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nThis is by no means an exhaustive list; there are dozens of GAN flavors that are groundbreaking in their own fields (e.g., video generation or text-to-image genera‐ tion). Here, I show a selection of the most recent developments that have pushed the boundaries of generative modeling in general.\\n\\nSince mid-2018 there has been a flurry of remarkable developments in both sequence and image-based generative modeling. Sequence modeling has primarily been driven by the invention of the Transformer, an attention-based module that removes the need for recurrent or convolutional neural networks entirely and now powers most state-of-the-art sequential models, such as BERT, GPT-2, and MuseNet. Image gener‐ ation has reached new heights through the development of new GAN-based techni‐ ques such as ProGAN, SAGAN, BigGAN, and StyleGAN.\\n\\nExplaining these developments and their repercussions in detail could easily fill another book. In this chapter, we will simply explore each in enough detail to under‐ stand the fundamental ideas behind the current state of the art in generative model‐ ing. Armed with this knowledge, we will then hypothesize how the field will continue to develop in the near future, providing a tantalizing view of what might be possible in the years to come.\\n\\nThe Transformer The Transformer was first introduced in the 2017 paper “Attention is All You Need,”1 where the authors show how it is possible to create powerful neural networks for sequential modeling that do not require complex recurrent or convolutional architec‐ tures but instead only rely on attention mechanisms. The architecture now powers some of the most impressive practical examples of generative modeling, such as Goo‐ gle’s BERT and GPT-2 for language tasks and MuseNet for music generation.\\n\\nThe overall architecture of the Transformer is shown in Figure 9-2.\\n\\n1 Ashish Vaswani et al., “Attention Is All You Need,” 12 June 2017, https://arxiv.org/abs/1706.03762.\\n\\nThe Transformer\\n\\n|\\n\\n277\\n\\nFigure 9-2. The Transformer model architecture2\\n\\nThe authors apply the Transformer to English–German and English–French transla‐ tion datasets. As is common for translation models, the Transformer has an encoder– decoder architecture (described in Chapter 6). The difference here is that instead of using a recurrent layer such as an LSTM inside the encoder and decoder, the Trans‐ former uses stacked attention layers.\\n\\nIn the lefthand half of Figure 9-2, a set of N = 6 stacked attention layers encodes the input sentence = x1, . . . xn to a sequence of representations. The decoder in the righthand half of the diagram then uses this encoding to generate output words one at a time, using previous words as additional input into the model.\\n\\n2 Source: Vaswani et al., 2017.\\n\\n278\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nTo understand how this works in practice, let’s follow a sample input sequence through the model, step by step.\\n\\nPositional Encoding The words are first passed through an embedding layer to convert each into a vector of length dmodel = 512. Now that we are not using a recurrent layer, we also need to encode the position of each word in the sentence. To achieve this, we use the follow‐ ing positional encoding function that converts the position pos of the word in the sentence into a vector of length dmodel:\\n\\nPEpos, 2i = sin\\n\\n10000\\n\\npos 2i/d\\n\\nmodel\\n\\nPEpos, 2i + 1 = cos\\n\\n10000\\n\\npos 2i + 1 /d\\n\\nmodel\\n\\nFor small i, the wavelength of this function is short and therefore the function value changes rapidly along the position axis. Larger values of i create a longer wavelength, and therefore nearby words are given approximately the same value. Each position thus has its own unique encoding, and since the function can be applied to any value of pos it can be used to encode any position, no matter what the sequence length of the input is.\\n\\nTo construct the input into the first encoder layer, the matrix of positional encodings is added to the word embedding matrix, as shown in Figure 9-3. This way, both the meaning and position for each word in the sequence are captured in a single vector, of length dmodel.\\n\\nThe Transformer\\n\\n|\\n\\n279\\n\\nFigure 9-3. The input embedding matrix is added to the positional encoding matrix to give the input into the first encoder layer\\n\\nMultihead Attention This tensor then flows through to the first of six encoder layers. Each encoder layer consists of several sublayers, starting with the multihead attention layer.\\n\\nThe same multihead attention architecture is used in both the encoder and decoder, with a few small changes. The general architecture is shown in Figure 9-4.\\n\\n280\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nFigure 9-4. Diagram of a multihead attention module, followed by the add & norm layer\\n\\nThe multihead attention layer requires two inputs: the query input, xQ, and the key– value input, xKV. The job of the layer is to learn which positions in the key–value input it should attend to, for every position of the query input. None of the layer’s weight matrices are dependent on the sequence length of the query input (nQ ) or the key–value input (nKV), so the layer can handle sequences of arbitrary length.\\n\\nThe encoder uses self-attention—that is, the query input and key–value input are the same (the output from the previous layer in the encoder). For example, in the first\\n\\nThe Transformer\\n\\n|\\n\\n281\\n\\nencoder layer, both inputs are the positionally encoded embedding of the input sequence. In the decoder, the query input comes from the previous layer in the decoder and the key–value input comes from the final output from the encoder.\\n\\nThe first step of the layer is to create three matrices, the query Q, key K, and value V, through multiplication of the input with three weight matrices, WQ, WK, and WV, as follows:\\n\\nQ = xQWQ K = xKVWK V = xKVWV\\n\\nQ and K are representations of the query input and key–value input, respectively. We want to measure the similarity of these representations across each position in the query input and key–value input.\\n\\nWe can achieve this by performing a matrix multiplication of Q with KT and scaling by a factor dk. This is known as scaled dot-product attention. Scaling is important, to ensure that the dot product between vectors in Q and K does not grow too large.\\n\\nWe then apply a softmax function to ensure all rows sum to 1. This matrix is of shape nQ × nKV and is the equivalent of the attention matrix in Figure 7-10.\\n\\nThe final step to complete the single attention head is to matrix multiply the attention matrix with the value matrix V. In other words, the head outputs a weighted sum of the value representations V for each position in the query, where the weights are determined by the attention matrix.\\n\\nThere’s no reason to only stop at one attention head! In the paper, the authors choose eight heads that are trained in parallel, each outputting an nQ × dv matrix. Incorpo‐ rating multiple heads allows each to learn a distinct attention and value mechanism, therefore enriching the output from the multihead attention layer.\\n\\nThe output matrices from the multiple heads are concatenated and passed through one final matrix multiplication with a weights matrix WO. This is then added point‐ wise to the original query input through a skip connection, and layer normalization (see Figure 5-7) is applied to the result.\\n\\nThe final part of the encoder consists of a feed-forward (densely connected) layer applied to each position separately. The weights are shared across positions, but not between layers of the encoder–decoder. The encoder concludes with one final skip connection and normalization layer. Notice that the output from the layer is the same shape as the query input (nQ × dmodel). This allows us to stack several encoder layers on top of each other, allowing the model to learn deeper features.\\n\\n282\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nThe Decoder The decoder layers are very similar to the encoder layers, with two key differences:\\n\\n1. The initial self-attention layer is masked, so that information from subsequent timesteps cannot be attended to during training. This is achieved by setting the appropriate elements of the input to the softmax to –∞.\\n\\n2. The output from the encoder layer is also incorporated into each layer of the decoder, after the initial self-attention mechanism. Here, the query input comes from the previous layer of the decoder and the key–value input comes from the encoder.\\n\\nEach position in the output from the final decoder layer is fed through one final dense layer with a softmax activation function to give next word probabilities.\\n\\nAnalysis of the Transformer The Tensorflow GitHub repository contains a Colab notebook where you can play around with a trained Transformer model and see how the attention mechanisms of the encoder and decoder impact the translation of a given sentence into German.\\n\\nFor example, Figure 9-5 shows how two attention heads of the decoder layer are able to work together to provide the correct German translation for the word the, when used in the context of the street. In German, there are three definite articles (der, die, das) depending on the gender of the noun, but the Transformer knows to choose die because one attention head is able to attend to the word street (a feminine word in German), while another attends to the word to translate (the).\\n\\nThe Transformer\\n\\n|\\n\\n283\\n\\nFigure 9-5. An example of how one attention head attends to the word “the” and another attends to the word “street” in order to correctly translate the word “the” to the German word “die” as the feminine definite article to “Straße”\\n\\nThis gives the Transformer the ability to translate extremely complex and long sen‐ tences, as it can incorporate information from several places across the input sentence and current translation to form its decision about the next word.\\n\\nThe Transformer architecture has inspired several subsequent models that make use of the multihead attention mechanism. We’ll look at some of these briefly next.\\n\\n284\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nBERT BERT (Bidirectional Encoder Representations from Transformers)3 is a model devel‐ oped by Google that predicts missing words from a sentence, given context from both before and after the missing word in all layers. It achieves this through a masked lan‐ guage model: during training, 15% of words are randomly masked out and the model must try to re-create the original sentence, given the masked input. Crucially, 10% of the tokens marked for masking are actually swapped with another word, rather than the <MASK> token, so not only must the model learn how to replace the <MASK> tokens with actual words, but also it should be looking out for words in the input sentence that do not seem to fit, as they could be words that have been switched.\\n\\nThe word representations learned by BERT are superior to counterparts such as GloVe because they change depending on the context of the word. For example, the word water can be used either as a verb (I need to water the plant) or as a noun (The ocean is full of water). GloVe vectors allocate exactly the same representation to the word water regardless of the context, whereas BERT incorporates surrounding infor‐ mation to create a bespoke representation for the word in context.\\n\\nBERT can be built upon by appending output layers that are specific to a given down‐ stream task. For example, classification tasks such as sentiment analysis can be con‐ structed by adding a classification layer on top of the Transformer output, and question answering tasks can be tackled by marking the answer in the input sequence using a pointer network as the output layer to BERT. By starting from a pretrained BERT model and fine-tuning the appended output layers, it is therefore possible to quickly train extremely sophisticated language models for a variety of modeling tasks.\\n\\nGPT-2 GPT-2 is a model developed by OpenAI that is trained to predict the next word in a passage of text. Whereas BERT was Google’s response to OpenAI’s earlier GPT model, GPT-2 is a direct response to BERT. The key difference between the models is that while BERT is bidirectional, GPT-2 is unidirectional. This means that GPT-2 does not use information from subsequent words to form representations of the cur‐ rent word and therefore is set up for sentence generation tasks, such as the Aesop’s Fables task that we explored in Chapter 6. An example of the output from GPT-2, given a system prompt sentence, is shown in Figure 9-6.\\n\\n3 Jacob Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” 11\\n\\nOctober 2018, https://arxiv.org/abs/1810.04805v1.\\n\\nThe Transformer\\n\\n|\\n\\n285\\n\\nFigure 9-6. An example of how GPT-2 can extend a given system prompt4\\n\\nIf you are slightly terrified at how realistic this appears, you are not alone. Due to concerns about how this model could be abused by malicious parties, for example to generate fake news, forged essays, fake accounts on social media, or impersonations of people online, OpenAI has decided to not release the dataset, code, or GPT-2 model weights. Instead, only small (117M parameter) and medium (345M parame‐ ter) versions of GPT-2 have been released officially.5\\n\\nMuseNet MuseNet is a model also released by OpenAI that applies the Transformer architec‐ ture to music generation. Like GPT-2, it is unidirectional, trained to predict the next note given a sequence of previous notes.\\n\\nIn music generation tasks, the length of the sequence N grows large as the music pro‐ gresses, and this means that the N × N attention matrix for each head becomes expen‐\\n\\n4 Source: “Better Language Models and Their Implications”, 2019, https://openai.com/blog/better-language-\\n\\nmodels.\\n\\n5 As of May 2019, a 1.5B parameter version of GPT-2 has also been released to trusted partners who are researching the potential impact of such sophisticated models and how to counteract misuse of GPT-2.\\n\\n286\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nsive to store and compute. We cannot just clip the input sequence, as we would like the model to construct the piece around a long-term structure and recapitulate motifs and phrases from several minutes ago, as would be the case with a human composer.\\n\\nTo tackle this problem, MuseNet utilizes a form of Transformer known as a Sparse Transformer. Each output position in the attention matrix only computes weights for a subset of input positions, thereby reducing the computational complexity and memory required to train the model. MuseNet can therefore operate with full atten‐ tion over 4,096 tokens and can learn long-term structure and melodic structure across a range of styles. (See, for example, OpenAI’s Chopin and Mozart recordings on SoundCloud.)\\n\\nAdvances in Image Generation In recent years, image-based generative modeling has been revolutionized by several significant advancements in the architecture and training of GAN-based models, in the same way that the Transformer has been pivotal to the progression of sequential generative modeling. In this section we will introduce four such developments—Pro‐ GAN, SAGAN, BigGAN, and StlyeGAN.\\n\\nProGAN ProGAN is a new technique developed by NVIDIA Labs to improve both the speed and stability of GAN training.6 Instead of immediately training a GAN on full- resolution images, the paper suggests first training the generator and discriminator on low-resolution images of, say, 4 × 4 pixels and then incrementally adding layers throughout the training process to increase the resolution. This process is shown in Figure 9-7.\\n\\n6 Tero Karras et al., “Progressive Growing of GANs for Improved Quality, Stability, and Variation,” 27 October\\n\\n2017, https://arxiv.org/abs/1710.10196\\n\\nAdvances in Image Generation\\n\\n|\\n\\n287\\n\\nFigure 9-7. The Progressive GAN training mechanism, and some example generated faces7\\n\\nThe earlier layers are not frozen as training progresses, but remain fully trainable. The new training mechanism was also applied to images from the LSUN dataset with excellent results, as shown in Figure 9-8.\\n\\n7 Source: Karras et al., 2017.\\n\\n288\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nFigure 9-8. Generated examples from a GAN trained progressively on the LSUN dataset at 256 × 256 resolution8\\n\\nSelf-Attention GAN (SAGAN) The Self-Attention GAN (SAGAN)9 is a key development for GANs as it shows how the attention mechanism that powers sequential models such as the Transformer can also be incorporated into GAN-based models for image generation. Figure 9-9 shows the self-attention mechanism from the paper. Note the similarity with the Trans‐ former attention head architecture in Figure 9-2.\\n\\n8 Source: Karras et al., 2017.\\n\\n9 Han Zhang et al., “Self-Attention Generative Adversarial Networks,” 21 May 2018, https://arxiv.org/abs/\\n\\n1805.08318.\\n\\nAdvances in Image Generation\\n\\n|\\n\\n289\\n\\nFigure 9-9. The self-attention mechanism within the SAGAN model10\\n\\nThe problem with GAN-based models that do not incorporate attention is that con‐ volutional feature maps are only able to process information locally. Connecting pixel information from one side of an image to the other requires multiple convolutional layers that reduce the spatial dimension of the image, while increasing the number of channels. Precise positional information is reduced throughout this process in favor of capturing higher-level features, making it computationally inefficient for the model to learn long-range dependencies between distantly connected pixels. SAGAN solves this problem by incorporating the attention mechanism that we explored earlier in this chapter into the GAN. The effect of this inclusion is shown in Figure 9-10.\\n\\nFigure 9-10. A SAGAN-generated image of a bird (leftmost cell) and the attention maps of the final attention-based generator layer for the pixels covered by the three colored dots (rightmost cells)11\\n\\nThe red dot is a pixel that is part of the bird’s body, and so attention naturally falls on surrounding body cells. The green dot is part of the background, and here the atten‐\\n\\n10 Source: Zhang et al., 2018.\\n\\n11 Source: Zhang et al., 2018.\\n\\n290\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\ntion actually falls on the other side of the bird’s head, on other background pixels. The blue dot is part of the bird’s long tail and so attention falls on other tail pixels, some of which are distant from the blue dot. It would be difficult to maintain this long-range dependency for pixels without attention, especially for long, thin struc‐ tures in the image (such as the tail in this case).\\n\\nBigGAN BigGAN,12 developed at DeepMind, extends the ideas from the SAGAN paper with extraordinary results. Figure 9-11 shows some of the images generated by BigGAN, trained on the ImageNet dataset.\\n\\nFigure 9-11. Examples of images generated by BigGAN, trained on the ImageNet dataset at 128 × 128 resolution13\\n\\nBigGAN is currently the state-of-the-art model for image generation on the Image‐ Net dataset. As well as some incremental changes to the base SAGAN model, there are also several innovations outlined in the paper that take the model to the next level of sophistication.\\n\\nOne such innovation is the so-called truncation trick. This is where the latent distri‐ bution used for sampling is different from the z N 0, 1 distribution used during training. Specifically, the distribution used during sampling is a truncated normal dis‐ tribution (resampling z that have magnitude greater than a certain threshold). The smaller the truncation threshold, the greater the believability of generated samples, at the expense of reduced variability. This concept is shown in Figure 9-12.\\n\\n12 Andrew Brock, Jeff Donahue, and Karen Simonyan, “Large Scale GAN Training for High Fidelity Natural\\n\\nImage Synthesis,” 28 September 2018, https://arxiv.org/abs/1809.11096.\\n\\n13 Source: Brock et al., 2018.\\n\\nAdvances in Image Generation\\n\\n|\\n\\n291\\n\\nFigure 9-12. The truncation trick: from left to right, the threshold is set to 2, 1, 0.5, and 0.0414\\n\\nAlso, as the name suggests, BigGAN is an improvement over SAGAN in part simply by being bigger. BigGAN uses a batch size of 2,048—8 times larger than the batch size of 256 used in SAGAN—and a channel size that is increased by 50% in each layer. However, BigGAN additionally shows that SAGAN can be improved structurally by the inclusion of a shared embedding, by orthogonal regularization, and by incorpo‐ rating the latent vector z into each layer of the generator, rather than just the initial layer.\\n\\nFor a full description of the innovations introduced by BigGAN, I recommend read‐ ing the original paper and accompanying presentation material.\\n\\nStyleGAN One of the most recent additions to the GAN literature is StyleGAN,15 from NVIDIA Labs. This builds upon two techniques that we have already explored in this book, ProGAN and neural style transfer (from Chapter 5). Often when training GANs it is difficult to separate out vectors in the latent space corresponding to high-level attributes—they are frequently entangled, meaning that adjusting an image in the latent space to give a face more freckles, for example, might also inadvertently change the background color. While ProGAN generates fantastically realistic images, it is no exception to this general rule. We would ideally like to have full control of the style of the image, and this requires a disentangled separation of high-level features in the latent space.\\n\\nThe overall architecture of the StyleGAN generator is shown in Figure 9-13.\\n\\n14 Source: Brock, Donahue, and Simonyan, 2018.\\n\\n15 Tero Karras, Samuli Laine, and Timo Aila, “A Style-Based Generator Architecture for Generative Adversarial\\n\\nNetworks,” 12 December 2018, https://arxiv.org/abs/1812.04948.\\n\\n292\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nFigure 9-13. The StyleGAN generator architecture16\\n\\nStyleGAN solves the entanglement problem by borrowing ideas from the style trans‐ fer literature. In particular, StyleGAN utilizes a method called adaptive instance nor‐ malization.17 This is a type of neural network layer that adjusts the mean and variance of each feature map i output from a given layer in the synthesis network with a ref‐ erence style bias b, i and scale s, i, respectively. The equation for adaptive instance normalization is as follows:\\n\\nAdaIN i, = s, i\\n\\ni − μ i σ i\\n\\n+ b, i\\n\\n16 Source: Karras, Laine, and Aila, 2018.\\n\\n17 Xun Huang and Serge Belongie, “Arbitrary Style Transfer in Real-Time with Adaptive Instance Normaliza‐\\n\\ntion,” 20 March 2017, https://arxiv.org/abs/1703.06868.\\n\\nAdvances in Image Generation\\n\\n|\\n\\n293\\n\\nThe style parameters are calculated by first passing a latent vector z through a map‐ ping network f to produce an intermediate vector . This is then transformed through a densely connected layer (A) to generate the b, i and s, i vectors, both of length n (the number of channels output from the convolutional layer in the synthesis network). The point of doing this is to separate out the process of choosing a style for the image (the mapping network) from the generation of an image with a given style (the synthesis network). The adaptive instance normalization layers ensure that the style vectors that are injected into each layer only affect features at that layer, by pre‐ venting any style information from leaking through between layers. The authors show that this results in the latent vectors being significantly more disentangled than the original z vectors.\\n\\nSince the synthesis network is based on the ProGAN architecture, the style vectors at earlier layers in the synthesis network (when the resolution of the image is lowest—4 × 4, 8 × 8) will affect coarser features than those later in the network (64 × 64 to 1,024 × 1,024 resolution). This means that not only do we have complete control over the generated image through the latent vector , but we can also switch the vector at different points in the synthesis network to change the style at a variety of levels of detail.\\n\\nFigure 9-14 shows this in action. Here, two images, source A and source B, are gener‐ ated from two different vectors. To generate a merged image, the source A vector is passed through the synthesis network but, at some point, switched for the source B vector. If this switch happens early on (4 × 4 or 8 × 8 resolution), coarse styles such as pose, face shape, and glasses from source B are carried across onto source A. How‐ ever, if the switch happens later, only fine-grained detail is carried across from source B, such as colors and microstructure of the face, while the coarse features from source A are preserved.\\n\\nFinally, the StyleGAN architecture adds noise after each convolution to account for stochastic details such as the placement of individual hairs, or the background behind the face. Again, the depth at which the noise is injected affects the coarseness of the impact on the image.\\n\\n294\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nFigure 9-14. Merging styles between two generated images at different levels of detail18\\n\\n18 Source: Karras, Laine, and Aila, 2018.\\n\\nAdvances in Image Generation\\n\\n|\\n\\n295\\n\\nApplications of Generative Modeling As is clear from the preceding examples, generative modeling has come a long way in the last five years. The field has developed to the point where is it not unreasonable to suggest that the next generation will be just as comfortable marveling at computer- generated art as human art, reading computer-generated novels, and listening to computer-generated music in their favorite style. This movement has already started to gather momentum, particularly among artists and musicians.\\n\\nAI Art I recently attended a meetup in London entitled “Form, Figures and BigGAN,” featur‐ ing presentations by artist Scott Eaton and BigGAN creator Andrew Brock, organized by AI art curator Luba Elliott. One facet of Scott’s work centers around using pix2pix models to create art of the human form. The model is trained on color photographs of dancers and matching grayscale edge-highlighted images. He is able to create new forms by producing a line drawing (i.e., in the edge-highlighted space) and allowing the model to convert back to the color photograph domain. These line drawings need not be realistic human forms; the model will find a way to make the drawing look as human as possible, as this is what it has been trained to do. Two examples of his work are shown in Figure 9-15. You can find more about his artistic process on YouTube.\\n\\n296\\n\\n|\\n\\nChapter 9: The Future of Generative Modeling\\n\\nFigure 9-15. Two examples of Scott Eaton’s work, generated through a pix2pix model trained on photographs of dancers19\\n\\nAI Music As well as producing aesthetically interesting and evocative images, generative mod‐ eling has practical application in the field of music generation, especially for video games and films. We have already seen how MuseNet is able to generate endless amounts of music in a given style and therefore could be adapted to provide the back‐ ground mood music for a film or video game.\\n\\nIn fact, on April 25, 2019, OpenAI live-streamed an experimental concert in which MuseNet generated music across a range of styles that no human had ever heard before. Could it be that before long, we will be able to tune into a radio station that plays music in our favorite style nonstop, so that we never hear the same thing twice? Perhaps we could have the option of saving passages that we particularly like to listen to again, or exploring new music generated on the fly by the model. We are not yet at the stage where text and music can be convincingly combined to produce pop songs with long-term structure, but given the impressive progress in both text and music generation in recent years it surely won’t be long before this is a reality.\\n\\n19 Source: Scott Eaton, 2018, http://www.scott-eaton.com.\\n\\nApplications of Generative Modeling\\n\\n|\\n\\n297\\n\\nCHAPTER 10 Conclusion\\n\\nIn this book, we have taken a journey through the last half-decade of generative mod‐ eling research, starting out with the basic ideas behind variational autoencoders, GANs, and recurrent neural networks and building upon these foundations to under‐ stand how state-of-the-art models such as the Transformer, advanced GAN architec‐ tures, and world models are now pushing the boundaries of what generative models are capable of achieving, across a variety of tasks.\\n\\nI believe that in the future, generative modeling may be the key to a deeper form of artificial intelligence that transcends any one particular task and instead allows machines to organically formulate their own rewards, strategies, and ultimately awareness within their environment.\\n\\nAs babies, we are constantly exploring our surroundings, building up a mental model of possible futures with no apparent aim other than to develop a deeper understand‐ ing of the world. There are no labels on the data that we receive—a seemingly ran‐ dom stream of light and sound waves that bombard our senses from the moment we are born. Even when our mother or father points to an apple and says apple, there is no reason for our young brains to associate the two and learn that the way in which light entered our eye at that particular moment is in any way related to the way the sound waves entered our ear. There is no training set of sounds and images, no train‐ ing set of smells and tastes, and no training set of actions and rewards. Just an endless stream of extremely noisy data.\\n\\nAnd yet here you are now, reading this sentence, perhaps enjoying the taste of a cup of coffee in a noisy cafe. You pay no attention to the background noise as you concen‐ trate on converting the absence of light on a tiny proportion of your retina into a sequence of abstract concepts that convey almost no meaning individually but, when combined, trigger a wave of parallel representations in your mind’s eye—images,\\n\\n299\\n\\nemotions, ideas, beliefs, and potential actions all flood your consciousness, awaiting your recognition.\\n\\nThe same noisy stream of data that was essentially meaningless to your infant brain is not so noisy any more. Everything makes sense to you. You see structure everywhere. You are never surprised by the physics of everyday life. The world is the way that it is, because your brain decided it should be that way.\\n\\nIn this sense, your brain is an extremely sophisticated generative model, equipped with the ability to attend to particular parts of the input data, form representations of concepts within a latent space of neural pathways, and process sequential data over time. But what exactly is it generating?\\n\\nAt this point, I must switch into pure speculation mode as we are close to the edge of what we currently understand about the human brain (and certainly at the very edge of what I understand about the human brain). However, we can conduct a thought experiment to understand the links between generative modeling and the brain.\\n\\nSuppose that the brain is a near-perfect generative model of the input stream of data that it is subjected to. In other words, it can generate the likely sequence of input data that would follow from receiving the cue of an egg-shaped region of light falling through the visual field to the sound of a splat as the egg-shaped region stops moving abruptly. It does this by creating representations of the key aspects of the visual and auditory fields and modeling how these latent representations will evolve over time. There is one fallacy in this view, however: the brain is not a passive observer of events. It’s attached to a neck and a set of legs that can put its core input sensors in any myriad of positions relative to the source of the input data. The generated sequence of possible futures is not only dependent on its understanding of the phys‐ ics of the environment, but also on its understanding of itself and how it acts.\\n\\nThis is the core idea that I believe will propel generative modeling into the spotlight in the next decade, as one of the keys to unlocking artificial general intelligence. Imagine if we could build a generative model that doesn’t model possible futures of the environment given an action, as per the world models example, but instead includes its own action-generating process as part of the environment to be modeled.\\n\\nIf actions are random to begin with, why would the model learn anything except to predict random actions from the body in which it resides? The answer is simple: because nonrandom actions make the stream of environmental data easier to gener‐ ate. If the sole goal of a brain is to minimize the amount of surprise between the actual input stream of data and the model of the future input stream, then the brain must find a way to make its actions create the future that it expects.\\n\\nThis may seem backward—wouldn’t it make more sense for the brain to act accord‐ ing to some policy that tries to maximize a reward? The problem with this is that nature does not provide us with rewards; it just provides data. The only true reward is\\n\\n300\\n\\n|\\n\\nChapter 10: Conclusion\\n\\nstaying alive, and this can hardly be used to explain every action of an intelligent being. Instead, if we flip this on its head and require that the action is part of the envi‐ ronment to be generated and that the sole goal of intelligence is to generate actions and futures that match the reality of the input data, then perhaps we avoid the need for any external reward function from the environment. However, whether this setup would generate actions that could be classed as intelligent remains to be seen.\\n\\nAs I stated, this is purely a speculative view, but it is fun to speculate, so I will con‐ tinue doing so. I encourage you to do the same and to continue learning more about generative models from all the great material that is available online and in other books. Thank you for taking the time to read to the end of this book—I hope you have enjoyed reading it as much as I have enjoyed generating it. <END>\\n\\nConclusion\\n\\n|\\n\\n301\\n\\nSymbols 1-Lipschitz continuous function, 117\\n\\nA activation functions, 38 Adam optimizer, 42 AI art, 296\\n\\n(see also style transfer)\\n\\nAI music, 297\\n\\n(see also music generation)\\n\\nAnaconda, 27 artifacts, 105 artificial neural networks (ANNs), 33 arXiv, xi attention mechanisms analysis of, 213-217 building in Keras, 208-212 in encoder-decoder networks, 217-221 examples in language translation, 206 generating polyphonic music, 221 multihead attention module, 280\\n\\nautoencoders\\n\\nanalysis of, 72-75 building, 66 decoder architecture, 68-71 encoder architecture, 66 joining encoder to decoder, 71 parts of, 64 process used by, 65 representation vectors in, 65 uses for, 65\\n\\nB backpropagation, 34\\n\\nIndex\\n\\nbatch normalization, 51-53, 55, 125 BERT (Bidirectional Encoder Representations\\n\\nfrom Transformers), 285\\n\\nBigGAN, 291 binary cross-entropy loss, 42, 71, 107\\n\\nC categorical cross-entropy loss, 42 CelebFaces Attributes (CelebA) dataset, 86 character tokens, 169 CIFAR-10 dataset, 35, 120 CMA-ES (covariance matrix adaptation evolu‐\\n\\ntion strategy), 261-268\\n\\nCNTK, 34 code examples, obtaining and using, x, xii, 27 comments and questions, xiii composition (see music generation) concatenate layer, 140 content loss, 154-156 convolutional layers in neural networks, 46-51,\\n\\n59\\n\\nconvolutional transpose layers, 68-69 covariate shift, 52 CycleGAN (cycle-consistent adversarial net‐\\n\\nwork) analysis of, 147 benefits of, 135 compiling, 144-146 CycleGAN versus pix2pix, 135 discriminators, 142 generators (ResNet), 150-151 generators (U-Net), 139-142 Keras-GAN code repository, 137 Monet-style transfer example, 149-153\\n\\n303\\n\\noverview of, 137 published paper on, 135 training, 146 training data, 137\\n\\nD DCGAN (deep convolutional generative adver‐\\n\\nsarial network), 101\\n\\ndeep learning\\n\\ndeep neural networks, 33, 59 defined, 31 Keras and TensorFlow for, 34 model creation, 35-46 model improvement, 46-58 premise behind, 154 structured versus unstructured data, 31\\n\\ndense layers, 33 density function, 11 discriminative modeling, 2 dropout layers, 54\\n\\nE encoder-decoder models, 187-190, 217-221 environment setup, 27-29 evolutionary strategies, 262 exploding gradient problem, 51\\n\\nF facial image generation dataset used, 86 encoder and decoder architecture, 88 generating new faces, 92 latent space arithmetic, 93 morphing between faces, 94 progress in, 5 VAE analysis, 91 VAE training, 87\\n\\nfeatures, 2 fit_generator method, 88 Functional API (Keras), 37-41\\n\\nG gated recurrent units (GRUs), 168, 185 generative adversarial networks (GANs)\\n\\nchallenges of, 112-115 defining, 100-106 discriminators, 101, 142 \"ganimal\" example, 97-99\\n\\n304\\n\\n|\\n\\nIndex\\n\\npublished paper on, 97 theory underlying, 99 training, 107-112 Wasserstein GAN, 115-121 WGAN-GP, 121-127 generative deep learning\\n\\nadditional resources, xi advances in generative modeling, 5-7,\\n\\n277-297\\n\\nchallenges of, 22-27 future of, 299-301 Generative modeling framework, 7-10 history of generative modeling, 275-277 introduction to, 1-5 learning objectives and approach, x learning prerequisites, x probabilistic generative models, 10-21\\n\\ngenerators\\n\\nattention-based, 290 bar generator, 229 in GANs, 103-106 MuseGAN generator, 226 question-answer generators, 190-200 ResNet generators, 150-151 StyleGAN generator, 292 U-Net generators, 139\\n\\nGloVe (“Global Vectors”), 195, 285 Goodfellow, Ian, 97 Google Colaboratory, xi GPT-2 language model, 5, 285 gradient descent, 153 Gram matrices, 158\\n\\nH Ha, David, 237, 243, 265 Hello Wrodl! example, 13-21 hidden layers, 34 hidden state, 174, 176 Hinton, Geoffrey, 4, 54 Hochreiter, Sepp, 167 Hou, Xianxu, 91 Hull, Jonathan, 118 hyperparameters, 114\\n\\nI identity, 145 image generation (see also facial image genera‐\\n\\ntion; neural style transfer technique) BigGAN, 291\\n\\nCIFAR-10 dataset for, 35 generative modeling process, 1 generative versus discriminative modeling,\\n\\n2\\n\\nkey breakthrough in, 4 ProGAN, 287 progress in facial image generation, 5 representation learning for, 23 rise of generative modeling for, 5 Self-Attention GAN (SAGAN), 289 StyleGAN, 292 ImageNet dataset, 154 ImageNet Large Scale Visual Recognition Chal‐\\n\\nlenge (ILSVRC), 4 in-dream training, 268-272 inference, 196 instance normalization layers, 140\\n\\nK Keras\\n\\nattention mechanisms in, 208-212 autoencoder creation in, 67 backends for, 34 benefits of, 34 content loss calculation in, 155 Conv2DTranspose layer, 70, 105 custom loss function creation, 259 CycleGAN creation and training, 137-149 data loading, 35 decoder creation in, 70 documentation, 43 fit_generator method, 88 GAN discriminator creation in, 102 importing, 28 inference model in, 197 LSTM in, 168 model building, 37-41 model compilation, 41 model evaluation, 44 model improvement, 46-58 model training, 43 MuseGAN generator in, 230 PatchGAN discriminators in, 143 residual blocks in, 150 U-Net generators in, 141 VAE creation in, 81\\n\\nKeras layers\\n\\nActivation, 56 Batch Normalization, 51\\n\\nBidirectional, 187 Concatenate, 140 Conv2D, 47 Conv2DTranspose, 70 Conv3D, 232 Dense, 38 Dropout, 54 Embedding, 172 Flatten, 38 GRU, 168 Input, 38 Lambda, 82 LeakyReLU, 56 LSTM, 174 Reshape, 211 Upsampling2D layer, 104\\n\\nKingma, Diederik, 61 Kullback–Leibler (KL) divergence, 84\\n\\nL L-BFGS-B algorithm, 161 labels, 3 language translation, 188, 206 layers, 33 LeakyReLU, 38 likelihood, 12 Lipschitz constraint, 117, 121 loss functions, 41 LSTM (long short-term memory) networks\\n\\nwith attention mechanism, 206 dataset used, 168 embedding layer, 172 generating datasets, 171 generating new text, 179-182 history of, 167 LSTM architecture, 172 LSTM cell, 176-178 LSTM layer, 174-176 published paper on, 167 tokenizing the text, 168-170\\n\\nM machine learning advances in, 4 major branches of, 238\\n\\nmachine painting (see style transfer) machine writing (see text data generation) Machine-Learning-as-a-Service (MLaaS), 5 Maluuba NewsQA dataset, 191\\n\\nIndex\\n\\n|\\n\\n305\\n\\nmasked language model, 285 maximum likelihood estimation, 13 MDN (mixture density network), 243, 255-261 mean squared error loss, 42 MIDI files, 202 mode collapse, 113 models\\n\\nCycleGAN, 135-152 deep neural networks, 35-58 encoder-decoder models, 187-190 generative adversarial networks (GANs),\\n\\n97-115\\n\\ngenerative modeling, 1-10 generative versus discriminative modeling,\\n\\n2\\n\\nimproving models, 46-58 LSTM (long short-term memory) networks,\\n\\n168-178\\n\\nneural style transfer, 153-162 parametric modeling, 11 probabilistic generative models, 10-21 probabilistic versus deterministic, 2 question-answer generators, 190-200 RNNs (recurrent neural networks), 205-221 variational autoencoders (VAEs), 61-96 Wasserstein GAN, 115-121 WGAN-GP, 121-127 World Model architecture, 241-244\\n\\nMonet-to-photo dataset, 149 multihead attention module, 280 multilayer RNNs, 183 MuseGAN, 223-235 analysis of, 233 creation, 223-231\\n\\nMuseNet, 286, 297 MuseScore, 202 music generation\\n\\nchallenges of, 201 data extraction, 204 dataset used, 202 generating polyphonic music, 221 importing MIDI files, 202 MuseGAN analysis, 233 MuseGAN creation, 223-231 MuseGAN critic, 232 MuseGAN example, 221 music versus text generation, 201 musical notation, 204 prerequisites to, 202\\n\\n306\\n\\n|\\n\\nIndex\\n\\nRNN (recurrent neural network) for,\\n\\n205-221 music21 library, 202\\n\\nN Naive Bayes parametric model, 17-20 neural style transfer technique\\n\\nanalysis of, 161 content loss, 154-156 definition of, 153 premise of, 153 running, 160 style loss, 156-159 total variance loss, 160 nontrainable parameters, 53 normal distribution, 79\\n\\nO observations, 1 OpenAI Gym, 239 optimizers, 41 oscillating loss, 112 overfitting, 54\\n\\nP padding, 48 painting (see style transfer) Papers with Code, xi parameters, trainable and nontrainable, 53 parametric modeling, 11 PatchGAN discriminators, 143 pix2pix, 135 positional encoding, 279 probabilistic generative models\\n\\nchallenges of, 22 Hello Wrodl! example, 13, 20 model construction, 14-17 Naive Bayes parametric model, 17-20 probabilistic theory behind, 10-13\\n\\nprobability density function, 11, 79 ProGAN, 287 Project Gutenberg, 168 Python, 27\\n\\nQ qgen-workshop TensorFlow codebase, 190 question-answer generators\\n\\ndataset used, 191\\n\\nencoder-decoder models, 188 inference, 196 model architecture, 192-196 model parts, 190 model results, 198-200 questions and comments, xiii\\n\\nR reconstruction loss, 84 regularization techniques, 54 reinforcement learning (RL)\\n\\ndefined, 238 key terminology, 238 OpenAI Gym toolkit for, 239 process of, 239\\n\\nReLU (rectified linear unit), 38 representation learning, 23-27 representation vectors, 65 residual networks (ResNets), 150-151 RMSProp optimizer, 43 RNNs (recurrent neural networks)\\n\\nbidirectional cells, 187 gated recurrent units (GRUs), 185 history of, 167, 167 LSTM (long short-term memory) networks,\\n\\n167-182\\n\\nMDN-RNN World Model architecture, 243 for music generation, 205-221 stacked recurrent networks, 183, 206\\n\\nroot mean squared error (RMSE), 71\\n\\nS sample space, 11 scaled dot-product attention, 282 Schmidhuber, Jurgen, 167, 237 self-attention, 281 Self-Attention GAN (SAGAN), 289 sequence modeling, 277 Sequential models (Keras), 37-41 sigmoid activation, 39 skip connections, 139 softmax activation, 39 stacked recurrent networks, 183 standard deviation, 79 standard normal curves, 79 stemming, 169 stochastic (random) elements, 2 strides parameter (Keras), 48 structured data, 31\\n\\nstyle loss, 156-159 style transfer\\n\\naim of, 131 apples and organges example, 132-134 CycleGAN analysis, 147 CycleGAN creation and training, 137-147 CycleGAN introduction, 135 CycleGAN Monet example, 149-152 neural style transfer technique, 153-162 uses for, 131 StyleGAN, 5, 292 supervised learning, 3\\n\\nT TensorFlow, 34 text data generation\\n\\nencoder-decoder models, 187-190 LSTM (long short-term memory) networks,\\n\\n167-182\\n\\nquestion-answer generators, 190-200 RNN (recurrent neural network) exten‐\\n\\nsions, 183-187\\n\\nshort story generation example, 166 text versus image data, 165 text versus music generation, 201\\n\\ntext summarization, 188 Theano, 34 tokenization, 168-170 total variance loss, 160 trainable parameters, 53 training data, 1 training process, 34 Transformer\\n\\nanalysis of, 283 BERT model, 285 decoder layers, 283 GPT-2 language model, 285 history of, 206 models architecture, 277 multihead attention layer, 280 MuseNet model, 286 positional encoding function, 279 published paper on, 277\\n\\ntruncation trick, 291\\n\\nU U-Net, 139-142 uninformative loss, 114 units, 33, 174\\n\\nIndex\\n\\n|\\n\\n307\\n\\nunstructured data, 31 upsampling, 104\\n\\nV validity, 145 vanishing gradient problem, 151, 167 variance, 79 variational autoencoders (VAEs) autoencoder analysis, 72-75 autoencoder example, 66-72 autoencoder parts, 64 decoders, 68-71 encoders, 66-68 facial image generation using, 86-95 generative art example, 61-64, 75-77 published paper on, 61, 91 VAE analysis, 85 VAE build in Keras, 81 VAE diagram, 82 VAE loss function, 84 VAE parts, 78-85 World Model architecture, 242 World Model training, 248-255\\n\\nVGG19 network, 154 virtual environments, 27\\n\\nW Wasserstein GANs (WGANs)\\n\\n308\\n\\n|\\n\\nIndex\\n\\nanalysis of, 120 benefits of, 115 Lipschitz constraint, 117 training, 119 Wasserstein loss, 115-117 weight clipping, 118\\n\\nWasserstein GAN–Gradient Penalty (WGAN-\\n\\nGP) analysis of, 125 converting WGAN to WGAN-GP, 121 gradient penalty loss, 121-125\\n\\nweight clipping, 118, 121 weights, 33 Welling, Max, 61 World Models\\n\\ncollecting random rollout data, 245 collecting RNN training data, 255 model setup, 244 published paper on, 237 training in-dream, 268-272 training overview, 245 training the controller, 261-268 training the MDN-RNN, 257-261 training the VAE, 248-255 World Model architecture, 241-244\\n\\nWorld Models paper, 237, 243, 275 Wrodl!, 13\\n\\nAbout the Author\\n\\nDavid Foster is the cofounder of Applied Data Science, a data science consultancy delivering bespoke solutions for clients. He holds an MA in mathematics from Trinity College, Cambridge, UK, and an MSc in operational research from the University of Warwick.\\n\\nDavid has won several international machine learning competitions, including the InnoCentive Predicting Product Purchase challenge, and was awarded first prize for a visualization that enables a pharmaceutical company in the US to optimize site selec‐ tion for clinical trials.\\n\\nHe is an active participant in the online data science community and has authored several successful blog posts on deep reinforcement learning including “How To Build Your Own AlphaZero AI Using Python and Keras”.\\n\\nColophon\\n\\nThe animal on the cover of Generative Deep Learning is a painted parakeet (Pyrrhura picta). The Pyrrhura genus falls under the family Psittacidae, one of three families of parrots. Within its subfamily Arinae are several macaw and parakeet species of the Western Hemisphere. The painted parakeet inhabits the coastal forest and mountains of northeastern South America.\\n\\nBright green feathers cover most of a painted parakeet, but they are blue above the beak, brown in the face, and reddish in the breast and tail. Most strikingly, the feath‐ ers on the painted parakeet’s neck look like scales; the brown center is outlined in off- white. This combination of colors camouflages the birds in the rainforest.\\n\\nPainted parakeets tend to feed in the forest canopy, where their green plumage masks them best. They forage in flocks of 5 to 12 birds for a wide variety of fruits, seeds, and flowers. Occasionally, when feeding below the canopy, painted parakeets will eat algae from forest pools. They grow to about 9 inches in length and live for 13 to 15 years. A clutch of painted parakeet chicks is usually around five eggs, which are less than one inch wide at hatching.\\n\\nMany of the animals on O’Reilly’s covers are endangered; all of them are important to the world.\\n\\nThe cover illustration is by Karen Montgomery, based on a black and white engraving from Shaw’s Zoology. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\\n\\nThere’s much more where this came from.\\n\\nExperience books, videos, live online training courses, and more from O’Reilly and our 200+ partners—all in one place.\\n\\nLearn more at oreilly.com/online-learning\\n\\n5 7 1\\n\\nc n\\n\\na d e M y\\n\\ni\\n\\nl l i\\n\\ne R O\\n\\n’\\n\\nf o k r a m e d a r t d e r e t s g e r\\n\\ni\\n\\na\\n\\ns\\n\\ny\\n\\nl l i\\n\\ne R O\\n\\n’\\n\\nc n\\n\\na d e M y\\n\\ni\\n\\nl l i\\n\\ne R O 9 1\\n\\n’\\n\\n0 2 ©', metadata={'source': '../llm_gemini_pdf_chat/docs/David Foster - Generative Deep Learning_ Teaching Machines to Paint, Write, Compose, and Play-O’Reilly Media (2019).pdf'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we load the documents, we split them using the RecursiveCharacterTextSplitter from Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=50):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Text Using Langchain : will use SentenceTransformerEmbeddings from Langchain  OR we can import huggingface library to use transformer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Vector Store with Chroma DB:\n",
    "- Vector stores serve as a prevalent method for handling and searching through unstructured data. \n",
    "- The standard process involves creating embeddings from the unstructured data, saving these generated vectors, and then, during a query, embedding the unstructured query to retrieve the 'most similar' vectors to this embedded query. \n",
    "- The role of a vector store is primarily to facilitate this storage of embedded data and execute the similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a vector store using the Chroma DB from the documents\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# as we have not specified any persistent directory, this db is stored in memory right now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "query = \"What is Batch Normalization?\"\n",
    "matching_docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In Keras, the BatchNormalization layer implements the batch normalization functionality:\\n\\nBatchNormalization(momentum = 0.9)\\n\\nThe momentum parameter is the weight given to the previous value when calculating the moving average and moving standard deviation.\\n\\n6 Source: Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by\\n\\nReducing Internal Covariate Shift,” 11 February 2015, https://arxiv.org/abs/1502.03167.\\n\\nImproving the Model\\n\\n|\\n\\n53', metadata={'source': '../llm_gemini_pdf_chat/docs/David Foster - Generative Deep Learning_ Teaching Machines to Paint, Write, Compose, and Play-O’Reilly Media (2019).pdf'}),\n",
       " Document(page_content='In Keras, the BatchNormalization layer implements the batch normalization functionality:\\n\\nBatchNormalization(momentum = 0.9)\\n\\nThe momentum parameter is the weight given to the previous value when calculating the moving average and moving standard deviation.\\n\\n6 Source: Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by\\n\\nReducing Internal Covariate Shift,” 11 February 2015, https://arxiv.org/abs/1502.03167.\\n\\nImproving the Model\\n\\n|\\n\\n53', metadata={'source': '../llm_gemini_pdf_chat/docs/David Foster - Generative Deep Learning_ Teaching Machines to Paint, Write, Compose, and Play-O’Reilly Media (2019).pdf'}),\n",
       " Document(page_content='Instance Normalization Layer The generator of this CycleGAN uses InstanceNormalization layers rather than BatchNormalization layers, which in style transfer problems can lead to more satis‐ fying results.8\\n\\n8 Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, “Instance Normalization: The Missing Ingredient for\\n\\nFast Stylization,” 27 July 2016, https://arxiv.org/pdf/1607.08022.pdf.\\n\\n140\\n\\n|\\n\\nChapter 5: Paint\\n\\nAn InstanceNormalization layer normalizes every single observation individually, rather than as a batch. Unlike a BatchNormalization layer, it doesn’t require mu and sigma parameters to be calculated as a running average during training, since at test time the layer can normalize per instance in the same way as it does at train time. The means and standard deviations used to normalize each layer are calculated per chan‐ nel and per observation.', metadata={'source': '../llm_gemini_pdf_chat/docs/David Foster - Generative Deep Learning_ Teaching Machines to Paint, Write, Compose, and Play-O’Reilly Media (2019).pdf'}),\n",
       " Document(page_content='Instance Normalization Layer The generator of this CycleGAN uses InstanceNormalization layers rather than BatchNormalization layers, which in style transfer problems can lead to more satis‐ fying results.8\\n\\n8 Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, “Instance Normalization: The Missing Ingredient for\\n\\nFast Stylization,” 27 July 2016, https://arxiv.org/pdf/1607.08022.pdf.\\n\\n140\\n\\n|\\n\\nChapter 5: Paint\\n\\nAn InstanceNormalization layer normalizes every single observation individually, rather than as a batch. Unlike a BatchNormalization layer, it doesn’t require mu and sigma parameters to be calculated as a running average during training, since at test time the layer can normalize per instance in the same way as it does at train time. The means and standard deviations used to normalize each layer are calculated per chan‐ nel and per observation.', metadata={'source': '../llm_gemini_pdf_chat/docs/David Foster - Generative Deep Learning_ Teaching Machines to Paint, Write, Compose, and Play-O’Reilly Media (2019).pdf'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='In Keras, the BatchNormalization layer implements the batch normalization functionality:\\n\\nBatchNormalization(momentum = 0.9)\\n\\nThe momentum parameter is the weight given to the previous value when calculating the moving average and moving standard deviation.\\n\\n6 Source: Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by\\n\\nReducing Internal Covariate Shift,” 11 February 2015, https://arxiv.org/abs/1502.03167.\\n\\nImproving the Model\\n\\n|\\n\\n53', metadata={'source': '../llm_gemini_pdf_chat/docs/David Foster - Generative Deep Learning_ Teaching Machines to Paint, Write, Compose, and Play-O’Reilly Media (2019).pdf'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Persistence in Chroma DB:\n",
    " If you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs, embedding=embeddings, persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "\n",
    "# creates folder chroma_db and inside some files like data_level0.bin, header.bin, length.bin, link_lists.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-1.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from openai) (4.1.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from openai) (2.5.2)\n",
      "Requirement already satisfied: sniffio in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /home/db/miniconda3/envs/kllm/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.5)\n",
      "Using cached openai-1.9.0-py3-none-any.whl (223 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: httpcore, distro, httpx, openai\n",
      "Successfully installed distro-1.9.0 httpcore-1.0.2 httpx-0.26.0 openai-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OpenAI Large Language Models (LLM) with Chroma DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# model_name = \"gpt-3.5-turbo\"\n",
    "# llm = ChatOpenAI(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Answers from Documents: \n",
    "- LangChain introduces a useful abstraction called a 'Chain' for representing sequences of calls to components. \n",
    "- These components can include other chains, making it possible to build complex, nested sequences of operations. \n",
    "- One specific type of chain is the question-answering (QA) chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "|\n",
      "\n",
      "59\n",
      "\n",
      "CHAPTER 3 Variational Autoencoders\n",
      "\n",
      "In 2013, Diederik P. Kingma and Max Welling published a paper that laid the founda‐ tions for a type of neural network known as a variational autoencoder (VAE).1 This is now one of the most fundamental and well-known deep learning architectures for generative modeling. In this chapter, we shall start by building a standard autoen‐ coder and then see how we can extend this framework to develop a variational autoencoder—our first example of a generative deep learning model.\n",
      "\n",
      "Along the way, we will pick apart both types of model, to understand how they work at a granular level. By the end of the chapter you should have a complete understand‐ ing of how to build and manipulate autoencoder-based models and, in particular, how to build a variational autoencoder from scratch to generate images based on your own training set.\n",
      "\n",
      "Let’s start by paying a visit to a strange art exhibition…\n",
      "\n",
      "|\n",
      "\n",
      "59\n",
      "\n",
      "CHAPTER 3 Variational Autoencoders\n",
      "\n",
      "In 2013, Diederik P. Kingma and Max Welling published a paper that laid the founda‐ tions for a type of neural network known as a variational autoencoder (VAE).1 This is now one of the most fundamental and well-known deep learning architectures for generative modeling. In this chapter, we shall start by building a standard autoen‐ coder and then see how we can extend this framework to develop a variational autoencoder—our first example of a generative deep learning model.\n",
      "\n",
      "Along the way, we will pick apart both types of model, to understand how they work at a granular level. By the end of the chapter you should have a complete understand‐ ing of how to build and manipulate autoencoder-based models and, in particular, how to build a variational autoencoder from scratch to generate images based on your own training set.\n",
      "\n",
      "Let’s start by paying a visit to a strange art exhibition…\n",
      "\n",
      "Summary In this chapter we have seen how variational autoencoders are a powerful tool in the generative modeling toolbox. We started by exploring how plain autoencoders can be used to map high-dimensional images into a low-dimensional latent space, so that high-level features can be extracted from the individually uninformative pixels. How‐ ever, like with the Coder brothers’ art exhibition, we quickly found that there were some drawbacks to using plain autoencoders as a generative model—sampling from the learned latent space was problematic, for a number of reasons.\n",
      "\n",
      "Summary\n",
      "\n",
      "|\n",
      "\n",
      "95\n",
      "\n",
      "Variational autoencoders solve these problems, by introducing randomness into the model and constraining how points in the latent space are distributed. We saw that with a few minor adjustments, we can transform our autoencoder into a variational autoencoder, thus giving it the power to be a generative model.\n",
      "\n",
      "Summary In this chapter we have seen how variational autoencoders are a powerful tool in the generative modeling toolbox. We started by exploring how plain autoencoders can be used to map high-dimensional images into a low-dimensional latent space, so that high-level features can be extracted from the individually uninformative pixels. How‐ ever, like with the Coder brothers’ art exhibition, we quickly found that there were some drawbacks to using plain autoencoders as a generative model—sampling from the learned latent space was problematic, for a number of reasons.\n",
      "\n",
      "Summary\n",
      "\n",
      "|\n",
      "\n",
      "95\n",
      "\n",
      "Variational autoencoders solve these problems, by introducing randomness into the model and constraining how points in the latent space are distributed. We saw that with a few minor adjustments, we can transform our autoencoder into a variational autoencoder, thus giving it the power to be a generative model.\n",
      "Human: What are Variational Autoencoders?\u001b[0m\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are Variational Autoencoders?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m matching_docs \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39msimilarity_search(query)\n\u001b[0;32m----> 6\u001b[0m answer \u001b[38;5;241m=\u001b[39m  \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatching_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m answer\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/base.py:543\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    539\u001b[0m         _output_key\n\u001b[1;32m    540\u001b[0m     ]\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    544\u001b[0m         _output_key\n\u001b[1;32m    545\u001b[0m     ]\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    361\u001b[0m }\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    151\u001b[0m     inputs,\n\u001b[1;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:136\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    135\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 136\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:244\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    361\u001b[0m }\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    151\u001b[0m     inputs,\n\u001b[1;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain/chains/llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    124\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:543\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    541\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    542\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:407\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    406\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    409\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    411\u001b[0m ]\n\u001b[1;32m    412\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:397\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 397\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:576\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m     )\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:439\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    434\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    438\u001b[0m }\n\u001b[0;32m--> 439\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:356\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/resources/chat/completions.py:648\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    647\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_base_client.py:1179\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1167\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1176\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1177\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1178\u001b[0m     )\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_base_client.py:868\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    861\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    867\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_base_client.py:944\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    943\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_base_client.py:992\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_base_client.py:944\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    943\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_base_client.py:992\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kllm/lib/python3.11/site-packages/openai/_base_client.py:959\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    956\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    962\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    963\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    967\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\",verbose=True)\n",
    "\n",
    "query = \"What are Variational Autoencoders?\"\n",
    "matching_docs = db.similarity_search(query)\n",
    "answer =  chain.run(input_documents=matching_docs, question=query)\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
